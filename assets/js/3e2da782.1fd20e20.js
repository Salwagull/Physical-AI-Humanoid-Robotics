"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[672],{5476:(r,e,n)=>{n.r(e),n.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"chapter10_computer_vision","title":"Chapter 10: Computer Vision for Robotics","description":"Master computer vision techniques for robotic perception including image processing, feature detection, depth estimation, and 3D reconstruction","source":"@site/docs/chapter10_computer_vision.md","sourceDirName":".","slug":"/chapter10_computer_vision","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter10_computer_vision","draft":false,"unlisted":false,"editUrl":"https://github.com/Salwagull/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter10_computer_vision.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10,"title":"Chapter 10: Computer Vision for Robotics","description":"Master computer vision techniques for robotic perception including image processing, feature detection, depth estimation, and 3D reconstruction","keywords":["computer vision","robotics","opencv","perception","image processing","depth estimation","3d reconstruction","ros2","camera calibration"]},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: LLM Planning and Voice Commands","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter9_llm_voice_commands"},"next":{"title":"Chapter 11: 3D Perception & Depth Sensing","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter11_3d_perception"}}');var i=n(4848),a=n(8453);const s={sidebar_position:10,title:"Chapter 10: Computer Vision for Robotics",description:"Master computer vision techniques for robotic perception including image processing, feature detection, depth estimation, and 3D reconstruction",keywords:["computer vision","robotics","opencv","perception","image processing","depth estimation","3d reconstruction","ros2","camera calibration"]},o="Chapter 10: Computer Vision for Robotics",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Robotic Vision",id:"introduction-to-robotic-vision",level:2},{value:"Why Vision Matters for Physical AI",id:"why-vision-matters-for-physical-ai",level:3},{value:"Vision System Architecture",id:"vision-system-architecture",level:2},{value:"Camera Types for Robotics",id:"camera-types-for-robotics",level:3},{value:"ROS 2 Vision Stack",id:"ros-2-vision-stack",level:3},{value:"Camera Calibration",id:"camera-calibration",level:2},{value:"Understanding Camera Models",id:"understanding-camera-models",level:3},{value:"Calibration Implementation",id:"calibration-implementation",level:3},{value:"Feature Detection and Matching",id:"feature-detection-and-matching",level:2},{value:"Common Feature Detectors",id:"common-feature-detectors",level:3},{value:"Depth Estimation and 3D Reconstruction",id:"depth-estimation-and-3d-reconstruction",level:2},{value:"Stereo Vision",id:"stereo-vision",level:3},{value:"Visual Odometry",id:"visual-odometry",level:2},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Computer Vision Pipeline Architecture",id:"computer-vision-pipeline-architecture",level:2},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Camera Calibration Practice",id:"exercise-1-camera-calibration-practice",level:3},{value:"Exercise 2: Feature Tracking Application",id:"exercise-2-feature-tracking-application",level:3},{value:"Exercise 3: Depth-Based Obstacle Detection",id:"exercise-3-depth-based-obstacle-detection",level:3},{value:"Challenge: Build a Visual SLAM System",id:"challenge-build-a-visual-slam-system",level:3},{value:"Up Next",id:"up-next",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function p(r){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...r.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-10-computer-vision-for-robotics",children:"Chapter 10: Computer Vision for Robotics"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand the fundamentals of computer vision for robotic systems"}),"\n",(0,i.jsx)(e.li,{children:"Implement image processing pipelines using OpenCV and ROS 2"}),"\n",(0,i.jsx)(e.li,{children:"Perform camera calibration for accurate measurements"}),"\n",(0,i.jsx)(e.li,{children:"Apply feature detection and matching for visual navigation"}),"\n",(0,i.jsx)(e.li,{children:"Estimate depth and reconstruct 3D scenes from images"}),"\n",(0,i.jsx)(e.li,{children:"Integrate computer vision with robot control systems"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-robotic-vision",children:"Introduction to Robotic Vision"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Computer vision"})," is the field of AI that enables machines to interpret and understand visual information from the world. For robots, vision is a primary sensing modality that provides rich information about the environment, enabling navigation, manipulation, and human-robot interaction."]}),"\n",(0,i.jsx)(e.h3,{id:"why-vision-matters-for-physical-ai",children:"Why Vision Matters for Physical AI"}),"\n",(0,i.jsx)(e.p,{children:"Robots operating in the real world must perceive their surroundings to act intelligently. Computer vision provides:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Environmental awareness"}),": Understand the layout and contents of a space"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Object recognition"}),": Identify and classify objects for manipulation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Obstacle detection"}),": Navigate safely around barriers"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Pose estimation"}),": Determine object positions and orientations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human understanding"}),": Recognize gestures, faces, and intentions"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    Computer Vision Pipeline for Robotics                    \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                             \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502  Camera  \u2502\u2500\u2500\u2500\u25b6\u2502   Image      \u2502\u2500\u2500\u2500\u25b6\u2502   Feature    \u2502\u2500\u2500\u2500\u25b6\u2502   Object     \u2502 \u2502\r\n\u2502  \u2502  Input   \u2502    \u2502 Processing   \u2502    \u2502  Extraction  \u2502    \u2502  Detection   \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2502                                                                   \u2502        \u2502\r\n\u2502                                                                   \u25bc        \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502  Robot   \u2502\u25c0\u2500\u2500\u2500\u2502   Motion     \u2502\u25c0\u2500\u2500\u2500\u2502   Depth      \u2502\u25c0\u2500\u2500\u2500\u2502   3D Scene   \u2502 \u2502\r\n\u2502  \u2502  Action  \u2502    \u2502  Planning    \u2502    \u2502  Estimation  \u2502    \u2502 Reconstruction\u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2502                                                                             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h2,{id:"vision-system-architecture",children:"Vision System Architecture"}),"\n",(0,i.jsx)(e.p,{children:"A complete robotic vision system consists of several interconnected components working together to transform raw sensor data into actionable information."}),"\n",(0,i.jsx)(e.h3,{id:"camera-types-for-robotics",children:"Camera Types for Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Different camera technologies serve different purposes in robotics:"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Camera Type"}),(0,i.jsx)(e.th,{children:"Best For"}),(0,i.jsx)(e.th,{children:"Typical Use"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"RGB Camera"})}),(0,i.jsx)(e.td,{children:"Color perception, object recognition"}),(0,i.jsx)(e.td,{children:"General-purpose vision"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Depth Camera"})}),(0,i.jsx)(e.td,{children:"3D perception, obstacle avoidance"}),(0,i.jsx)(e.td,{children:"Navigation, manipulation"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Stereo Camera"})}),(0,i.jsx)(e.td,{children:"Outdoor depth estimation"}),(0,i.jsx)(e.td,{children:"Mobile robots, drones"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Event Camera"})}),(0,i.jsx)(e.td,{children:"High-speed motion tracking"}),(0,i.jsx)(e.td,{children:"Dynamic environments"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Thermal Camera"})}),(0,i.jsx)(e.td,{children:"Heat detection, night vision"}),(0,i.jsx)(e.td,{children:"Rescue robots, inspection"})]})]})]}),"\n",(0,i.jsx)(e.h3,{id:"ros-2-vision-stack",children:"ROS 2 Vision Stack"}),"\n",(0,i.jsx)(e.p,{children:"ROS 2 provides standardized interfaces for camera data:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nCamera interface node for ROS 2 robotic vision systems.\r\n\r\nThis node demonstrates how to subscribe to camera topics,\r\nprocess images, and publish results for downstream nodes.\r\n\r\nTopics:\r\n    Subscribed:\r\n        /camera/image_raw (sensor_msgs/Image)\r\n        /camera/camera_info (sensor_msgs/CameraInfo)\r\n        /camera/depth/image_raw (sensor_msgs/Image)\r\n\r\n    Published:\r\n        /vision/processed_image (sensor_msgs/Image)\r\n        /vision/detections (vision_msgs/Detection2DArray)\r\n\r\nPrerequisites:\r\n    pip install opencv-python numpy\r\n    sudo apt install ros-humble-cv-bridge ros-humble-vision-msgs\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nfrom typing import Optional, Tuple\r\n\r\n\r\nclass VisionSystemNode(Node):\r\n    """\r\n    Core vision processing node for robotic perception.\r\n\r\n    Handles camera input, image processing, and publishes\r\n    processed results for other ROS 2 nodes to consume.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'vision_system_node\')\r\n\r\n        # Initialize CV bridge for ROS <-> OpenCV conversion\r\n        self.bridge = CvBridge()\r\n\r\n        # Camera intrinsics (populated from CameraInfo)\r\n        self.camera_matrix: Optional[np.ndarray] = None\r\n        self.dist_coeffs: Optional[np.ndarray] = None\r\n        self.image_size: Optional[Tuple[int, int]] = None\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/depth/image_raw\',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/camera_info\',\r\n            self.camera_info_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.processed_pub = self.create_publisher(\r\n            Image,\r\n            \'/vision/processed_image\',\r\n            10\r\n        )\r\n\r\n        # State\r\n        self.latest_rgb: Optional[np.ndarray] = None\r\n        self.latest_depth: Optional[np.ndarray] = None\r\n\r\n        # Processing timer (30 Hz)\r\n        self.timer = self.create_timer(1/30.0, self.process_frame)\r\n\r\n        self.get_logger().info(\'Vision System Node initialized\')\r\n\r\n    def camera_info_callback(self, msg: CameraInfo):\r\n        """Store camera calibration parameters."""\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.dist_coeffs = np.array(msg.d)\r\n        self.image_size = (msg.width, msg.height)\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Convert and store incoming RGB images."""\r\n        try:\r\n            self.latest_rgb = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Image conversion error: {e}\')\r\n\r\n    def depth_callback(self, msg: Image):\r\n        """Convert and store incoming depth images."""\r\n        try:\r\n            # Depth images are typically 16UC1 (millimeters) or 32FC1 (meters)\r\n            self.latest_depth = self.bridge.imgmsg_to_cv2(msg, \'passthrough\')\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Depth conversion error: {e}\')\r\n\r\n    def process_frame(self):\r\n        """Main processing loop - called at 30 Hz."""\r\n        if self.latest_rgb is None:\r\n            return\r\n\r\n        # Example processing pipeline\r\n        processed = self.preprocess_image(self.latest_rgb)\r\n\r\n        # Publish processed image\r\n        try:\r\n            msg = self.bridge.cv2_to_imgmsg(processed, \'bgr8\')\r\n            self.processed_pub.publish(msg)\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Publishing error: {e}\')\r\n\r\n    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\r\n        """\r\n        Apply standard preprocessing for robotic vision.\r\n\r\n        Args:\r\n            image: Raw BGR image from camera\r\n\r\n        Returns:\r\n            Preprocessed image ready for further analysis\r\n        """\r\n        # Undistort if calibration available\r\n        if self.camera_matrix is not None:\r\n            image = cv2.undistort(image, self.camera_matrix, self.dist_coeffs)\r\n\r\n        # Denoise\r\n        image = cv2.fastNlMeansDenoisingColored(image, None, 3, 3, 7, 21)\r\n\r\n        # Enhance contrast using CLAHE\r\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\r\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\r\n        lab[:, :, 0] = clahe.apply(lab[:, :, 0])\r\n        image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\r\n\r\n        return image\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisionSystemNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,i.jsx)(e.p,{children:"Accurate camera calibration is essential for measuring real-world distances and performing 3D reconstruction."}),"\n",(0,i.jsx)(e.h3,{id:"understanding-camera-models",children:"Understanding Camera Models"}),"\n",(0,i.jsx)(e.p,{children:"Cameras introduce distortions that must be corrected for precise measurements:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                      Camera Pinhole Model                          \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                    \u2502\r\n\u2502    3D World Point                      2D Image Point              \u2502\r\n\u2502    (X, Y, Z)                           (u, v)                      \u2502\r\n\u2502         \u2502                                  \u25b2                       \u2502\r\n\u2502         \u2502                                  \u2502                       \u2502\r\n\u2502         \u25bc                                  \u2502                       \u2502\r\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    Projection Matrix    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\r\n\u2502    \u2502  World  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Image \u2502                  \u2502\r\n\u2502    \u2502  Frame  \u2502    K[R|t]               \u2502 Plane \u2502                  \u2502\r\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\r\n\u2502                                                                    \u2502\r\n\u2502    Intrinsic Matrix K:    \u2502 fx  0  cx \u2502                           \u2502\r\n\u2502                           \u2502 0  fy  cy \u2502                           \u2502\r\n\u2502                           \u2502 0   0   1 \u2502                           \u2502\r\n\u2502                                                                    \u2502\r\n\u2502    fx, fy = focal lengths (pixels)                                 \u2502\r\n\u2502    cx, cy = principal point (image center)                         \u2502\r\n\u2502                                                                    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h3,{id:"calibration-implementation",children:"Calibration Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nCamera calibration utilities for robotic vision systems.\r\n\r\nUses a checkerboard pattern to compute camera intrinsics\r\nand distortion coefficients for accurate measurements.\r\n\r\nPrerequisites:\r\n    pip install opencv-python numpy\r\n\r\nUsage:\r\n    python camera_calibration.py --images ./calibration_images/\r\n"""\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom pathlib import Path\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Tuple, Optional\r\nimport json\r\n\r\n\r\n@dataclass\r\nclass CalibrationResult:\r\n    """Stores camera calibration results."""\r\n    camera_matrix: np.ndarray\r\n    dist_coeffs: np.ndarray\r\n    rvecs: List[np.ndarray]\r\n    tvecs: List[np.ndarray]\r\n    reprojection_error: float\r\n    image_size: Tuple[int, int]\r\n\r\n    def save(self, filepath: str):\r\n        """Save calibration to JSON file."""\r\n        data = {\r\n            \'camera_matrix\': self.camera_matrix.tolist(),\r\n            \'dist_coeffs\': self.dist_coeffs.tolist(),\r\n            \'reprojection_error\': self.reprojection_error,\r\n            \'image_size\': list(self.image_size)\r\n        }\r\n        with open(filepath, \'w\') as f:\r\n            json.dump(data, f, indent=2)\r\n\r\n    @classmethod\r\n    def load(cls, filepath: str) -> \'CalibrationResult\':\r\n        """Load calibration from JSON file."""\r\n        with open(filepath, \'r\') as f:\r\n            data = json.load(f)\r\n        return cls(\r\n            camera_matrix=np.array(data[\'camera_matrix\']),\r\n            dist_coeffs=np.array(data[\'dist_coeffs\']),\r\n            rvecs=[],\r\n            tvecs=[],\r\n            reprojection_error=data[\'reprojection_error\'],\r\n            image_size=tuple(data[\'image_size\'])\r\n        )\r\n\r\n\r\nclass CameraCalibrator:\r\n    """\r\n    Performs camera calibration using checkerboard pattern.\r\n\r\n    The calibration process:\r\n    1. Detect checkerboard corners in multiple images\r\n    2. Compute intrinsic matrix and distortion coefficients\r\n    3. Validate with reprojection error\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        board_size: Tuple[int, int] = (9, 6),\r\n        square_size: float = 0.025  # meters\r\n    ):\r\n        """\r\n        Initialize calibrator.\r\n\r\n        Args:\r\n            board_size: Number of inner corners (columns, rows)\r\n            square_size: Size of each square in meters\r\n        """\r\n        self.board_size = board_size\r\n        self.square_size = square_size\r\n\r\n        # Prepare 3D object points for the checkerboard\r\n        self.objp = np.zeros((board_size[0] * board_size[1], 3), np.float32)\r\n        self.objp[:, :2] = np.mgrid[0:board_size[0], 0:board_size[1]].T.reshape(-1, 2)\r\n        self.objp *= square_size\r\n\r\n        # Detection criteria for corner refinement\r\n        self.criteria = (\r\n            cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,\r\n            30,\r\n            0.001\r\n        )\r\n\r\n    def detect_corners(\r\n        self,\r\n        image: np.ndarray,\r\n        visualize: bool = False\r\n    ) -> Tuple[bool, Optional[np.ndarray]]:\r\n        """\r\n        Detect checkerboard corners in an image.\r\n\r\n        Args:\r\n            image: Input image (BGR or grayscale)\r\n            visualize: Whether to show detection result\r\n\r\n        Returns:\r\n            (success, corners) tuple\r\n        """\r\n        # Convert to grayscale if needed\r\n        if len(image.shape) == 3:\r\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n        else:\r\n            gray = image.copy()\r\n\r\n        # Find checkerboard corners\r\n        ret, corners = cv2.findChessboardCorners(\r\n            gray,\r\n            self.board_size,\r\n            cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE\r\n        )\r\n\r\n        if ret:\r\n            # Refine corner positions to sub-pixel accuracy\r\n            corners = cv2.cornerSubPix(\r\n                gray, corners, (11, 11), (-1, -1), self.criteria\r\n            )\r\n\r\n            if visualize:\r\n                vis_image = image.copy() if len(image.shape) == 3 else cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\r\n                cv2.drawChessboardCorners(vis_image, self.board_size, corners, ret)\r\n                cv2.imshow(\'Detected Corners\', vis_image)\r\n                cv2.waitKey(500)\r\n\r\n        return ret, corners if ret else None\r\n\r\n    def calibrate(\r\n        self,\r\n        images: List[np.ndarray],\r\n        verbose: bool = True\r\n    ) -> Optional[CalibrationResult]:\r\n        """\r\n        Perform camera calibration from a list of images.\r\n\r\n        Args:\r\n            images: List of calibration images\r\n            verbose: Print progress information\r\n\r\n        Returns:\r\n            CalibrationResult or None if calibration failed\r\n        """\r\n        object_points = []  # 3D points in world coordinates\r\n        image_points = []   # 2D points in image coordinates\r\n        image_size = None\r\n\r\n        for i, image in enumerate(images):\r\n            if image_size is None:\r\n                image_size = (image.shape[1], image.shape[0])\r\n\r\n            ret, corners = self.detect_corners(image)\r\n\r\n            if ret:\r\n                object_points.append(self.objp)\r\n                image_points.append(corners)\r\n                if verbose:\r\n                    print(f\'Image {i+1}/{len(images)}: corners detected\')\r\n            else:\r\n                if verbose:\r\n                    print(f\'Image {i+1}/{len(images)}: no corners found\')\r\n\r\n        if len(object_points) < 3:\r\n            print(\'Error: Need at least 3 valid calibration images\')\r\n            return None\r\n\r\n        if verbose:\r\n            print(f\'\\nCalibrating with {len(object_points)} images...\')\r\n\r\n        # Perform calibration\r\n        ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\r\n            object_points,\r\n            image_points,\r\n            image_size,\r\n            None,\r\n            None\r\n        )\r\n\r\n        if not ret:\r\n            print(\'Calibration failed\')\r\n            return None\r\n\r\n        # Calculate reprojection error\r\n        total_error = 0\r\n        for i in range(len(object_points)):\r\n            projected, _ = cv2.projectPoints(\r\n                object_points[i], rvecs[i], tvecs[i],\r\n                camera_matrix, dist_coeffs\r\n            )\r\n            error = cv2.norm(image_points[i], projected, cv2.NORM_L2)\r\n            total_error += error ** 2\r\n\r\n        reprojection_error = np.sqrt(total_error / len(object_points))\r\n\r\n        if verbose:\r\n            print(f\'\\nCalibration successful!\')\r\n            print(f\'Reprojection error: {reprojection_error:.4f} pixels\')\r\n            print(f\'\\nCamera matrix:\\n{camera_matrix}\')\r\n            print(f\'\\nDistortion coefficients: {dist_coeffs.ravel()}\')\r\n\r\n        return CalibrationResult(\r\n            camera_matrix=camera_matrix,\r\n            dist_coeffs=dist_coeffs,\r\n            rvecs=rvecs,\r\n            tvecs=tvecs,\r\n            reprojection_error=reprojection_error,\r\n            image_size=image_size\r\n        )\r\n\r\n    def undistort(\r\n        self,\r\n        image: np.ndarray,\r\n        calibration: CalibrationResult\r\n    ) -> np.ndarray:\r\n        """Remove lens distortion from an image."""\r\n        return cv2.undistort(\r\n            image,\r\n            calibration.camera_matrix,\r\n            calibration.dist_coeffs\r\n        )\r\n\r\n\r\ndef calibrate_from_folder(folder_path: str, output_path: str = \'calibration.json\'):\r\n    """\r\n    Convenience function to calibrate from a folder of images.\r\n\r\n    Args:\r\n        folder_path: Path to folder containing calibration images\r\n        output_path: Where to save calibration results\r\n    """\r\n    calibrator = CameraCalibrator()\r\n    folder = Path(folder_path)\r\n\r\n    # Load all images\r\n    images = []\r\n    for ext in [\'*.jpg\', \'*.png\', \'*.bmp\']:\r\n        for img_path in folder.glob(ext):\r\n            image = cv2.imread(str(img_path))\r\n            if image is not None:\r\n                images.append(image)\r\n\r\n    print(f\'Found {len(images)} images in {folder_path}\')\r\n\r\n    result = calibrator.calibrate(images)\r\n\r\n    if result:\r\n        result.save(output_path)\r\n        print(f\'Calibration saved to {output_path}\')\r\n\r\n    return result\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    import sys\r\n    if len(sys.argv) > 1:\r\n        calibrate_from_folder(sys.argv[1])\r\n    else:\r\n        print(\'Usage: python camera_calibration.py <image_folder>\')\n'})}),"\n",(0,i.jsx)(e.h2,{id:"feature-detection-and-matching",children:"Feature Detection and Matching"}),"\n",(0,i.jsx)(e.p,{children:"Feature detection enables visual odometry, SLAM, and object tracking by finding distinctive points in images."}),"\n",(0,i.jsx)(e.h3,{id:"common-feature-detectors",children:"Common Feature Detectors"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nFeature detection and matching for robotic vision.\r\n\r\nImplements ORB, SIFT, and custom feature pipelines for\r\nvisual navigation and object tracking.\r\n\r\nPrerequisites:\r\n    pip install opencv-python opencv-contrib-python numpy\r\n"""\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Tuple, Optional\r\nfrom enum import Enum\r\n\r\n\r\nclass FeatureType(Enum):\r\n    """Supported feature detector types."""\r\n    ORB = "orb"       # Fast, rotation-invariant, open-source\r\n    SIFT = "sift"     # Scale-invariant, more accurate but slower\r\n    AKAZE = "akaze"   # Modern alternative to SIFT, open-source\r\n    BRISK = "brisk"   # Binary descriptor, very fast\r\n\r\n\r\n@dataclass\r\nclass FeatureMatch:\r\n    """Represents a feature match between two images."""\r\n    pt1: Tuple[float, float]  # Point in image 1\r\n    pt2: Tuple[float, float]  # Point in image 2\r\n    distance: float           # Match quality (lower = better)\r\n\r\n\r\nclass FeatureDetector:\r\n    """\r\n    Unified interface for feature detection and matching.\r\n\r\n    Supports multiple feature types with consistent API\r\n    for robotic vision applications.\r\n    """\r\n\r\n    def __init__(self, feature_type: FeatureType = FeatureType.ORB):\r\n        """\r\n        Initialize feature detector.\r\n\r\n        Args:\r\n            feature_type: Type of features to detect\r\n        """\r\n        self.feature_type = feature_type\r\n        self.detector = self._create_detector()\r\n        self.matcher = self._create_matcher()\r\n\r\n    def _create_detector(self):\r\n        """Create the appropriate feature detector."""\r\n        if self.feature_type == FeatureType.ORB:\r\n            return cv2.ORB_create(\r\n                nfeatures=1000,\r\n                scaleFactor=1.2,\r\n                nlevels=8,\r\n                edgeThreshold=31,\r\n                patchSize=31\r\n            )\r\n        elif self.feature_type == FeatureType.SIFT:\r\n            return cv2.SIFT_create(\r\n                nfeatures=1000,\r\n                contrastThreshold=0.04,\r\n                edgeThreshold=10\r\n            )\r\n        elif self.feature_type == FeatureType.AKAZE:\r\n            return cv2.AKAZE_create()\r\n        elif self.feature_type == FeatureType.BRISK:\r\n            return cv2.BRISK_create()\r\n        else:\r\n            raise ValueError(f"Unknown feature type: {self.feature_type}")\r\n\r\n    def _create_matcher(self):\r\n        """Create appropriate feature matcher."""\r\n        if self.feature_type in [FeatureType.ORB, FeatureType.BRISK, FeatureType.AKAZE]:\r\n            # Binary descriptors use Hamming distance\r\n            return cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\r\n        else:\r\n            # Float descriptors use L2 distance\r\n            return cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\r\n\r\n    def detect(\r\n        self,\r\n        image: np.ndarray,\r\n        mask: Optional[np.ndarray] = None\r\n    ) -> Tuple[List[cv2.KeyPoint], np.ndarray]:\r\n        """\r\n        Detect features in an image.\r\n\r\n        Args:\r\n            image: Input image (BGR or grayscale)\r\n            mask: Optional mask specifying where to detect\r\n\r\n        Returns:\r\n            (keypoints, descriptors) tuple\r\n        """\r\n        # Convert to grayscale if needed\r\n        if len(image.shape) == 3:\r\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n        else:\r\n            gray = image\r\n\r\n        # Detect and compute descriptors\r\n        keypoints, descriptors = self.detector.detectAndCompute(gray, mask)\r\n\r\n        return keypoints, descriptors\r\n\r\n    def match(\r\n        self,\r\n        desc1: np.ndarray,\r\n        desc2: np.ndarray,\r\n        ratio_threshold: float = 0.75\r\n    ) -> List[cv2.DMatch]:\r\n        """\r\n        Match features using Lowe\'s ratio test.\r\n\r\n        Args:\r\n            desc1: Descriptors from image 1\r\n            desc2: Descriptors from image 2\r\n            ratio_threshold: Ratio test threshold (0.7-0.8 typical)\r\n\r\n        Returns:\r\n            List of good matches\r\n        """\r\n        if desc1 is None or desc2 is None:\r\n            return []\r\n\r\n        # Find k=2 nearest neighbors\r\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\r\n\r\n        # Apply Lowe\'s ratio test\r\n        good_matches = []\r\n        for match_pair in matches:\r\n            if len(match_pair) == 2:\r\n                m, n = match_pair\r\n                if m.distance < ratio_threshold * n.distance:\r\n                    good_matches.append(m)\r\n\r\n        return good_matches\r\n\r\n    def find_matches(\r\n        self,\r\n        image1: np.ndarray,\r\n        image2: np.ndarray,\r\n        ratio_threshold: float = 0.75\r\n    ) -> Tuple[List[FeatureMatch], np.ndarray]:\r\n        """\r\n        Complete pipeline: detect features in both images and match.\r\n\r\n        Args:\r\n            image1: First image\r\n            image2: Second image\r\n            ratio_threshold: Match quality threshold\r\n\r\n        Returns:\r\n            (matches, visualization) tuple\r\n        """\r\n        # Detect features\r\n        kp1, desc1 = self.detect(image1)\r\n        kp2, desc2 = self.detect(image2)\r\n\r\n        # Match features\r\n        raw_matches = self.match(desc1, desc2, ratio_threshold)\r\n\r\n        # Convert to FeatureMatch objects\r\n        matches = []\r\n        for m in raw_matches:\r\n            matches.append(FeatureMatch(\r\n                pt1=kp1[m.queryIdx].pt,\r\n                pt2=kp2[m.trainIdx].pt,\r\n                distance=m.distance\r\n            ))\r\n\r\n        # Create visualization\r\n        vis = cv2.drawMatches(\r\n            image1, kp1,\r\n            image2, kp2,\r\n            raw_matches,\r\n            None,\r\n            matchColor=(0, 255, 0),\r\n            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\r\n        )\r\n\r\n        return matches, vis\r\n\r\n    def compute_homography(\r\n        self,\r\n        matches: List[FeatureMatch],\r\n        ransac_threshold: float = 5.0\r\n    ) -> Tuple[Optional[np.ndarray], np.ndarray]:\r\n        """\r\n        Compute homography from matches using RANSAC.\r\n\r\n        Args:\r\n            matches: List of feature matches\r\n            ransac_threshold: RANSAC inlier threshold in pixels\r\n\r\n        Returns:\r\n            (homography_matrix, inlier_mask) tuple\r\n        """\r\n        if len(matches) < 4:\r\n            return None, np.array([])\r\n\r\n        pts1 = np.float32([m.pt1 for m in matches]).reshape(-1, 1, 2)\r\n        pts2 = np.float32([m.pt2 for m in matches]).reshape(-1, 1, 2)\r\n\r\n        H, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, ransac_threshold)\r\n\r\n        return H, mask.ravel() if mask is not None else np.array([])\r\n\r\n\r\ndef demo_feature_matching():\r\n    """Demonstrate feature detection and matching."""\r\n    # Create synthetic test images\r\n    img1 = np.zeros((480, 640, 3), dtype=np.uint8)\r\n    img2 = np.zeros((480, 640, 3), dtype=np.uint8)\r\n\r\n    # Add some shapes for features\r\n    cv2.rectangle(img1, (100, 100), (200, 200), (255, 255, 255), -1)\r\n    cv2.circle(img1, (400, 300), 50, (0, 255, 255), -1)\r\n    cv2.putText(img1, \'Test\', (300, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\r\n\r\n    # Create transformed version (rotated and translated)\r\n    M = cv2.getRotationMatrix2D((320, 240), 15, 1.0)\r\n    img2 = cv2.warpAffine(img1, M, (640, 480))\r\n\r\n    # Test different feature types\r\n    for ftype in [FeatureType.ORB, FeatureType.AKAZE]:\r\n        detector = FeatureDetector(ftype)\r\n\r\n        matches, vis = detector.find_matches(img1, img2)\r\n\r\n        print(f"\\n{ftype.value.upper()} Features:")\r\n        print(f"  Found {len(matches)} matches")\r\n\r\n        if len(matches) >= 4:\r\n            H, inliers = detector.compute_homography(matches)\r\n            if H is not None:\r\n                print(f"  Homography inliers: {inliers.sum()}/{len(matches)}")\r\n\r\n        cv2.imshow(f\'{ftype.value} Matches\', vis)\r\n\r\n    cv2.waitKey(0)\r\n    cv2.destroyAllWindows()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    demo_feature_matching()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"depth-estimation-and-3d-reconstruction",children:"Depth Estimation and 3D Reconstruction"}),"\n",(0,i.jsx)(e.p,{children:"Robots need to understand the 3D structure of their environment for navigation and manipulation."}),"\n",(0,i.jsx)(e.h3,{id:"stereo-vision",children:"Stereo Vision"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nStereo vision for depth estimation in robotic systems.\r\n\r\nComputes depth maps from stereo camera pairs for\r\n3D perception and obstacle avoidance.\r\n\r\nPrerequisites:\r\n    pip install opencv-python numpy\r\n"""\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import Tuple, Optional\r\n\r\n\r\n@dataclass\r\nclass StereoCalibration:\r\n    """Stereo camera calibration parameters."""\r\n    camera_matrix_left: np.ndarray\r\n    dist_coeffs_left: np.ndarray\r\n    camera_matrix_right: np.ndarray\r\n    dist_coeffs_right: np.ndarray\r\n    R: np.ndarray  # Rotation between cameras\r\n    T: np.ndarray  # Translation between cameras\r\n    baseline: float  # Distance between cameras (meters)\r\n\r\n\r\nclass StereoDepthEstimator:\r\n    """\r\n    Computes depth from stereo image pairs.\r\n\r\n    Uses semi-global block matching (SGBM) for dense\r\n    depth estimation with sub-pixel accuracy.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        calibration: Optional[StereoCalibration] = None,\r\n        image_size: Tuple[int, int] = (640, 480)\r\n    ):\r\n        """\r\n        Initialize stereo depth estimator.\r\n\r\n        Args:\r\n            calibration: Stereo calibration parameters\r\n            image_size: Expected image size (width, height)\r\n        """\r\n        self.calibration = calibration\r\n        self.image_size = image_size\r\n\r\n        # Stereo matcher parameters\r\n        self.num_disparities = 128  # Must be divisible by 16\r\n        self.block_size = 11       # Odd number >= 1\r\n\r\n        # Create stereo matcher\r\n        self.stereo_matcher = cv2.StereoSGBM_create(\r\n            minDisparity=0,\r\n            numDisparities=self.num_disparities,\r\n            blockSize=self.block_size,\r\n            P1=8 * 3 * self.block_size ** 2,\r\n            P2=32 * 3 * self.block_size ** 2,\r\n            disp12MaxDiff=1,\r\n            uniquenessRatio=10,\r\n            speckleWindowSize=100,\r\n            speckleRange=32,\r\n            preFilterCap=63,\r\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\r\n        )\r\n\r\n        # Rectification maps (computed from calibration)\r\n        self.map_left = None\r\n        self.map_right = None\r\n        self.Q = None  # Disparity-to-depth mapping matrix\r\n\r\n        if calibration:\r\n            self._compute_rectification_maps()\r\n\r\n    def _compute_rectification_maps(self):\r\n        """Compute stereo rectification maps from calibration."""\r\n        cal = self.calibration\r\n\r\n        # Stereo rectification\r\n        R1, R2, P1, P2, Q, _, _ = cv2.stereoRectify(\r\n            cal.camera_matrix_left,\r\n            cal.dist_coeffs_left,\r\n            cal.camera_matrix_right,\r\n            cal.dist_coeffs_right,\r\n            self.image_size,\r\n            cal.R,\r\n            cal.T,\r\n            flags=cv2.CALIB_ZERO_DISPARITY,\r\n            alpha=0\r\n        )\r\n\r\n        # Compute rectification maps\r\n        self.map_left = cv2.initUndistortRectifyMap(\r\n            cal.camera_matrix_left,\r\n            cal.dist_coeffs_left,\r\n            R1, P1,\r\n            self.image_size,\r\n            cv2.CV_32FC1\r\n        )\r\n\r\n        self.map_right = cv2.initUndistortRectifyMap(\r\n            cal.camera_matrix_right,\r\n            cal.dist_coeffs_right,\r\n            R2, P2,\r\n            self.image_size,\r\n            cv2.CV_32FC1\r\n        )\r\n\r\n        self.Q = Q\r\n\r\n    def rectify(\r\n        self,\r\n        left: np.ndarray,\r\n        right: np.ndarray\r\n    ) -> Tuple[np.ndarray, np.ndarray]:\r\n        """\r\n        Rectify stereo image pair.\r\n\r\n        Args:\r\n            left: Left camera image\r\n            right: Right camera image\r\n\r\n        Returns:\r\n            (rectified_left, rectified_right) tuple\r\n        """\r\n        if self.map_left is None:\r\n            return left, right\r\n\r\n        rect_left = cv2.remap(\r\n            left,\r\n            self.map_left[0],\r\n            self.map_left[1],\r\n            cv2.INTER_LINEAR\r\n        )\r\n\r\n        rect_right = cv2.remap(\r\n            right,\r\n            self.map_right[0],\r\n            self.map_right[1],\r\n            cv2.INTER_LINEAR\r\n        )\r\n\r\n        return rect_left, rect_right\r\n\r\n    def compute_disparity(\r\n        self,\r\n        left: np.ndarray,\r\n        right: np.ndarray\r\n    ) -> np.ndarray:\r\n        """\r\n        Compute disparity map from stereo pair.\r\n\r\n        Args:\r\n            left: Left camera image (grayscale or BGR)\r\n            right: Right camera image (grayscale or BGR)\r\n\r\n        Returns:\r\n            Disparity map (float32, in pixels)\r\n        """\r\n        # Convert to grayscale if needed\r\n        if len(left.shape) == 3:\r\n            left_gray = cv2.cvtColor(left, cv2.COLOR_BGR2GRAY)\r\n            right_gray = cv2.cvtColor(right, cv2.COLOR_BGR2GRAY)\r\n        else:\r\n            left_gray = left\r\n            right_gray = right\r\n\r\n        # Rectify images\r\n        rect_left, rect_right = self.rectify(left_gray, right_gray)\r\n\r\n        # Compute disparity\r\n        disparity = self.stereo_matcher.compute(rect_left, rect_right)\r\n\r\n        # Convert to float (disparity is in fixed-point with 4 fractional bits)\r\n        disparity = disparity.astype(np.float32) / 16.0\r\n\r\n        return disparity\r\n\r\n    def disparity_to_depth(\r\n        self,\r\n        disparity: np.ndarray,\r\n        baseline: Optional[float] = None,\r\n        focal_length: Optional[float] = None\r\n    ) -> np.ndarray:\r\n        """\r\n        Convert disparity map to depth map.\r\n\r\n        Args:\r\n            disparity: Disparity map in pixels\r\n            baseline: Camera baseline in meters (optional if calibrated)\r\n            focal_length: Focal length in pixels (optional if calibrated)\r\n\r\n        Returns:\r\n            Depth map in meters\r\n        """\r\n        # Use calibration values if available\r\n        if baseline is None and self.calibration:\r\n            baseline = self.calibration.baseline\r\n        if focal_length is None and self.calibration:\r\n            focal_length = self.calibration.camera_matrix_left[0, 0]\r\n\r\n        if baseline is None or focal_length is None:\r\n            raise ValueError("Baseline and focal length required for depth computation")\r\n\r\n        # Avoid division by zero\r\n        disparity_safe = np.where(disparity > 0, disparity, 0.1)\r\n\r\n        # depth = baseline * focal_length / disparity\r\n        depth = (baseline * focal_length) / disparity_safe\r\n\r\n        # Mask invalid disparities\r\n        depth = np.where(disparity > 0, depth, 0)\r\n\r\n        return depth\r\n\r\n    def compute_point_cloud(\r\n        self,\r\n        disparity: np.ndarray,\r\n        image: Optional[np.ndarray] = None\r\n    ) -> np.ndarray:\r\n        """\r\n        Generate 3D point cloud from disparity map.\r\n\r\n        Args:\r\n            disparity: Disparity map\r\n            image: Optional RGB image for coloring points\r\n\r\n        Returns:\r\n            Point cloud as Nx3 (or Nx6 with color) array\r\n        """\r\n        if self.Q is None:\r\n            raise ValueError("Calibration required for point cloud generation")\r\n\r\n        # Reproject to 3D\r\n        points_3d = cv2.reprojectImageTo3D(disparity, self.Q)\r\n\r\n        # Create mask for valid points\r\n        mask = disparity > 0\r\n\r\n        # Extract valid points\r\n        points = points_3d[mask]\r\n\r\n        # Add color if image provided\r\n        if image is not None:\r\n            if len(image.shape) == 3:\r\n                colors = image[mask]\r\n            else:\r\n                colors = np.stack([image[mask]] * 3, axis=-1)\r\n            points = np.hstack([points, colors])\r\n\r\n        return points\r\n\r\n\r\ndef visualize_disparity(disparity: np.ndarray) -> np.ndarray:\r\n    """Create colorized visualization of disparity map."""\r\n    # Normalize to 0-255\r\n    disp_vis = disparity.copy()\r\n    disp_vis = np.clip(disp_vis, 0, disp_vis.max())\r\n    disp_vis = (disp_vis / disp_vis.max() * 255).astype(np.uint8)\r\n\r\n    # Apply colormap\r\n    colored = cv2.applyColorMap(disp_vis, cv2.COLORMAP_JET)\r\n\r\n    # Mask invalid regions\r\n    colored[disparity <= 0] = 0\r\n\r\n    return colored\r\n\r\n\r\ndef demo_stereo_depth():\r\n    """Demonstrate stereo depth estimation."""\r\n    # Create synthetic stereo pair\r\n    # In practice, these would come from real stereo cameras\r\n\r\n    # Create left image with objects at different depths\r\n    left = np.zeros((480, 640, 3), dtype=np.uint8)\r\n    cv2.rectangle(left, (100, 100), (200, 300), (255, 255, 255), -1)  # Near\r\n    cv2.rectangle(left, (400, 150), (500, 350), (128, 128, 128), -1)  # Far\r\n\r\n    # Create right image (shifted version simulating stereo)\r\n    right = np.zeros((480, 640, 3), dtype=np.uint8)\r\n    cv2.rectangle(right, (80, 100), (180, 300), (255, 255, 255), -1)   # 20px shift (near)\r\n    cv2.rectangle(right, (395, 150), (495, 350), (128, 128, 128), -1)  # 5px shift (far)\r\n\r\n    # Compute depth\r\n    estimator = StereoDepthEstimator()\r\n    disparity = estimator.compute_disparity(left, right)\r\n\r\n    # Visualize\r\n    vis_disparity = visualize_disparity(disparity)\r\n\r\n    combined = np.vstack([\r\n        np.hstack([left, right]),\r\n        np.hstack([vis_disparity, np.zeros_like(left)])\r\n    ])\r\n\r\n    cv2.putText(combined, \'Left\', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\r\n    cv2.putText(combined, \'Right\', (650, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\r\n    cv2.putText(combined, \'Disparity\', (10, 510), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\r\n\r\n    cv2.imshow(\'Stereo Depth Demo\', combined)\r\n    cv2.waitKey(0)\r\n    cv2.destroyAllWindows()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    demo_stereo_depth()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"visual-odometry",children:"Visual Odometry"}),"\n",(0,i.jsx)(e.p,{children:"Visual odometry estimates robot motion from camera images, essential for navigation when wheel encoders are unreliable."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nVisual odometry for robot pose estimation.\r\n\r\nTracks camera motion through feature matching between\r\nconsecutive frames for localization and mapping.\r\n\r\nPrerequisites:\r\n    pip install opencv-python numpy\r\n"""\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom dataclasses import dataclass, field\r\nfrom typing import List, Tuple, Optional\r\n\r\n\r\n@dataclass\r\nclass Pose:\r\n    """Robot pose in 3D space."""\r\n    position: np.ndarray = field(default_factory=lambda: np.zeros(3))\r\n    rotation: np.ndarray = field(default_factory=lambda: np.eye(3))\r\n\r\n    def transform_matrix(self) -> np.ndarray:\r\n        """Get 4x4 transformation matrix."""\r\n        T = np.eye(4)\r\n        T[:3, :3] = self.rotation\r\n        T[:3, 3] = self.position\r\n        return T\r\n\r\n\r\nclass VisualOdometry:\r\n    """\r\n    Monocular visual odometry using feature tracking.\r\n\r\n    Estimates camera motion by tracking features between\r\n    consecutive frames and computing the essential matrix.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        camera_matrix: np.ndarray,\r\n        dist_coeffs: Optional[np.ndarray] = None\r\n    ):\r\n        """\r\n        Initialize visual odometry.\r\n\r\n        Args:\r\n            camera_matrix: 3x3 camera intrinsic matrix\r\n            dist_coeffs: Distortion coefficients\r\n        """\r\n        self.K = camera_matrix\r\n        self.dist_coeffs = dist_coeffs if dist_coeffs is not None else np.zeros(5)\r\n\r\n        # Feature detector\r\n        self.detector = cv2.ORB_create(nfeatures=2000)\r\n\r\n        # Feature matcher\r\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\r\n\r\n        # State\r\n        self.prev_frame: Optional[np.ndarray] = None\r\n        self.prev_keypoints: Optional[List[cv2.KeyPoint]] = None\r\n        self.prev_descriptors: Optional[np.ndarray] = None\r\n\r\n        # Accumulated pose\r\n        self.pose = Pose()\r\n        self.trajectory: List[np.ndarray] = [self.pose.position.copy()]\r\n\r\n    def process_frame(\r\n        self,\r\n        frame: np.ndarray\r\n    ) -> Tuple[Pose, Optional[np.ndarray]]:\r\n        """\r\n        Process a new frame and update pose estimate.\r\n\r\n        Args:\r\n            frame: New camera frame (BGR or grayscale)\r\n\r\n        Returns:\r\n            (current_pose, visualization) tuple\r\n        """\r\n        # Convert to grayscale\r\n        if len(frame.shape) == 3:\r\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n        else:\r\n            gray = frame.copy()\r\n\r\n        # Undistort\r\n        gray = cv2.undistort(gray, self.K, self.dist_coeffs)\r\n\r\n        # Detect features\r\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\r\n\r\n        vis = None\r\n\r\n        if self.prev_frame is not None and descriptors is not None:\r\n            # Match features\r\n            matches = self._match_features(\r\n                self.prev_descriptors,\r\n                descriptors,\r\n                self.prev_keypoints,\r\n                keypoints\r\n            )\r\n\r\n            if len(matches) >= 8:\r\n                # Estimate motion\r\n                R, t, inliers = self._estimate_motion(matches)\r\n\r\n                if R is not None:\r\n                    # Update pose\r\n                    self.pose.position += self.pose.rotation @ t.ravel()\r\n                    self.pose.rotation = R @ self.pose.rotation\r\n\r\n                    self.trajectory.append(self.pose.position.copy())\r\n\r\n                # Create visualization\r\n                vis = self._visualize_matches(\r\n                    self.prev_frame, gray,\r\n                    self.prev_keypoints, keypoints,\r\n                    matches, inliers\r\n                )\r\n\r\n        # Update state\r\n        self.prev_frame = gray\r\n        self.prev_keypoints = keypoints\r\n        self.prev_descriptors = descriptors\r\n\r\n        return self.pose, vis\r\n\r\n    def _match_features(\r\n        self,\r\n        desc1: np.ndarray,\r\n        desc2: np.ndarray,\r\n        kp1: List[cv2.KeyPoint],\r\n        kp2: List[cv2.KeyPoint]\r\n    ) -> List[Tuple[cv2.KeyPoint, cv2.KeyPoint, cv2.DMatch]]:\r\n        """Match features between frames using ratio test."""\r\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\r\n\r\n        good_matches = []\r\n        for match_pair in matches:\r\n            if len(match_pair) == 2:\r\n                m, n = match_pair\r\n                if m.distance < 0.75 * n.distance:\r\n                    good_matches.append((\r\n                        kp1[m.queryIdx],\r\n                        kp2[m.trainIdx],\r\n                        m\r\n                    ))\r\n\r\n        return good_matches\r\n\r\n    def _estimate_motion(\r\n        self,\r\n        matches: List[Tuple[cv2.KeyPoint, cv2.KeyPoint, cv2.DMatch]]\r\n    ) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:\r\n        """Estimate camera motion from matched features."""\r\n        # Extract point correspondences\r\n        pts1 = np.float32([m[0].pt for m in matches])\r\n        pts2 = np.float32([m[1].pt for m in matches])\r\n\r\n        # Compute essential matrix\r\n        E, mask = cv2.findEssentialMat(\r\n            pts1, pts2, self.K,\r\n            method=cv2.RANSAC,\r\n            prob=0.999,\r\n            threshold=1.0\r\n        )\r\n\r\n        if E is None:\r\n            return None, None, None\r\n\r\n        # Recover pose from essential matrix\r\n        _, R, t, pose_mask = cv2.recoverPose(E, pts1, pts2, self.K, mask)\r\n\r\n        return R, t, pose_mask\r\n\r\n    def _visualize_matches(\r\n        self,\r\n        img1: np.ndarray,\r\n        img2: np.ndarray,\r\n        kp1: List[cv2.KeyPoint],\r\n        kp2: List[cv2.KeyPoint],\r\n        matches: List[Tuple],\r\n        inlier_mask: Optional[np.ndarray]\r\n    ) -> np.ndarray:\r\n        """Create visualization of feature matches."""\r\n        h, w = img1.shape[:2]\r\n        vis = np.zeros((h, w * 2, 3), dtype=np.uint8)\r\n\r\n        # Convert grayscale to color\r\n        vis[:, :w] = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\r\n        vis[:, w:] = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\r\n\r\n        # Draw matches\r\n        for i, (kp_prev, kp_curr, _) in enumerate(matches):\r\n            color = (0, 255, 0) if inlier_mask is None or inlier_mask[i] else (0, 0, 255)\r\n\r\n            pt1 = (int(kp_prev.pt[0]), int(kp_prev.pt[1]))\r\n            pt2 = (int(kp_curr.pt[0]) + w, int(kp_curr.pt[1]))\r\n\r\n            cv2.circle(vis, pt1, 3, color, -1)\r\n            cv2.circle(vis, pt2, 3, color, -1)\r\n            cv2.line(vis, pt1, pt2, color, 1)\r\n\r\n        # Add pose info\r\n        pos = self.pose.position\r\n        cv2.putText(\r\n            vis,\r\n            f"Position: ({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})",\r\n            (10, 30),\r\n            cv2.FONT_HERSHEY_SIMPLEX,\r\n            0.7,\r\n            (255, 255, 255),\r\n            2\r\n        )\r\n\r\n        return vis\r\n\r\n    def get_trajectory_image(self, scale: float = 100.0) -> np.ndarray:\r\n        """Create top-down view of trajectory."""\r\n        # Create image\r\n        img_size = 800\r\n        img = np.zeros((img_size, img_size, 3), dtype=np.uint8)\r\n        center = img_size // 2\r\n\r\n        # Draw trajectory\r\n        for i in range(1, len(self.trajectory)):\r\n            p1 = self.trajectory[i - 1]\r\n            p2 = self.trajectory[i]\r\n\r\n            pt1 = (int(center + p1[0] * scale), int(center - p1[2] * scale))\r\n            pt2 = (int(center + p2[0] * scale), int(center - p2[2] * scale))\r\n\r\n            cv2.line(img, pt1, pt2, (0, 255, 0), 2)\r\n\r\n        # Draw current position\r\n        curr = self.trajectory[-1]\r\n        curr_pt = (int(center + curr[0] * scale), int(center - curr[2] * scale))\r\n        cv2.circle(img, curr_pt, 5, (0, 0, 255), -1)\r\n\r\n        # Draw start\r\n        start_pt = (center, center)\r\n        cv2.circle(img, start_pt, 5, (255, 0, 0), -1)\r\n\r\n        # Labels\r\n        cv2.putText(img, "Start", (center + 10, center), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\r\n        cv2.putText(img, "Current", (curr_pt[0] + 10, curr_pt[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\r\n\r\n        return img\r\n\r\n\r\ndef demo_visual_odometry():\r\n    """Demonstrate visual odometry with webcam."""\r\n    # Approximate camera matrix (adjust for your camera)\r\n    K = np.array([\r\n        [500, 0, 320],\r\n        [0, 500, 240],\r\n        [0, 0, 1]\r\n    ], dtype=np.float64)\r\n\r\n    vo = VisualOdometry(K)\r\n    cap = cv2.VideoCapture(0)\r\n\r\n    print("Visual Odometry Demo")\r\n    print("Move the camera slowly. Press \'q\' to quit.")\r\n\r\n    while True:\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            break\r\n\r\n        pose, vis = vo.process_frame(frame)\r\n\r\n        if vis is not None:\r\n            # Show feature matches\r\n            cv2.imshow(\'Feature Matches\', vis)\r\n\r\n        # Show trajectory\r\n        traj = vo.get_trajectory_image()\r\n        cv2.imshow(\'Trajectory\', traj)\r\n\r\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    demo_visual_odometry()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,i.jsx)(e.p,{children:"Integrating vision pipelines with ROS 2 enables modular, reusable perception systems."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nComplete ROS 2 vision pipeline node.\r\n\r\nIntegrates camera input, feature detection, depth estimation,\r\nand publishes results for downstream navigation and manipulation.\r\n\r\nTopics:\r\n    Subscribed:\r\n        /camera/image_raw (sensor_msgs/Image)\r\n        /camera/depth/image_raw (sensor_msgs/Image)\r\n        /camera/camera_info (sensor_msgs/CameraInfo)\r\n\r\n    Published:\r\n        /vision/features (custom feature message)\r\n        /vision/depth_colorized (sensor_msgs/Image)\r\n        /vision/point_cloud (sensor_msgs/PointCloud2)\r\n\r\nServices:\r\n    /vision/detect_object (custom service)\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, PointField\r\nfrom geometry_msgs.msg import Point\r\nfrom std_msgs.msg import Header\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nfrom typing import Optional\r\nimport struct\r\n\r\n\r\nclass VisionPipelineNode(Node):\r\n    \"\"\"\r\n    Comprehensive vision pipeline for robotic perception.\r\n\r\n    Processes camera data to extract features, compute depth,\r\n    and generate 3D point clouds for navigation and manipulation.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('vision_pipeline_node')\r\n\r\n        # Parameters\r\n        self.declare_parameter('feature_type', 'orb')\r\n        self.declare_parameter('publish_rate', 30.0)\r\n        self.declare_parameter('enable_depth', True)\r\n\r\n        self.feature_type = self.get_parameter('feature_type').value\r\n        self.publish_rate = self.get_parameter('publish_rate').value\r\n        self.enable_depth = self.get_parameter('enable_depth').value\r\n\r\n        # Initialize components\r\n        self.bridge = CvBridge()\r\n        self.detector = cv2.ORB_create(nfeatures=1000)\r\n\r\n        # Camera data\r\n        self.camera_matrix: Optional[np.ndarray] = None\r\n        self.latest_rgb: Optional[np.ndarray] = None\r\n        self.latest_depth: Optional[np.ndarray] = None\r\n\r\n        # Subscribers\r\n        self.rgb_sub = self.create_subscription(\r\n            Image, '/camera/image_raw',\r\n            self.rgb_callback, 10\r\n        )\r\n\r\n        self.depth_sub = self.create_subscription(\r\n            Image, '/camera/depth/image_raw',\r\n            self.depth_callback, 10\r\n        )\r\n\r\n        self.info_sub = self.create_subscription(\r\n            CameraInfo, '/camera/camera_info',\r\n            self.info_callback, 10\r\n        )\r\n\r\n        # Publishers\r\n        self.depth_vis_pub = self.create_publisher(\r\n            Image, '/vision/depth_colorized', 10\r\n        )\r\n\r\n        self.pointcloud_pub = self.create_publisher(\r\n            PointCloud2, '/vision/point_cloud', 10\r\n        )\r\n\r\n        self.features_pub = self.create_publisher(\r\n            Image, '/vision/features', 10\r\n        )\r\n\r\n        # Processing timer\r\n        self.timer = self.create_timer(\r\n            1.0 / self.publish_rate,\r\n            self.process_callback\r\n        )\r\n\r\n        self.get_logger().info('Vision Pipeline Node started')\r\n\r\n    def rgb_callback(self, msg: Image):\r\n        \"\"\"Handle incoming RGB images.\"\"\"\r\n        try:\r\n            self.latest_rgb = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\r\n        except Exception as e:\r\n            self.get_logger().error(f'RGB conversion error: {e}')\r\n\r\n    def depth_callback(self, msg: Image):\r\n        \"\"\"Handle incoming depth images.\"\"\"\r\n        try:\r\n            self.latest_depth = self.bridge.imgmsg_to_cv2(msg, 'passthrough')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Depth conversion error: {e}')\r\n\r\n    def info_callback(self, msg: CameraInfo):\r\n        \"\"\"Store camera calibration.\"\"\"\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n\r\n    def process_callback(self):\r\n        \"\"\"Main processing loop.\"\"\"\r\n        if self.latest_rgb is None:\r\n            return\r\n\r\n        # Detect and visualize features\r\n        gray = cv2.cvtColor(self.latest_rgb, cv2.COLOR_BGR2GRAY)\r\n        keypoints, _ = self.detector.detectAndCompute(gray, None)\r\n\r\n        # Draw features\r\n        features_img = cv2.drawKeypoints(\r\n            self.latest_rgb, keypoints, None,\r\n            color=(0, 255, 0),\r\n            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\r\n        )\r\n\r\n        # Publish feature visualization\r\n        try:\r\n            msg = self.bridge.cv2_to_imgmsg(features_img, 'bgr8')\r\n            self.features_pub.publish(msg)\r\n        except Exception as e:\r\n            self.get_logger().error(f'Feature publish error: {e}')\r\n\r\n        # Process depth if available\r\n        if self.enable_depth and self.latest_depth is not None:\r\n            self.process_depth()\r\n\r\n    def process_depth(self):\r\n        \"\"\"Process and publish depth data.\"\"\"\r\n        # Colorize depth for visualization\r\n        depth_norm = cv2.normalize(\r\n            self.latest_depth, None, 0, 255, cv2.NORM_MINMAX\r\n        )\r\n        depth_color = cv2.applyColorMap(\r\n            depth_norm.astype(np.uint8),\r\n            cv2.COLORMAP_JET\r\n        )\r\n\r\n        # Publish colorized depth\r\n        try:\r\n            msg = self.bridge.cv2_to_imgmsg(depth_color, 'bgr8')\r\n            self.depth_vis_pub.publish(msg)\r\n        except Exception as e:\r\n            self.get_logger().error(f'Depth vis publish error: {e}')\r\n\r\n        # Generate and publish point cloud\r\n        if self.camera_matrix is not None:\r\n            cloud_msg = self.create_point_cloud()\r\n            if cloud_msg:\r\n                self.pointcloud_pub.publish(cloud_msg)\r\n\r\n    def create_point_cloud(self) -> Optional[PointCloud2]:\r\n        \"\"\"Create PointCloud2 message from depth data.\"\"\"\r\n        if self.latest_depth is None or self.camera_matrix is None:\r\n            return None\r\n\r\n        fx = self.camera_matrix[0, 0]\r\n        fy = self.camera_matrix[1, 1]\r\n        cx = self.camera_matrix[0, 2]\r\n        cy = self.camera_matrix[1, 2]\r\n\r\n        h, w = self.latest_depth.shape\r\n\r\n        # Create point cloud\r\n        points = []\r\n\r\n        # Downsample for performance\r\n        step = 4\r\n        for v in range(0, h, step):\r\n            for u in range(0, w, step):\r\n                z = self.latest_depth[v, u]\r\n\r\n                if z <= 0 or z > 10000:  # Invalid or too far\r\n                    continue\r\n\r\n                # Convert to meters if in millimeters\r\n                z_m = z / 1000.0 if z > 100 else z\r\n\r\n                # Back-project to 3D\r\n                x = (u - cx) * z_m / fx\r\n                y = (v - cy) * z_m / fy\r\n\r\n                # Get color\r\n                if self.latest_rgb is not None:\r\n                    b, g, r = self.latest_rgb[v, u]\r\n                else:\r\n                    r = g = b = 255\r\n\r\n                # Pack RGB into single float\r\n                rgb = struct.unpack('f', struct.pack('I', (r << 16) | (g << 8) | b))[0]\r\n\r\n                points.append([x, y, z_m, rgb])\r\n\r\n        if not points:\r\n            return None\r\n\r\n        # Create PointCloud2 message\r\n        header = Header()\r\n        header.frame_id = 'camera_optical_frame'\r\n        header.stamp = self.get_clock().now().to_msg()\r\n\r\n        fields = [\r\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\r\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\r\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),\r\n            PointField(name='rgb', offset=12, datatype=PointField.FLOAT32, count=1),\r\n        ]\r\n\r\n        cloud_data = np.array(points, dtype=np.float32).tobytes()\r\n\r\n        msg = PointCloud2()\r\n        msg.header = header\r\n        msg.height = 1\r\n        msg.width = len(points)\r\n        msg.fields = fields\r\n        msg.is_bigendian = False\r\n        msg.point_step = 16\r\n        msg.row_step = msg.point_step * len(points)\r\n        msg.data = cloud_data\r\n        msg.is_dense = True\r\n\r\n        return msg\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisionPipelineNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"computer-vision-pipeline-architecture",children:"Computer Vision Pipeline Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                Complete Robotic Vision Pipeline                               \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Camera    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Image     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Feature   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Object    \u2502\r\n\u2502   Driver    \u2502     \u2502 Preprocessing\u2502     \u2502  Detection  \u2502     \u2502  Detection  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n      \u2502                   \u2502                   \u2502                    \u2502\r\n      \u2502                   \u2502                   \u2502                    \u2502\r\n      \u25bc                   \u25bc                   \u25bc                    \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Depth     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Stereo    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Visual    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    SLAM     \u2502\r\n\u2502   Sensor    \u2502     \u2502  Matching   \u2502     \u2502  Odometry   \u2502     \u2502   Mapping   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n      \u2502                   \u2502                   \u2502                    \u2502\r\n      \u2502                   \u2502                   \u2502                    \u2502\r\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                    \u2502\r\n                                    \u25bc\r\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                          \u2502   Robot Control \u2502\r\n                          \u2502   Navigation &  \u2502\r\n                          \u2502   Manipulation  \u2502\r\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Vision system architecture"}),": How camera data flows through processing pipelines to robot actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Camera calibration"}),": Computing intrinsics and removing distortion for accurate measurements"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feature detection"}),": Using ORB, SIFT, and other detectors for visual tracking"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Depth estimation"}),": Stereo matching and depth cameras for 3D perception"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Visual odometry"}),": Estimating robot motion from image sequences"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"ROS 2 integration"}),": Building modular vision pipelines with standard interfaces"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["Proper ",(0,i.jsx)(e.strong,{children:"camera calibration"})," is essential for accurate 3D measurements"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feature detection and matching"})," enables tracking, localization, and mapping"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Stereo vision"})," provides dense depth information for navigation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Visual odometry"})," complements wheel encoders for robust localization"]}),"\n",(0,i.jsxs)(e.li,{children:["ROS 2 provides ",(0,i.jsx)(e.strong,{children:"standardized interfaces"})," for building reusable vision components"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(e.h3,{id:"exercise-1-camera-calibration-practice",children:"Exercise 1: Camera Calibration Practice"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Print a checkerboard pattern (9x6 inner corners recommended)"}),"\n",(0,i.jsx)(e.li,{children:"Capture 15-20 images from different angles"}),"\n",(0,i.jsx)(e.li,{children:"Run the calibration script and evaluate reprojection error"}),"\n",(0,i.jsx)(e.li,{children:"Test undistortion on a new image"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"exercise-2-feature-tracking-application",children:"Exercise 2: Feature Tracking Application"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Modify the feature detector to track features across multiple frames"}),"\n",(0,i.jsx)(e.li,{children:"Implement a simple tracker that follows a specific object"}),"\n",(0,i.jsx)(e.li,{children:"Visualize the tracking trajectory over time"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"exercise-3-depth-based-obstacle-detection",children:"Exercise 3: Depth-Based Obstacle Detection"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Use the depth estimation code with a depth camera"}),"\n",(0,i.jsx)(e.li,{children:"Implement a simple obstacle detector (find regions closer than threshold)"}),"\n",(0,i.jsx)(e.li,{children:"Publish obstacle locations as ROS 2 markers"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"challenge-build-a-visual-slam-system",children:"Challenge: Build a Visual SLAM System"}),"\n",(0,i.jsx)(e.p,{children:"Extend the visual odometry code to:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Maintain a map of observed features"}),"\n",(0,i.jsx)(e.li,{children:"Perform loop closure detection"}),"\n",(0,i.jsx)(e.li,{children:"Optimize the trajectory using bundle adjustment"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"up-next",children:"Up Next"}),"\n",(0,i.jsxs)(e.p,{children:["In the ",(0,i.jsx)(e.strong,{children:"Capstone Project"}),", we'll integrate all components from this book to build a complete humanoid robot system that combines ROS 2 communication, simulation, perception, and voice-controlled action."]}),"\n",(0,i.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://docs.opencv.org/",children:"OpenCV Documentation"})," - Comprehensive computer vision library"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://github.com/ros-perception",children:"ROS 2 Vision Packages"})," - Pre-built vision components"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://github.com/UZ-SLAMLab/ORB_SLAM3",children:"ORB-SLAM3"})," - State-of-the-art visual SLAM"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"http://www.open3d.org/",children:"Open3D"})," - 3D data processing library"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://www.robots.ox.ac.uk/~vgg/hzbook/",children:"Multiple View Geometry Book"})," - Foundational computer vision theory"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Sources:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://docs.opencv.org/4.x/d9/df8/tutorial_root.html",children:"OpenCV Tutorials"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://github.com/ros-perception/image_pipeline",children:"ROS 2 Image Pipeline"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://www.mathworks.com/help/vision/ug/camera-calibration.html",children:"Camera Calibration Theory"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2102.04060",children:"Visual SLAM Overview"})}),"\n"]})]})}function d(r={}){const{wrapper:e}={...(0,a.R)(),...r.components};return e?(0,i.jsx)(e,{...r,children:(0,i.jsx)(p,{...r})}):p(r)}},8453:(r,e,n)=>{n.d(e,{R:()=>s,x:()=>o});var t=n(6540);const i={},a=t.createContext(i);function s(r){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof r?r(e):{...e,...r}},[e,r])}function o(r){let e;return e=r.disableParentContext?"function"==typeof r.components?r.components(i):r.components||i:s(r.components),t.createElement(a.Provider,{value:e},r.children)}}}]);