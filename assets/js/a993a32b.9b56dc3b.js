"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[677],{8453:(n,r,e)=>{e.d(r,{R:()=>o,x:()=>a});var i=e(6540);const s={},t=i.createContext(s);function o(n){const r=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(r):{...r,...n}},[r,n])}function a(n){let r;return r=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(t.Provider,{value:r},n.children)}},9804:(n,r,e)=>{e.r(r),e.d(r,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapter11_3d_perception","title":"Chapter 11: 3D Perception & Depth Sensing","description":"Master 3D perception technologies including depth cameras, LiDAR, point cloud processing, and spatial understanding for robotic applications","source":"@site/docs/chapter11_3d_perception.md","sourceDirName":".","slug":"/chapter11_3d_perception","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter11_3d_perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Salwagull/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter11_3d_perception.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"Chapter 11: 3D Perception & Depth Sensing","description":"Master 3D perception technologies including depth cameras, LiDAR, point cloud processing, and spatial understanding for robotic applications","keywords":["3d perception","depth sensing","lidar","point cloud","rgb-d","realsense","spatial understanding","ros2","pcl","open3d"]},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 10: Computer Vision for Robotics","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter10_computer_vision"}}');var s=e(4848),t=e(8453);const o={sidebar_position:11,title:"Chapter 11: 3D Perception & Depth Sensing",description:"Master 3D perception technologies including depth cameras, LiDAR, point cloud processing, and spatial understanding for robotic applications",keywords:["3d perception","depth sensing","lidar","point cloud","rgb-d","realsense","spatial understanding","ros2","pcl","open3d"]},a="Chapter 11: 3D Perception & Depth Sensing",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to 3D Perception",id:"introduction-to-3d-perception",level:2},{value:"Why 3D Perception Matters for Physical AI",id:"why-3d-perception-matters-for-physical-ai",level:3},{value:"Depth Sensing Technologies",id:"depth-sensing-technologies",level:2},{value:"Comparison of Depth Sensors",id:"comparison-of-depth-sensors",level:3},{value:"RGB-D Cameras",id:"rgb-d-cameras",level:3},{value:"LiDAR Sensors",id:"lidar-sensors",level:3},{value:"Point Cloud Processing with Open3D",id:"point-cloud-processing-with-open3d",level:2},{value:"ROS 2 Integration for 3D Perception",id:"ros-2-integration-for-3d-perception",level:2},{value:"Spatial Understanding and Scene Reconstruction",id:"spatial-understanding-and-scene-reconstruction",level:2},{value:"3D Perception Architecture",id:"3d-perception-architecture",level:2},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Depth Camera Calibration",id:"exercise-1-depth-camera-calibration",level:3},{value:"Exercise 2: Ground Plane Removal",id:"exercise-2-ground-plane-removal",level:3},{value:"Exercise 3: Object Detection from Point Clouds",id:"exercise-3-object-detection-from-point-clouds",level:3},{value:"Challenge: Multi-Sensor Fusion",id:"challenge-multi-sensor-fusion",level:3},{value:"Up Next",id:"up-next",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function p(n){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"chapter-11-3d-perception--depth-sensing",children:"Chapter 11: 3D Perception & Depth Sensing"})}),"\n",(0,s.jsx)(r.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(r.p,{children:"By the end of this chapter, you will:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Understand different depth sensing technologies and their trade-offs"}),"\n",(0,s.jsx)(r.li,{children:"Work with RGB-D cameras and LiDAR sensors in ROS 2"}),"\n",(0,s.jsx)(r.li,{children:"Process and filter point cloud data using PCL and Open3D"}),"\n",(0,s.jsx)(r.li,{children:"Implement plane detection, clustering, and object segmentation"}),"\n",(0,s.jsx)(r.li,{children:"Build spatial maps for robot navigation and manipulation"}),"\n",(0,s.jsx)(r.li,{children:"Integrate 3D perception with robot control systems"}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"introduction-to-3d-perception",children:"Introduction to 3D Perception"}),"\n",(0,s.jsxs)(r.p,{children:[(0,s.jsx)(r.strong,{children:"3D perception"})," enables robots to understand the spatial structure of their environment. Unlike 2D images that capture appearance, 3D data provides geometric information essential for physical interaction with the world."]}),"\n",(0,s.jsx)(r.h3,{id:"why-3d-perception-matters-for-physical-ai",children:"Why 3D Perception Matters for Physical AI"}),"\n",(0,s.jsx)(r.p,{children:"Robots operating in the real world must reason about space and geometry to:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Navigate safely"}),": Detect obstacles and plan collision-free paths"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Manipulate objects"}),": Understand object shapes for grasping"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Build maps"}),": Create spatial representations for localization"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Understand scenes"}),": Recognize surfaces, objects, and their relationships"]}),"\n"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                     3D Perception Pipeline                                  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                             \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\r\n\u2502  \u2502    Depth     \u2502    \u2502    Point     \u2502    \u2502   Spatial    \u2502                  \u2502\r\n\u2502  \u2502   Sensors    \u2502\u2500\u2500\u2500\u25b6\u2502    Cloud     \u2502\u2500\u2500\u2500\u25b6\u2502  Analysis    \u2502                  \u2502\r\n\u2502  \u2502 RGB-D/LiDAR  \u2502    \u2502  Generation  \u2502    \u2502              \u2502                  \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\r\n\u2502         \u2502                   \u2502                   \u2502                          \u2502\r\n\u2502         \u2502                   \u2502                   \u2502                          \u2502\r\n\u2502         \u25bc                   \u25bc                   \u25bc                          \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\r\n\u2502  \u2502    Depth     \u2502    \u2502  Filtering   \u2502    \u2502   Plane &    \u2502                  \u2502\r\n\u2502  \u2502     Map      \u2502    \u2502 Downsampling \u2502    \u2502   Object     \u2502                  \u2502\r\n\u2502  \u2502              \u2502    \u2502   Outliers   \u2502    \u2502  Detection   \u2502                  \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\r\n\u2502                                                 \u2502                          \u2502\r\n\u2502                                                 \u25bc                          \u2502\r\n\u2502                                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\r\n\u2502                                          \u2502    Robot     \u2502                  \u2502\r\n\u2502                                          \u2502   Control    \u2502                  \u2502\r\n\u2502                                          \u2502  Navigation  \u2502                  \u2502\r\n\u2502                                          \u2502 Manipulation \u2502                  \u2502\r\n\u2502                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\r\n\u2502                                                                             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(r.h2,{id:"depth-sensing-technologies",children:"Depth Sensing Technologies"}),"\n",(0,s.jsx)(r.p,{children:"Different sensors provide 3D data with varying characteristics suitable for different applications."}),"\n",(0,s.jsx)(r.h3,{id:"comparison-of-depth-sensors",children:"Comparison of Depth Sensors"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Technology"}),(0,s.jsx)(r.th,{children:"Range"}),(0,s.jsx)(r.th,{children:"Resolution"}),(0,s.jsx)(r.th,{children:"Outdoor"}),(0,s.jsx)(r.th,{children:"Cost"}),(0,s.jsx)(r.th,{children:"Best For"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Structured Light"})}),(0,s.jsx)(r.td,{children:"0.2-4m"}),(0,s.jsx)(r.td,{children:"High"}),(0,s.jsx)(r.td,{children:"Poor"}),(0,s.jsx)(r.td,{children:"Low"}),(0,s.jsx)(r.td,{children:"Indoor manipulation"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Time-of-Flight (ToF)"})}),(0,s.jsx)(r.td,{children:"0.1-10m"}),(0,s.jsx)(r.td,{children:"Medium"}),(0,s.jsx)(r.td,{children:"Fair"}),(0,s.jsx)(r.td,{children:"Medium"}),(0,s.jsx)(r.td,{children:"Mobile robots"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Stereo Vision"})}),(0,s.jsx)(r.td,{children:"0.5-20m"}),(0,s.jsx)(r.td,{children:"Variable"}),(0,s.jsx)(r.td,{children:"Good"}),(0,s.jsx)(r.td,{children:"Low"}),(0,s.jsx)(r.td,{children:"Outdoor navigation"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"LiDAR"})}),(0,s.jsx)(r.td,{children:"1-200m"}),(0,s.jsx)(r.td,{children:"High"}),(0,s.jsx)(r.td,{children:"Excellent"}),(0,s.jsx)(r.td,{children:"High"}),(0,s.jsx)(r.td,{children:"Autonomous vehicles"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Active Stereo"})}),(0,s.jsx)(r.td,{children:"0.3-10m"}),(0,s.jsx)(r.td,{children:"High"}),(0,s.jsx)(r.td,{children:"Fair"}),(0,s.jsx)(r.td,{children:"Medium"}),(0,s.jsx)(r.td,{children:"General purpose"})]})]})]}),"\n",(0,s.jsx)(r.h3,{id:"rgb-d-cameras",children:"RGB-D Cameras"}),"\n",(0,s.jsx)(r.p,{children:"RGB-D cameras combine color imaging with depth sensing, providing rich perceptual data."}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nRGB-D camera interface for 3D perception.\r\n\r\nDemonstrates working with Intel RealSense D435/D455 cameras\r\nfor robotic perception applications.\r\n\r\nPrerequisites:\r\n    pip install pyrealsense2 opencv-python numpy\r\n\r\nHardware:\r\n    Intel RealSense D435, D455, or similar RGB-D camera\r\n"""\r\n\r\nimport numpy as np\r\nimport cv2\r\nfrom dataclasses import dataclass\r\nfrom typing import Tuple, Optional\r\nimport time\r\n\r\n# Try to import RealSense SDK\r\ntry:\r\n    import pyrealsense2 as rs\r\n    REALSENSE_AVAILABLE = True\r\nexcept ImportError:\r\n    REALSENSE_AVAILABLE = False\r\n    print("Warning: pyrealsense2 not installed. Using simulated data.")\r\n\r\n\r\n@dataclass\r\nclass DepthFrame:\r\n    """Container for synchronized RGB-D data."""\r\n    color: np.ndarray          # RGB image (H, W, 3)\r\n    depth: np.ndarray          # Depth in meters (H, W)\r\n    depth_raw: np.ndarray      # Raw depth in millimeters (H, W)\r\n    timestamp: float           # Frame timestamp\r\n    intrinsics: dict           # Camera parameters\r\n\r\n\r\nclass RGBDCamera:\r\n    """\r\n    Interface for RGB-D camera sensors.\r\n\r\n    Provides synchronized color and depth streams with\r\n    proper calibration for 3D reconstruction.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        width: int = 640,\r\n        height: int = 480,\r\n        fps: int = 30,\r\n        align_depth: bool = True\r\n    ):\r\n        """\r\n        Initialize RGB-D camera.\r\n\r\n        Args:\r\n            width: Frame width\r\n            height: Frame height\r\n            fps: Frames per second\r\n            align_depth: Align depth to color frame\r\n        """\r\n        self.width = width\r\n        self.height = height\r\n        self.fps = fps\r\n        self.align_depth = align_depth\r\n\r\n        self.pipeline = None\r\n        self.align = None\r\n        self.intrinsics = None\r\n\r\n        if REALSENSE_AVAILABLE:\r\n            self._init_realsense()\r\n        else:\r\n            self._init_simulated()\r\n\r\n    def _init_realsense(self):\r\n        """Initialize Intel RealSense camera."""\r\n        self.pipeline = rs.pipeline()\r\n        config = rs.config()\r\n\r\n        # Enable streams\r\n        config.enable_stream(\r\n            rs.stream.color, self.width, self.height,\r\n            rs.format.bgr8, self.fps\r\n        )\r\n        config.enable_stream(\r\n            rs.stream.depth, self.width, self.height,\r\n            rs.format.z16, self.fps\r\n        )\r\n\r\n        # Start pipeline\r\n        profile = self.pipeline.start(config)\r\n\r\n        # Get depth scale\r\n        depth_sensor = profile.get_device().first_depth_sensor()\r\n        self.depth_scale = depth_sensor.get_depth_scale()\r\n\r\n        # Set up alignment\r\n        if self.align_depth:\r\n            self.align = rs.align(rs.stream.color)\r\n\r\n        # Get intrinsics\r\n        depth_profile = profile.get_stream(rs.stream.depth)\r\n        intr = depth_profile.as_video_stream_profile().get_intrinsics()\r\n\r\n        self.intrinsics = {\r\n            \'fx\': intr.fx,\r\n            \'fy\': intr.fy,\r\n            \'cx\': intr.ppx,\r\n            \'cy\': intr.ppy,\r\n            \'width\': intr.width,\r\n            \'height\': intr.height\r\n        }\r\n\r\n    def _init_simulated(self):\r\n        """Initialize simulated depth data for testing."""\r\n        self.depth_scale = 0.001  # 1mm per unit\r\n\r\n        # Simulated intrinsics\r\n        self.intrinsics = {\r\n            \'fx\': 600.0,\r\n            \'fy\': 600.0,\r\n            \'cx\': self.width / 2,\r\n            \'cy\': self.height / 2,\r\n            \'width\': self.width,\r\n            \'height\': self.height\r\n        }\r\n\r\n    def get_frame(self) -> Optional[DepthFrame]:\r\n        """\r\n        Capture synchronized RGB-D frame.\r\n\r\n        Returns:\r\n            DepthFrame with color and depth data, or None if capture failed\r\n        """\r\n        if REALSENSE_AVAILABLE and self.pipeline:\r\n            return self._get_realsense_frame()\r\n        else:\r\n            return self._get_simulated_frame()\r\n\r\n    def _get_realsense_frame(self) -> Optional[DepthFrame]:\r\n        """Get frame from RealSense camera."""\r\n        frames = self.pipeline.wait_for_frames()\r\n\r\n        if self.align:\r\n            frames = self.align.process(frames)\r\n\r\n        color_frame = frames.get_color_frame()\r\n        depth_frame = frames.get_depth_frame()\r\n\r\n        if not color_frame or not depth_frame:\r\n            return None\r\n\r\n        # Convert to numpy arrays\r\n        color = np.asanyarray(color_frame.get_data())\r\n        depth_raw = np.asanyarray(depth_frame.get_data())\r\n        depth = depth_raw.astype(np.float32) * self.depth_scale\r\n\r\n        return DepthFrame(\r\n            color=color,\r\n            depth=depth,\r\n            depth_raw=depth_raw,\r\n            timestamp=time.time(),\r\n            intrinsics=self.intrinsics\r\n        )\r\n\r\n    def _get_simulated_frame(self) -> DepthFrame:\r\n        """Generate simulated RGB-D data for testing."""\r\n        # Simulated color image\r\n        color = np.zeros((self.height, self.width, 3), dtype=np.uint8)\r\n        cv2.rectangle(color, (200, 150), (440, 330), (0, 255, 0), -1)\r\n        cv2.circle(color, (500, 350), 80, (255, 0, 0), -1)\r\n\r\n        # Simulated depth (objects at different distances)\r\n        depth = np.ones((self.height, self.width), dtype=np.float32) * 3.0\r\n        depth[150:330, 200:440] = 1.5  # Rectangle at 1.5m\r\n        cv2.circle(depth, (500, 350), 80, 1.0, -1)  # Circle at 1.0m\r\n\r\n        depth_raw = (depth / self.depth_scale).astype(np.uint16)\r\n\r\n        return DepthFrame(\r\n            color=color,\r\n            depth=depth,\r\n            depth_raw=depth_raw,\r\n            timestamp=time.time(),\r\n            intrinsics=self.intrinsics\r\n        )\r\n\r\n    def depth_to_pointcloud(\r\n        self,\r\n        depth: np.ndarray,\r\n        color: Optional[np.ndarray] = None\r\n    ) -> np.ndarray:\r\n        """\r\n        Convert depth image to 3D point cloud.\r\n\r\n        Args:\r\n            depth: Depth image in meters (H, W)\r\n            color: Optional RGB image for colored points\r\n\r\n        Returns:\r\n            Point cloud as (N, 3) or (N, 6) array with XYZ [RGB]\r\n        """\r\n        fx = self.intrinsics[\'fx\']\r\n        fy = self.intrinsics[\'fy\']\r\n        cx = self.intrinsics[\'cx\']\r\n        cy = self.intrinsics[\'cy\']\r\n\r\n        # Create pixel coordinate grids\r\n        h, w = depth.shape\r\n        u = np.arange(w)\r\n        v = np.arange(h)\r\n        u, v = np.meshgrid(u, v)\r\n\r\n        # Back-project to 3D\r\n        z = depth\r\n        x = (u - cx) * z / fx\r\n        y = (v - cy) * z / fy\r\n\r\n        # Stack into point cloud\r\n        points = np.stack([x, y, z], axis=-1)\r\n\r\n        # Filter invalid points\r\n        valid = (z > 0.1) & (z < 10.0)\r\n        points = points[valid]\r\n\r\n        if color is not None:\r\n            colors = color[valid]\r\n            points = np.hstack([points, colors])\r\n\r\n        return points\r\n\r\n    def close(self):\r\n        """Release camera resources."""\r\n        if self.pipeline:\r\n            self.pipeline.stop()\r\n\r\n\r\ndef visualize_depth(depth: np.ndarray, max_depth: float = 5.0) -> np.ndarray:\r\n    """Create colorized visualization of depth map."""\r\n    depth_normalized = np.clip(depth / max_depth, 0, 1)\r\n    depth_uint8 = (depth_normalized * 255).astype(np.uint8)\r\n    colorized = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_JET)\r\n    colorized[depth <= 0] = 0  # Mask invalid regions\r\n    return colorized\r\n\r\n\r\ndef demo_rgbd_camera():\r\n    """Demonstrate RGB-D camera usage."""\r\n    camera = RGBDCamera()\r\n\r\n    print("RGB-D Camera Demo")\r\n    print("Press \'q\' to quit, \'s\' to save point cloud")\r\n\r\n    while True:\r\n        frame = camera.get_frame()\r\n        if frame is None:\r\n            continue\r\n\r\n        # Visualize\r\n        depth_vis = visualize_depth(frame.depth)\r\n        combined = np.hstack([frame.color, depth_vis])\r\n\r\n        # Add info overlay\r\n        cv2.putText(\r\n            combined, f"Depth range: {frame.depth[frame.depth > 0].min():.2f}m - {frame.depth.max():.2f}m",\r\n            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2\r\n        )\r\n\r\n        cv2.imshow(\'RGB-D Camera\', combined)\r\n\r\n        key = cv2.waitKey(1) & 0xFF\r\n        if key == ord(\'q\'):\r\n            break\r\n        elif key == ord(\'s\'):\r\n            points = camera.depth_to_pointcloud(frame.depth, frame.color)\r\n            np.save(\'pointcloud.npy\', points)\r\n            print(f"Saved {len(points)} points to pointcloud.npy")\r\n\r\n    camera.close()\r\n    cv2.destroyAllWindows()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    demo_rgbd_camera()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"lidar-sensors",children:"LiDAR Sensors"}),"\n",(0,s.jsx)(r.p,{children:"LiDAR (Light Detection and Ranging) provides precise, long-range 3D measurements."}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nLiDAR interface for 3D perception.\r\n\r\nDemonstrates processing LiDAR point clouds for\r\nrobotic navigation and obstacle detection.\r\n\r\nPrerequisites:\r\n    pip install numpy open3d\r\n\r\nROS 2 Integration:\r\n    Subscribes to sensor_msgs/PointCloud2 topics\r\n"""\r\n\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Tuple, Optional\r\nimport struct\r\n\r\n\r\n@dataclass\r\nclass LiDARScan:\r\n    """Container for LiDAR scan data."""\r\n    points: np.ndarray         # (N, 3) XYZ coordinates\r\n    intensities: np.ndarray    # (N,) Intensity values\r\n    ring_ids: np.ndarray       # (N,) Ring/layer indices\r\n    timestamps: np.ndarray     # (N,) Point timestamps\r\n    frame_id: str              # Coordinate frame\r\n\r\n\r\nclass LiDARProcessor:\r\n    """\r\n    Process LiDAR point cloud data for robotics.\r\n\r\n    Handles common LiDAR processing tasks including\r\n    filtering, ground removal, and obstacle detection.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        min_range: float = 0.5,\r\n        max_range: float = 100.0,\r\n        min_height: float = -2.0,\r\n        max_height: float = 3.0\r\n    ):\r\n        """\r\n        Initialize LiDAR processor.\r\n\r\n        Args:\r\n            min_range: Minimum valid range (meters)\r\n            max_range: Maximum valid range (meters)\r\n            min_height: Minimum height filter (meters)\r\n            max_height: Maximum height filter (meters)\r\n        """\r\n        self.min_range = min_range\r\n        self.max_range = max_range\r\n        self.min_height = min_height\r\n        self.max_height = max_height\r\n\r\n    def filter_by_range(self, points: np.ndarray) -> np.ndarray:\r\n        """\r\n        Filter points by distance from sensor.\r\n\r\n        Args:\r\n            points: (N, 3) point cloud\r\n\r\n        Returns:\r\n            Filtered point cloud\r\n        """\r\n        distances = np.linalg.norm(points[:, :2], axis=1)  # XY distance\r\n        mask = (distances >= self.min_range) & (distances <= self.max_range)\r\n        return points[mask]\r\n\r\n    def filter_by_height(self, points: np.ndarray) -> np.ndarray:\r\n        """\r\n        Filter points by height (Z coordinate).\r\n\r\n        Args:\r\n            points: (N, 3) point cloud\r\n\r\n        Returns:\r\n            Filtered point cloud\r\n        """\r\n        mask = (points[:, 2] >= self.min_height) & (points[:, 2] <= self.max_height)\r\n        return points[mask]\r\n\r\n    def remove_ground_ransac(\r\n        self,\r\n        points: np.ndarray,\r\n        distance_threshold: float = 0.15,\r\n        max_iterations: int = 100\r\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\r\n        """\r\n        Remove ground plane using RANSAC.\r\n\r\n        Args:\r\n            points: (N, 3) point cloud\r\n            distance_threshold: Max distance from plane to be inlier\r\n            max_iterations: RANSAC iterations\r\n\r\n        Returns:\r\n            (non_ground_points, ground_points, plane_coefficients)\r\n        """\r\n        best_inliers = None\r\n        best_plane = None\r\n        n_points = len(points)\r\n\r\n        for _ in range(max_iterations):\r\n            # Sample 3 random points\r\n            indices = np.random.choice(n_points, 3, replace=False)\r\n            sample = points[indices]\r\n\r\n            # Fit plane through 3 points\r\n            v1 = sample[1] - sample[0]\r\n            v2 = sample[2] - sample[0]\r\n            normal = np.cross(v1, v2)\r\n\r\n            if np.linalg.norm(normal) < 1e-6:\r\n                continue\r\n\r\n            normal = normal / np.linalg.norm(normal)\r\n\r\n            # Plane equation: ax + by + cz + d = 0\r\n            d = -np.dot(normal, sample[0])\r\n            plane = np.append(normal, d)\r\n\r\n            # Count inliers\r\n            distances = np.abs(np.dot(points, normal) + d)\r\n            inliers = distances < distance_threshold\r\n\r\n            if best_inliers is None or inliers.sum() > best_inliers.sum():\r\n                best_inliers = inliers\r\n                best_plane = plane\r\n\r\n        if best_inliers is None:\r\n            return points, np.array([]), None\r\n\r\n        ground_points = points[best_inliers]\r\n        non_ground_points = points[~best_inliers]\r\n\r\n        return non_ground_points, ground_points, best_plane\r\n\r\n    def cluster_obstacles(\r\n        self,\r\n        points: np.ndarray,\r\n        eps: float = 0.5,\r\n        min_samples: int = 10\r\n    ) -> List[np.ndarray]:\r\n        """\r\n        Cluster points into distinct obstacles using DBSCAN.\r\n\r\n        Args:\r\n            points: (N, 3) point cloud\r\n            eps: Maximum distance between neighbors\r\n            min_samples: Minimum points to form cluster\r\n\r\n        Returns:\r\n            List of point cloud clusters\r\n        """\r\n        try:\r\n            from sklearn.cluster import DBSCAN\r\n        except ImportError:\r\n            print("sklearn required for clustering")\r\n            return [points]\r\n\r\n        if len(points) < min_samples:\r\n            return []\r\n\r\n        # Cluster using DBSCAN\r\n        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(points)\r\n        labels = clustering.labels_\r\n\r\n        # Extract clusters (ignore noise label -1)\r\n        clusters = []\r\n        unique_labels = set(labels)\r\n\r\n        for label in unique_labels:\r\n            if label == -1:\r\n                continue  # Skip noise\r\n            cluster_mask = labels == label\r\n            clusters.append(points[cluster_mask])\r\n\r\n        return clusters\r\n\r\n    def compute_obstacle_bboxes(\r\n        self,\r\n        clusters: List[np.ndarray]\r\n    ) -> List[dict]:\r\n        """\r\n        Compute 3D bounding boxes for obstacle clusters.\r\n\r\n        Args:\r\n            clusters: List of point cloud clusters\r\n\r\n        Returns:\r\n            List of bounding box dictionaries\r\n        """\r\n        bboxes = []\r\n\r\n        for cluster in clusters:\r\n            if len(cluster) < 3:\r\n                continue\r\n\r\n            # Axis-aligned bounding box\r\n            min_pt = cluster.min(axis=0)\r\n            max_pt = cluster.max(axis=0)\r\n            center = (min_pt + max_pt) / 2\r\n            dimensions = max_pt - min_pt\r\n\r\n            bboxes.append({\r\n                \'center\': center,\r\n                \'dimensions\': dimensions,\r\n                \'min_point\': min_pt,\r\n                \'max_point\': max_pt,\r\n                \'num_points\': len(cluster)\r\n            })\r\n\r\n        return bboxes\r\n\r\n    def process_scan(\r\n        self,\r\n        points: np.ndarray\r\n    ) -> dict:\r\n        """\r\n        Full processing pipeline for LiDAR scan.\r\n\r\n        Args:\r\n            points: (N, 3) raw point cloud\r\n\r\n        Returns:\r\n            Dictionary with processed results\r\n        """\r\n        # Apply filters\r\n        filtered = self.filter_by_range(points)\r\n        filtered = self.filter_by_height(filtered)\r\n\r\n        # Remove ground\r\n        obstacles, ground, plane = self.remove_ground_ransac(filtered)\r\n\r\n        # Cluster obstacles\r\n        clusters = self.cluster_obstacles(obstacles)\r\n\r\n        # Compute bounding boxes\r\n        bboxes = self.compute_obstacle_bboxes(clusters)\r\n\r\n        return {\r\n            \'filtered_points\': filtered,\r\n            \'obstacle_points\': obstacles,\r\n            \'ground_points\': ground,\r\n            \'ground_plane\': plane,\r\n            \'clusters\': clusters,\r\n            \'bounding_boxes\': bboxes\r\n        }\r\n\r\n\r\ndef generate_simulated_lidar_scan(\r\n    num_rings: int = 16,\r\n    points_per_ring: int = 1800,\r\n    max_range: float = 50.0\r\n) -> np.ndarray:\r\n    """\r\n    Generate simulated LiDAR scan data for testing.\r\n\r\n    Args:\r\n        num_rings: Number of vertical rings\r\n        points_per_ring: Horizontal resolution\r\n        max_range: Maximum range\r\n\r\n    Returns:\r\n        (N, 3) point cloud\r\n    """\r\n    points = []\r\n\r\n    # Vertical angles (typical 16-beam LiDAR)\r\n    vertical_angles = np.linspace(-15, 15, num_rings) * np.pi / 180\r\n\r\n    for v_angle in vertical_angles:\r\n        # Horizontal sweep\r\n        h_angles = np.linspace(0, 2 * np.pi, points_per_ring, endpoint=False)\r\n\r\n        for h_angle in h_angles:\r\n            # Simulate ground plane\r\n            if v_angle < 0:\r\n                # Range to ground at this angle\r\n                ground_height = -1.5  # Sensor 1.5m above ground\r\n                range_to_ground = abs(ground_height / np.sin(v_angle))\r\n\r\n                if range_to_ground < max_range:\r\n                    x = range_to_ground * np.cos(v_angle) * np.cos(h_angle)\r\n                    y = range_to_ground * np.cos(v_angle) * np.sin(h_angle)\r\n                    z = range_to_ground * np.sin(v_angle)\r\n                    points.append([x, y, z])\r\n\r\n            # Add some obstacles\r\n            # Obstacle 1: Wall-like structure\r\n            if 0.3 < h_angle < 0.8:\r\n                wall_dist = 5.0\r\n                x = wall_dist * np.cos(h_angle)\r\n                y = wall_dist * np.sin(h_angle)\r\n                z = np.sin(v_angle) * wall_dist + 0.5\r\n                if -1 < z < 2:\r\n                    points.append([x, y, z])\r\n\r\n            # Obstacle 2: Cylindrical object\r\n            obstacle_pos = np.array([3.0, -2.0])\r\n            obstacle_radius = 0.5\r\n            ray_dir = np.array([np.cos(h_angle), np.sin(h_angle)])\r\n\r\n            # Ray-cylinder intersection\r\n            a = np.dot(ray_dir, ray_dir)\r\n            b = 2 * np.dot(ray_dir, -obstacle_pos)\r\n            c = np.dot(obstacle_pos, obstacle_pos) - obstacle_radius**2\r\n            discriminant = b**2 - 4*a*c\r\n\r\n            if discriminant > 0:\r\n                t = (-b - np.sqrt(discriminant)) / (2*a)\r\n                if 0 < t < max_range:\r\n                    x = t * np.cos(h_angle)\r\n                    y = t * np.sin(h_angle)\r\n                    z = t * np.sin(v_angle)\r\n                    if -0.5 < z < 1.5:\r\n                        points.append([x, y, z])\r\n\r\n    return np.array(points)\r\n\r\n\r\ndef demo_lidar_processor():\r\n    """Demonstrate LiDAR processing pipeline."""\r\n    print("LiDAR Processing Demo")\r\n    print("=" * 50)\r\n\r\n    # Generate simulated data\r\n    print("Generating simulated LiDAR scan...")\r\n    raw_points = generate_simulated_lidar_scan()\r\n    print(f"Raw points: {len(raw_points)}")\r\n\r\n    # Process scan\r\n    processor = LiDARProcessor()\r\n    results = processor.process_scan(raw_points)\r\n\r\n    print(f"\\nProcessing Results:")\r\n    print(f"  Filtered points: {len(results[\'filtered_points\'])}")\r\n    print(f"  Ground points: {len(results[\'ground_points\'])}")\r\n    print(f"  Obstacle points: {len(results[\'obstacle_points\'])}")\r\n    print(f"  Detected clusters: {len(results[\'clusters\'])}")\r\n\r\n    print(f"\\nBounding Boxes:")\r\n    for i, bbox in enumerate(results[\'bounding_boxes\']):\r\n        print(f"  Obstacle {i+1}:")\r\n        print(f"    Center: ({bbox[\'center\'][0]:.2f}, {bbox[\'center\'][1]:.2f}, {bbox[\'center\'][2]:.2f})")\r\n        print(f"    Size: ({bbox[\'dimensions\'][0]:.2f}, {bbox[\'dimensions\'][1]:.2f}, {bbox[\'dimensions\'][2]:.2f})")\r\n        print(f"    Points: {bbox[\'num_points\']}")\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    demo_lidar_processor()\n'})}),"\n",(0,s.jsx)(r.h2,{id:"point-cloud-processing-with-open3d",children:"Point Cloud Processing with Open3D"}),"\n",(0,s.jsx)(r.p,{children:"Open3D provides efficient algorithms for point cloud manipulation and analysis."}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nAdvanced point cloud processing using Open3D.\r\n\r\nDemonstrates filtering, registration, and surface\r\nreconstruction for 3D perception applications.\r\n\r\nPrerequisites:\r\n    pip install open3d numpy\r\n"""\r\n\r\nimport numpy as np\r\nfrom typing import Tuple, Optional, List\r\n\r\ntry:\r\n    import open3d as o3d\r\n    OPEN3D_AVAILABLE = True\r\nexcept ImportError:\r\n    OPEN3D_AVAILABLE = False\r\n    print("Warning: Open3D not installed. Some features unavailable.")\r\n\r\n\r\nclass PointCloudProcessor:\r\n    """\r\n    Advanced point cloud processing using Open3D.\r\n\r\n    Provides filtering, registration, segmentation,\r\n    and surface reconstruction capabilities.\r\n    """\r\n\r\n    def __init__(self):\r\n        """Initialize point cloud processor."""\r\n        if not OPEN3D_AVAILABLE:\r\n            raise RuntimeError("Open3D required. Install with: pip install open3d")\r\n\r\n    @staticmethod\r\n    def from_numpy(points: np.ndarray, colors: Optional[np.ndarray] = None) -> \'o3d.geometry.PointCloud\':\r\n        """\r\n        Create Open3D point cloud from numpy array.\r\n\r\n        Args:\r\n            points: (N, 3) XYZ coordinates\r\n            colors: Optional (N, 3) RGB colors [0-1]\r\n\r\n        Returns:\r\n            Open3D PointCloud object\r\n        """\r\n        pcd = o3d.geometry.PointCloud()\r\n        pcd.points = o3d.utility.Vector3dVector(points)\r\n\r\n        if colors is not None:\r\n            # Normalize colors if needed\r\n            if colors.max() > 1.0:\r\n                colors = colors / 255.0\r\n            pcd.colors = o3d.utility.Vector3dVector(colors)\r\n\r\n        return pcd\r\n\r\n    @staticmethod\r\n    def to_numpy(pcd: \'o3d.geometry.PointCloud\') -> Tuple[np.ndarray, Optional[np.ndarray]]:\r\n        """Convert Open3D point cloud to numpy arrays."""\r\n        points = np.asarray(pcd.points)\r\n        colors = np.asarray(pcd.colors) if pcd.has_colors() else None\r\n        return points, colors\r\n\r\n    def voxel_downsample(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\',\r\n        voxel_size: float = 0.05\r\n    ) -> \'o3d.geometry.PointCloud\':\r\n        """\r\n        Downsample point cloud using voxel grid.\r\n\r\n        Args:\r\n            pcd: Input point cloud\r\n            voxel_size: Voxel edge length (meters)\r\n\r\n        Returns:\r\n            Downsampled point cloud\r\n        """\r\n        return pcd.voxel_down_sample(voxel_size)\r\n\r\n    def remove_outliers_statistical(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\',\r\n        nb_neighbors: int = 20,\r\n        std_ratio: float = 2.0\r\n    ) -> Tuple[\'o3d.geometry.PointCloud\', np.ndarray]:\r\n        """\r\n        Remove outliers using statistical analysis.\r\n\r\n        Args:\r\n            pcd: Input point cloud\r\n            nb_neighbors: Number of neighbors for analysis\r\n            std_ratio: Standard deviation threshold\r\n\r\n        Returns:\r\n            (filtered_pcd, inlier_indices)\r\n        """\r\n        filtered, indices = pcd.remove_statistical_outlier(\r\n            nb_neighbors=nb_neighbors,\r\n            std_ratio=std_ratio\r\n        )\r\n        return filtered, np.asarray(indices)\r\n\r\n    def remove_outliers_radius(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\',\r\n        nb_points: int = 16,\r\n        radius: float = 0.1\r\n    ) -> Tuple[\'o3d.geometry.PointCloud\', np.ndarray]:\r\n        """\r\n        Remove outliers using radius search.\r\n\r\n        Args:\r\n            pcd: Input point cloud\r\n            nb_points: Minimum neighbors in radius\r\n            radius: Search radius (meters)\r\n\r\n        Returns:\r\n            (filtered_pcd, inlier_indices)\r\n        """\r\n        filtered, indices = pcd.remove_radius_outlier(\r\n            nb_points=nb_points,\r\n            radius=radius\r\n        )\r\n        return filtered, np.asarray(indices)\r\n\r\n    def estimate_normals(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\',\r\n        search_radius: float = 0.1,\r\n        max_nn: int = 30\r\n    ) -> None:\r\n        """\r\n        Estimate surface normals for each point.\r\n\r\n        Args:\r\n            pcd: Point cloud (modified in place)\r\n            search_radius: KNN search radius\r\n            max_nn: Maximum neighbors to consider\r\n        """\r\n        pcd.estimate_normals(\r\n            search_param=o3d.geometry.KDTreeSearchParamHybrid(\r\n                radius=search_radius,\r\n                max_nn=max_nn\r\n            )\r\n        )\r\n        # Orient normals consistently\r\n        pcd.orient_normals_consistent_tangent_plane(k=15)\r\n\r\n    def segment_plane(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\',\r\n        distance_threshold: float = 0.02,\r\n        ransac_n: int = 3,\r\n        num_iterations: int = 1000\r\n    ) -> Tuple[np.ndarray, List[int]]:\r\n        """\r\n        Segment the dominant plane (e.g., ground, table).\r\n\r\n        Args:\r\n            pcd: Input point cloud\r\n            distance_threshold: RANSAC distance threshold\r\n            ransac_n: Points to sample per iteration\r\n            num_iterations: RANSAC iterations\r\n\r\n        Returns:\r\n            (plane_model, inlier_indices)\r\n            plane_model: [a, b, c, d] where ax + by + cz + d = 0\r\n        """\r\n        plane_model, inliers = pcd.segment_plane(\r\n            distance_threshold=distance_threshold,\r\n            ransac_n=ransac_n,\r\n            num_iterations=num_iterations\r\n        )\r\n        return np.array(plane_model), list(inliers)\r\n\r\n    def cluster_dbscan(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\',\r\n        eps: float = 0.05,\r\n        min_points: int = 10\r\n    ) -> List[\'o3d.geometry.PointCloud\']:\r\n        """\r\n        Cluster point cloud using DBSCAN.\r\n\r\n        Args:\r\n            pcd: Input point cloud\r\n            eps: Cluster distance threshold\r\n            min_points: Minimum points per cluster\r\n\r\n        Returns:\r\n            List of clustered point clouds\r\n        """\r\n        labels = np.array(pcd.cluster_dbscan(\r\n            eps=eps,\r\n            min_points=min_points\r\n        ))\r\n\r\n        max_label = labels.max()\r\n        clusters = []\r\n\r\n        for i in range(max_label + 1):\r\n            mask = labels == i\r\n            cluster = pcd.select_by_index(np.where(mask)[0])\r\n            clusters.append(cluster)\r\n\r\n        return clusters\r\n\r\n    def compute_convex_hull(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\'\r\n    ) -> \'o3d.geometry.TriangleMesh\':\r\n        """\r\n        Compute convex hull of point cloud.\r\n\r\n        Args:\r\n            pcd: Input point cloud\r\n\r\n        Returns:\r\n            Convex hull as triangle mesh\r\n        """\r\n        hull, _ = pcd.compute_convex_hull()\r\n        return hull\r\n\r\n    def surface_reconstruction_poisson(\r\n        self,\r\n        pcd: \'o3d.geometry.PointCloud\',\r\n        depth: int = 9\r\n    ) -> \'o3d.geometry.TriangleMesh\':\r\n        """\r\n        Reconstruct surface using Poisson reconstruction.\r\n\r\n        Requires normals to be computed first.\r\n\r\n        Args:\r\n            pcd: Point cloud with normals\r\n            depth: Octree depth (higher = more detail)\r\n\r\n        Returns:\r\n            Reconstructed triangle mesh\r\n        """\r\n        if not pcd.has_normals():\r\n            self.estimate_normals(pcd)\r\n\r\n        mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\r\n            pcd, depth=depth\r\n        )\r\n\r\n        # Remove low-density vertices (outlier artifacts)\r\n        densities = np.asarray(densities)\r\n        threshold = np.quantile(densities, 0.01)\r\n        vertices_to_remove = densities < threshold\r\n        mesh.remove_vertices_by_mask(vertices_to_remove)\r\n\r\n        return mesh\r\n\r\n    def icp_registration(\r\n        self,\r\n        source: \'o3d.geometry.PointCloud\',\r\n        target: \'o3d.geometry.PointCloud\',\r\n        init_transform: Optional[np.ndarray] = None,\r\n        max_distance: float = 0.1\r\n    ) -> Tuple[np.ndarray, float]:\r\n        """\r\n        Align two point clouds using ICP.\r\n\r\n        Args:\r\n            source: Source point cloud (to be transformed)\r\n            target: Target point cloud (reference)\r\n            init_transform: Initial transformation guess\r\n            max_distance: Max correspondence distance\r\n\r\n        Returns:\r\n            (transformation_matrix, fitness_score)\r\n        """\r\n        if init_transform is None:\r\n            init_transform = np.eye(4)\r\n\r\n        reg = o3d.pipelines.registration.registration_icp(\r\n            source, target, max_distance, init_transform,\r\n            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\r\n            o3d.pipelines.registration.ICPConvergenceCriteria(\r\n                max_iteration=50\r\n            )\r\n        )\r\n\r\n        return reg.transformation, reg.fitness\r\n\r\n\r\ndef demo_point_cloud_processing():\r\n    """Demonstrate Open3D point cloud processing."""\r\n    if not OPEN3D_AVAILABLE:\r\n        print("Open3D not available")\r\n        return\r\n\r\n    print("Point Cloud Processing Demo")\r\n    print("=" * 50)\r\n\r\n    processor = PointCloudProcessor()\r\n\r\n    # Create sample point cloud (table with objects)\r\n    # Table surface\r\n    table_points = np.random.uniform(\r\n        [-0.5, -0.3, 0],\r\n        [0.5, 0.3, 0.01],\r\n        (5000, 3)\r\n    )\r\n\r\n    # Object 1: Box on table\r\n    box_points = np.random.uniform(\r\n        [-0.1, -0.1, 0.01],\r\n        [0.1, 0.1, 0.15],\r\n        (2000, 3)\r\n    )\r\n\r\n    # Object 2: Cylinder\r\n    theta = np.random.uniform(0, 2*np.pi, 1500)\r\n    r = 0.05\r\n    cylinder_x = 0.3 + r * np.cos(theta)\r\n    cylinder_y = 0.0 + r * np.sin(theta)\r\n    cylinder_z = np.random.uniform(0.01, 0.2, 1500)\r\n    cylinder_points = np.stack([cylinder_x, cylinder_y, cylinder_z], axis=1)\r\n\r\n    # Combine all points\r\n    all_points = np.vstack([table_points, box_points, cylinder_points])\r\n\r\n    # Add noise\r\n    all_points += np.random.normal(0, 0.002, all_points.shape)\r\n\r\n    print(f"Total points: {len(all_points)}")\r\n\r\n    # Create Open3D point cloud\r\n    pcd = processor.from_numpy(all_points)\r\n\r\n    # Downsample\r\n    pcd_down = processor.voxel_downsample(pcd, voxel_size=0.01)\r\n    print(f"After downsampling: {len(np.asarray(pcd_down.points))}")\r\n\r\n    # Remove outliers\r\n    pcd_filtered, _ = processor.remove_outliers_statistical(pcd_down)\r\n    print(f"After outlier removal: {len(np.asarray(pcd_filtered.points))}")\r\n\r\n    # Segment plane (table)\r\n    plane_model, inliers = processor.segment_plane(pcd_filtered)\r\n    print(f"\\nPlane detected: {plane_model}")\r\n    print(f"Plane inliers: {len(inliers)}")\r\n\r\n    # Extract objects (non-plane points)\r\n    points_np, _ = processor.to_numpy(pcd_filtered)\r\n    outlier_mask = np.ones(len(points_np), dtype=bool)\r\n    outlier_mask[inliers] = False\r\n    object_points = points_np[outlier_mask]\r\n\r\n    print(f"Object points: {len(object_points)}")\r\n\r\n    # Cluster objects\r\n    object_pcd = processor.from_numpy(object_points)\r\n    clusters = processor.cluster_dbscan(object_pcd, eps=0.05, min_points=50)\r\n\r\n    print(f"\\nDetected objects: {len(clusters)}")\r\n    for i, cluster in enumerate(clusters):\r\n        cluster_points, _ = processor.to_numpy(cluster)\r\n        bbox = cluster.get_axis_aligned_bounding_box()\r\n        print(f"  Object {i+1}: {len(cluster_points)} points")\r\n        print(f"    Size: {bbox.get_extent()}")\r\n\r\n    # Visualize (if display available)\r\n    try:\r\n        # Color the point cloud\r\n        pcd_filtered.paint_uniform_color([0.7, 0.7, 0.7])\r\n\r\n        # Color clusters\r\n        colors = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]]\r\n        for i, cluster in enumerate(clusters):\r\n            cluster.paint_uniform_color(colors[i % len(colors)])\r\n\r\n        print("\\nVisualization window opened (close to continue)")\r\n        o3d.visualization.draw_geometries([pcd_filtered] + clusters)\r\n    except Exception as e:\r\n        print(f"Visualization skipped: {e}")\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    demo_point_cloud_processing()\n'})}),"\n",(0,s.jsx)(r.h2,{id:"ros-2-integration-for-3d-perception",children:"ROS 2 Integration for 3D Perception"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nROS 2 node for 3D perception and depth sensing.\r\n\r\nIntegrates depth cameras and LiDAR for comprehensive\r\nspatial perception in robotic systems.\r\n\r\nTopics:\r\n    Subscribed:\r\n        /camera/depth/image_raw (sensor_msgs/Image)\r\n        /camera/depth/camera_info (sensor_msgs/CameraInfo)\r\n        /velodyne_points (sensor_msgs/PointCloud2)\r\n\r\n    Published:\r\n        /perception_3d/obstacles (visualization_msgs/MarkerArray)\r\n        /perception_3d/ground_plane (visualization_msgs/Marker)\r\n        /perception_3d/processed_cloud (sensor_msgs/PointCloud2)\r\n\r\nServices:\r\n    /perception_3d/get_obstacle_positions\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, PointField\r\nfrom visualization_msgs.msg import Marker, MarkerArray\r\nfrom geometry_msgs.msg import Point, Vector3\r\nfrom std_msgs.msg import ColorRGBA, Header\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nfrom typing import List, Optional\r\nimport struct\r\n\r\n\r\nclass Perception3DNode(Node):\r\n    \"\"\"\r\n    Comprehensive 3D perception node for robotics.\r\n\r\n    Processes depth camera and LiDAR data to detect\r\n    obstacles, ground plane, and spatial features.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('perception_3d_node')\r\n\r\n        # Parameters\r\n        self.declare_parameter('depth_max_range', 5.0)\r\n        self.declare_parameter('ground_height_threshold', 0.1)\r\n        self.declare_parameter('obstacle_min_height', 0.1)\r\n        self.declare_parameter('cluster_tolerance', 0.1)\r\n        self.declare_parameter('min_cluster_size', 50)\r\n\r\n        self.max_range = self.get_parameter('depth_max_range').value\r\n        self.ground_threshold = self.get_parameter('ground_height_threshold').value\r\n        self.min_obstacle_height = self.get_parameter('obstacle_min_height').value\r\n        self.cluster_tolerance = self.get_parameter('cluster_tolerance').value\r\n        self.min_cluster_size = self.get_parameter('min_cluster_size').value\r\n\r\n        # State\r\n        self.bridge = CvBridge()\r\n        self.camera_info: Optional[CameraInfo] = None\r\n        self.obstacles: List[dict] = []\r\n\r\n        # Subscribers\r\n        self.depth_sub = self.create_subscription(\r\n            Image, '/camera/depth/image_raw',\r\n            self.depth_callback, 10\r\n        )\r\n\r\n        self.info_sub = self.create_subscription(\r\n            CameraInfo, '/camera/depth/camera_info',\r\n            self.camera_info_callback, 10\r\n        )\r\n\r\n        self.lidar_sub = self.create_subscription(\r\n            PointCloud2, '/velodyne_points',\r\n            self.lidar_callback, 10\r\n        )\r\n\r\n        # Publishers\r\n        self.obstacle_marker_pub = self.create_publisher(\r\n            MarkerArray, '/perception_3d/obstacles', 10\r\n        )\r\n\r\n        self.ground_marker_pub = self.create_publisher(\r\n            Marker, '/perception_3d/ground_plane', 10\r\n        )\r\n\r\n        self.processed_cloud_pub = self.create_publisher(\r\n            PointCloud2, '/perception_3d/processed_cloud', 10\r\n        )\r\n\r\n        self.get_logger().info('3D Perception Node initialized')\r\n\r\n    def camera_info_callback(self, msg: CameraInfo):\r\n        \"\"\"Store camera intrinsics.\"\"\"\r\n        self.camera_info = msg\r\n\r\n    def depth_callback(self, msg: Image):\r\n        \"\"\"Process depth image from RGB-D camera.\"\"\"\r\n        try:\r\n            # Convert depth image\r\n            depth = self.bridge.imgmsg_to_cv2(msg, 'passthrough')\r\n\r\n            # Convert to meters if needed\r\n            if depth.dtype == np.uint16:\r\n                depth = depth.astype(np.float32) / 1000.0\r\n\r\n            # Generate point cloud\r\n            if self.camera_info:\r\n                points = self.depth_to_points(depth)\r\n\r\n                # Process for obstacles\r\n                self.process_point_cloud(points, msg.header)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Depth processing error: {e}')\r\n\r\n    def depth_to_points(self, depth: np.ndarray) -> np.ndarray:\r\n        \"\"\"Convert depth image to 3D points.\"\"\"\r\n        if self.camera_info is None:\r\n            return np.array([])\r\n\r\n        fx = self.camera_info.k[0]\r\n        fy = self.camera_info.k[4]\r\n        cx = self.camera_info.k[2]\r\n        cy = self.camera_info.k[5]\r\n\r\n        h, w = depth.shape\r\n        u = np.arange(w)\r\n        v = np.arange(h)\r\n        u, v = np.meshgrid(u, v)\r\n\r\n        z = depth\r\n        x = (u - cx) * z / fx\r\n        y = (v - cy) * z / fy\r\n\r\n        points = np.stack([x, y, z], axis=-1)\r\n\r\n        # Filter valid points\r\n        valid = (z > 0.1) & (z < self.max_range)\r\n        return points[valid]\r\n\r\n    def lidar_callback(self, msg: PointCloud2):\r\n        \"\"\"Process LiDAR point cloud.\"\"\"\r\n        points = self.pointcloud2_to_numpy(msg)\r\n        self.process_point_cloud(points, msg.header)\r\n\r\n    def pointcloud2_to_numpy(self, msg: PointCloud2) -> np.ndarray:\r\n        \"\"\"Convert PointCloud2 message to numpy array.\"\"\"\r\n        # Find XYZ field offsets\r\n        x_offset = y_offset = z_offset = None\r\n\r\n        for field in msg.fields:\r\n            if field.name == 'x':\r\n                x_offset = field.offset\r\n            elif field.name == 'y':\r\n                y_offset = field.offset\r\n            elif field.name == 'z':\r\n                z_offset = field.offset\r\n\r\n        if None in [x_offset, y_offset, z_offset]:\r\n            return np.array([])\r\n\r\n        # Parse points\r\n        points = []\r\n        for i in range(0, len(msg.data), msg.point_step):\r\n            x = struct.unpack_from('f', msg.data, i + x_offset)[0]\r\n            y = struct.unpack_from('f', msg.data, i + y_offset)[0]\r\n            z = struct.unpack_from('f', msg.data, i + z_offset)[0]\r\n\r\n            if not np.isnan(x) and not np.isinf(x):\r\n                points.append([x, y, z])\r\n\r\n        return np.array(points)\r\n\r\n    def process_point_cloud(self, points: np.ndarray, header: Header):\r\n        \"\"\"Process point cloud for obstacle detection.\"\"\"\r\n        if len(points) == 0:\r\n            return\r\n\r\n        # Filter by height (remove ground)\r\n        non_ground_mask = points[:, 2] > self.ground_threshold\r\n        obstacles_points = points[non_ground_mask]\r\n\r\n        # Simple clustering (in production, use DBSCAN or similar)\r\n        obstacles = self.simple_cluster(obstacles_points)\r\n\r\n        # Publish markers\r\n        self.publish_obstacle_markers(obstacles, header)\r\n\r\n        # Publish processed cloud\r\n        self.publish_point_cloud(obstacles_points, header)\r\n\r\n    def simple_cluster(self, points: np.ndarray) -> List[dict]:\r\n        \"\"\"Simple grid-based clustering for obstacles.\"\"\"\r\n        if len(points) == 0:\r\n            return []\r\n\r\n        # Grid-based clustering\r\n        grid_size = self.cluster_tolerance\r\n        grid_points = np.floor(points[:, :2] / grid_size).astype(int)\r\n\r\n        # Group by grid cell\r\n        clusters = {}\r\n        for i, cell in enumerate(grid_points):\r\n            key = (cell[0], cell[1])\r\n            if key not in clusters:\r\n                clusters[key] = []\r\n            clusters[key].append(points[i])\r\n\r\n        # Convert to obstacle list\r\n        obstacles = []\r\n        for cell, pts in clusters.items():\r\n            pts = np.array(pts)\r\n            if len(pts) >= self.min_cluster_size:\r\n                center = pts.mean(axis=0)\r\n                size = pts.max(axis=0) - pts.min(axis=0)\r\n\r\n                if size[2] > self.min_obstacle_height:\r\n                    obstacles.append({\r\n                        'center': center,\r\n                        'size': size,\r\n                        'num_points': len(pts)\r\n                    })\r\n\r\n        return obstacles\r\n\r\n    def publish_obstacle_markers(self, obstacles: List[dict], header: Header):\r\n        \"\"\"Publish obstacle bounding box markers.\"\"\"\r\n        marker_array = MarkerArray()\r\n\r\n        for i, obs in enumerate(obstacles):\r\n            marker = Marker()\r\n            marker.header = header\r\n            marker.ns = 'obstacles'\r\n            marker.id = i\r\n            marker.type = Marker.CUBE\r\n            marker.action = Marker.ADD\r\n\r\n            marker.pose.position.x = float(obs['center'][0])\r\n            marker.pose.position.y = float(obs['center'][1])\r\n            marker.pose.position.z = float(obs['center'][2])\r\n            marker.pose.orientation.w = 1.0\r\n\r\n            marker.scale.x = float(max(obs['size'][0], 0.1))\r\n            marker.scale.y = float(max(obs['size'][1], 0.1))\r\n            marker.scale.z = float(max(obs['size'][2], 0.1))\r\n\r\n            marker.color = ColorRGBA(r=1.0, g=0.0, b=0.0, a=0.5)\r\n            marker.lifetime.sec = 1\r\n\r\n            marker_array.markers.append(marker)\r\n\r\n        self.obstacle_marker_pub.publish(marker_array)\r\n\r\n    def publish_point_cloud(self, points: np.ndarray, header: Header):\r\n        \"\"\"Publish processed point cloud.\"\"\"\r\n        if len(points) == 0:\r\n            return\r\n\r\n        fields = [\r\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\r\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\r\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),\r\n        ]\r\n\r\n        msg = PointCloud2()\r\n        msg.header = header\r\n        msg.height = 1\r\n        msg.width = len(points)\r\n        msg.fields = fields\r\n        msg.is_bigendian = False\r\n        msg.point_step = 12\r\n        msg.row_step = msg.point_step * len(points)\r\n        msg.data = points.astype(np.float32).tobytes()\r\n        msg.is_dense = True\r\n\r\n        self.processed_cloud_pub.publish(msg)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = Perception3DNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(r.h2,{id:"spatial-understanding-and-scene-reconstruction",children:"Spatial Understanding and Scene Reconstruction"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nSpatial understanding and scene reconstruction.\r\n\r\nBuilds volumetric maps and semantic representations\r\nof the robot\'s environment for navigation and planning.\r\n\r\nPrerequisites:\r\n    pip install numpy scipy\r\n"""\r\n\r\nimport numpy as np\r\nfrom dataclasses import dataclass, field\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom enum import Enum\r\n\r\n\r\nclass VoxelState(Enum):\r\n    """State of a voxel in the occupancy grid."""\r\n    UNKNOWN = 0\r\n    FREE = 1\r\n    OCCUPIED = 2\r\n\r\n\r\n@dataclass\r\nclass Voxel:\r\n    """Single voxel with occupancy and semantic info."""\r\n    state: VoxelState = VoxelState.UNKNOWN\r\n    probability: float = 0.5  # Occupancy probability [0, 1]\r\n    observations: int = 0\r\n    semantic_label: Optional[str] = None\r\n    color: Optional[np.ndarray] = None\r\n\r\n\r\nclass OccupancyGrid3D:\r\n    """\r\n    3D occupancy grid for spatial mapping.\r\n\r\n    Efficiently represents the robot\'s environment\r\n    using a volumetric grid with probabilistic updates.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        resolution: float = 0.05,\r\n        origin: Tuple[float, float, float] = (-5.0, -5.0, -1.0),\r\n        dimensions: Tuple[int, int, int] = (200, 200, 60)\r\n    ):\r\n        """\r\n        Initialize occupancy grid.\r\n\r\n        Args:\r\n            resolution: Voxel size in meters\r\n            origin: World coordinates of grid origin\r\n            dimensions: Grid size (x, y, z) in voxels\r\n        """\r\n        self.resolution = resolution\r\n        self.origin = np.array(origin)\r\n        self.dimensions = dimensions\r\n\r\n        # Log-odds representation for efficient updates\r\n        # P(occupied) = 1 / (1 + exp(-log_odds))\r\n        self.log_odds = np.zeros(dimensions, dtype=np.float32)\r\n\r\n        # Sensor model parameters\r\n        self.log_odds_hit = 0.7   # Log-odds update for hit\r\n        self.log_odds_miss = -0.4  # Log-odds update for miss\r\n        self.log_odds_max = 3.5\r\n        self.log_odds_min = -3.5\r\n\r\n    def world_to_grid(self, point: np.ndarray) -> Tuple[int, int, int]:\r\n        """Convert world coordinates to grid indices."""\r\n        grid_coords = (point - self.origin) / self.resolution\r\n        return tuple(grid_coords.astype(int))\r\n\r\n    def grid_to_world(self, indices: Tuple[int, int, int]) -> np.ndarray:\r\n        """Convert grid indices to world coordinates (voxel center)."""\r\n        return self.origin + (np.array(indices) + 0.5) * self.resolution\r\n\r\n    def is_valid_index(self, indices: Tuple[int, int, int]) -> bool:\r\n        """Check if grid indices are within bounds."""\r\n        return all(\r\n            0 <= indices[i] < self.dimensions[i]\r\n            for i in range(3)\r\n        )\r\n\r\n    def get_occupancy(self, point: np.ndarray) -> float:\r\n        """\r\n        Get occupancy probability at world coordinates.\r\n\r\n        Args:\r\n            point: World coordinates\r\n\r\n        Returns:\r\n            Probability [0, 1] that the point is occupied\r\n        """\r\n        indices = self.world_to_grid(point)\r\n\r\n        if not self.is_valid_index(indices):\r\n            return 0.5  # Unknown\r\n\r\n        log_odds = self.log_odds[indices]\r\n        return 1.0 / (1.0 + np.exp(-log_odds))\r\n\r\n    def update_ray(\r\n        self,\r\n        origin: np.ndarray,\r\n        endpoint: np.ndarray,\r\n        hit: bool = True\r\n    ):\r\n        """\r\n        Update occupancy along a ray using ray casting.\r\n\r\n        Args:\r\n            origin: Ray origin (sensor position)\r\n            endpoint: Ray endpoint (measurement point)\r\n            hit: Whether the endpoint is an obstacle\r\n        """\r\n        # Convert to grid coordinates\r\n        start = self.world_to_grid(origin)\r\n        end = self.world_to_grid(endpoint)\r\n\r\n        # Ray cast using 3D Bresenham\r\n        ray_voxels = self._bresenham_3d(start, end)\r\n\r\n        # Mark all voxels except last as free\r\n        for voxel in ray_voxels[:-1]:\r\n            if self.is_valid_index(voxel):\r\n                self.log_odds[voxel] = np.clip(\r\n                    self.log_odds[voxel] + self.log_odds_miss,\r\n                    self.log_odds_min,\r\n                    self.log_odds_max\r\n                )\r\n\r\n        # Mark endpoint as occupied or free\r\n        if ray_voxels and self.is_valid_index(ray_voxels[-1]):\r\n            update = self.log_odds_hit if hit else self.log_odds_miss\r\n            self.log_odds[ray_voxels[-1]] = np.clip(\r\n                self.log_odds[ray_voxels[-1]] + update,\r\n                self.log_odds_min,\r\n                self.log_odds_max\r\n            )\r\n\r\n    def _bresenham_3d(\r\n        self,\r\n        start: Tuple[int, int, int],\r\n        end: Tuple[int, int, int]\r\n    ) -> List[Tuple[int, int, int]]:\r\n        """3D Bresenham line algorithm for ray casting."""\r\n        voxels = []\r\n\r\n        dx = abs(end[0] - start[0])\r\n        dy = abs(end[1] - start[1])\r\n        dz = abs(end[2] - start[2])\r\n\r\n        sx = 1 if end[0] > start[0] else -1\r\n        sy = 1 if end[1] > start[1] else -1\r\n        sz = 1 if end[2] > start[2] else -1\r\n\r\n        x, y, z = start\r\n\r\n        # Determine driving axis\r\n        if dx >= dy and dx >= dz:\r\n            err_y = 2 * dy - dx\r\n            err_z = 2 * dz - dx\r\n\r\n            for _ in range(dx + 1):\r\n                voxels.append((x, y, z))\r\n                if err_y > 0:\r\n                    y += sy\r\n                    err_y -= 2 * dx\r\n                if err_z > 0:\r\n                    z += sz\r\n                    err_z -= 2 * dx\r\n                err_y += 2 * dy\r\n                err_z += 2 * dz\r\n                x += sx\r\n\r\n        elif dy >= dx and dy >= dz:\r\n            err_x = 2 * dx - dy\r\n            err_z = 2 * dz - dy\r\n\r\n            for _ in range(dy + 1):\r\n                voxels.append((x, y, z))\r\n                if err_x > 0:\r\n                    x += sx\r\n                    err_x -= 2 * dy\r\n                if err_z > 0:\r\n                    z += sz\r\n                    err_z -= 2 * dy\r\n                err_x += 2 * dx\r\n                err_z += 2 * dz\r\n                y += sy\r\n\r\n        else:\r\n            err_x = 2 * dx - dz\r\n            err_y = 2 * dy - dz\r\n\r\n            for _ in range(dz + 1):\r\n                voxels.append((x, y, z))\r\n                if err_x > 0:\r\n                    x += sx\r\n                    err_x -= 2 * dz\r\n                if err_y > 0:\r\n                    y += sy\r\n                    err_y -= 2 * dz\r\n                err_x += 2 * dx\r\n                err_y += 2 * dy\r\n                z += sz\r\n\r\n        return voxels\r\n\r\n    def integrate_point_cloud(\r\n        self,\r\n        points: np.ndarray,\r\n        sensor_origin: np.ndarray\r\n    ):\r\n        """\r\n        Integrate a point cloud into the occupancy grid.\r\n\r\n        Args:\r\n            points: (N, 3) point cloud\r\n            sensor_origin: Sensor position\r\n        """\r\n        for point in points:\r\n            self.update_ray(sensor_origin, point, hit=True)\r\n\r\n    def get_occupied_voxels(\r\n        self,\r\n        threshold: float = 0.7\r\n    ) -> np.ndarray:\r\n        """\r\n        Get world coordinates of occupied voxels.\r\n\r\n        Args:\r\n            threshold: Occupancy probability threshold\r\n\r\n        Returns:\r\n            (N, 3) array of occupied voxel centers\r\n        """\r\n        # Convert log-odds to probability\r\n        prob = 1.0 / (1.0 + np.exp(-self.log_odds))\r\n\r\n        # Find occupied indices\r\n        occupied = np.where(prob > threshold)\r\n        indices = np.stack(occupied, axis=1)\r\n\r\n        # Convert to world coordinates\r\n        return self.origin + (indices + 0.5) * self.resolution\r\n\r\n    def extract_2d_slice(\r\n        self,\r\n        z_range: Tuple[float, float] = (0.0, 2.0)\r\n    ) -> np.ndarray:\r\n        """\r\n        Extract 2D occupancy map from 3D grid.\r\n\r\n        Args:\r\n            z_range: Height range to project (meters)\r\n\r\n        Returns:\r\n            2D occupancy probability map\r\n        """\r\n        z_min = int((z_range[0] - self.origin[2]) / self.resolution)\r\n        z_max = int((z_range[1] - self.origin[2]) / self.resolution)\r\n\r\n        z_min = max(0, z_min)\r\n        z_max = min(self.dimensions[2], z_max)\r\n\r\n        # Max occupancy over z range\r\n        slice_log_odds = np.max(self.log_odds[:, :, z_min:z_max], axis=2)\r\n        prob_2d = 1.0 / (1.0 + np.exp(-slice_log_odds))\r\n\r\n        return prob_2d\r\n\r\n\r\ndef demo_occupancy_grid():\r\n    """Demonstrate 3D occupancy grid mapping."""\r\n    print("3D Occupancy Grid Demo")\r\n    print("=" * 50)\r\n\r\n    # Create grid\r\n    grid = OccupancyGrid3D(\r\n        resolution=0.1,\r\n        origin=(-5.0, -5.0, -1.0),\r\n        dimensions=(100, 100, 30)\r\n    )\r\n\r\n    # Simulate sensor at origin\r\n    sensor_origin = np.array([0.0, 0.0, 0.5])\r\n\r\n    # Simulate some obstacles\r\n    # Wall in front\r\n    wall_points = []\r\n    for y in np.linspace(-2, 2, 50):\r\n        for z in np.linspace(0, 2, 20):\r\n            wall_points.append([3.0, y, z])\r\n\r\n    # Box obstacle\r\n    box_points = []\r\n    for x in np.linspace(1.5, 2.0, 10):\r\n        for y in np.linspace(-0.5, 0.5, 10):\r\n            for z in np.linspace(0, 0.8, 10):\r\n                box_points.append([x, y, z])\r\n\r\n    all_points = np.array(wall_points + box_points)\r\n    print(f"Integrating {len(all_points)} points...")\r\n\r\n    # Integrate point cloud\r\n    grid.integrate_point_cloud(all_points, sensor_origin)\r\n\r\n    # Get statistics\r\n    occupied = grid.get_occupied_voxels(threshold=0.6)\r\n    print(f"Occupied voxels: {len(occupied)}")\r\n\r\n    # Extract 2D map\r\n    map_2d = grid.extract_2d_slice(z_range=(0.1, 1.5))\r\n    print(f"2D map shape: {map_2d.shape}")\r\n    print(f"2D map occupancy range: [{map_2d.min():.2f}, {map_2d.max():.2f}]")\r\n\r\n    # Visualize (simple ASCII art)\r\n    print("\\n2D Occupancy Map (50x50 region):")\r\n    map_vis = map_2d[25:75, 25:75]  # Center region\r\n    for row in map_vis[::2]:  # Subsample for display\r\n        line = ""\r\n        for val in row[::2]:\r\n            if val > 0.7:\r\n                line += "##"\r\n            elif val > 0.3:\r\n                line += ".."\r\n            else:\r\n                line += "  "\r\n        print(line)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    demo_occupancy_grid()\n'})}),"\n",(0,s.jsx)(r.h2,{id:"3d-perception-architecture",children:"3D Perception Architecture"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    Complete 3D Perception System                              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                              Sensor Layer                                    \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502   RGB-D Camera  \u2502  Stereo Camera  \u2502     LiDAR       \u2502   Ultrasonic        \u2502\r\n\u2502   (RealSense)   \u2502  (ZED, Intel)   \u2502  (Velodyne)     \u2502   (Short-range)     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502                 \u2502                 \u2502                   \u2502\r\n         \u25bc                 \u25bc                 \u25bc                   \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                           Point Cloud Generation                             \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  \u2022 Depth-to-3D projection     \u2022 Stereo matching                             \u2502\r\n\u2502  \u2022 Sensor calibration         \u2022 Time synchronization                        \u2502\r\n\u2502  \u2022 Coordinate transforms      \u2022 Multi-sensor fusion                         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                   \u2502\r\n                                   \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                             Point Cloud Processing                           \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502  Filtering    \u2502  \u2502  Downsampling \u2502  \u2502   Segmentation\u2502  \u2502  Clustering  \u2502 \u2502\r\n\u2502  \u2502  (Outliers)   \u2502  \u2502  (Voxel Grid) \u2502  \u2502   (RANSAC)    \u2502  \u2502  (DBSCAN)    \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                   \u2502\r\n                                   \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                            Spatial Understanding                             \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502  Occupancy    \u2502  \u2502   Surface     \u2502  \u2502    Object     \u2502  \u2502   Semantic   \u2502 \u2502\r\n\u2502  \u2502    Grid       \u2502  \u2502 Reconstruction\u2502  \u2502   Detection   \u2502  \u2502   Mapping    \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                   \u2502\r\n                                   \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                              Robot Applications                              \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502   Navigation    \u2502   Manipulation  \u2502   Localization  \u2502   Scene Analysis    \u2502\r\n\u2502   Path Planning \u2502   Grasp Planning\u2502     SLAM        \u2502   Understanding     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(r.p,{children:"In this chapter, you learned:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Depth sensing technologies"}),": RGB-D cameras, LiDAR, stereo vision and their trade-offs"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Point cloud generation"}),": Converting depth data to 3D representations"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Filtering and preprocessing"}),": Voxel downsampling, outlier removal, and noise reduction"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Segmentation"}),": RANSAC plane fitting and DBSCAN clustering for object detection"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Spatial mapping"}),": 3D occupancy grids for navigation and planning"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"ROS 2 integration"}),": Building perception pipelines with standard interfaces"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"RGB-D cameras"})," provide dense, colored depth for short-range manipulation"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"LiDAR"})," excels at long-range, accurate outdoor perception"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Point cloud filtering"})," is essential for robust perception"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Occupancy grids"})," enable safe navigation and collision avoidance"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Open3D"})," provides efficient algorithms for complex 3D processing"]}),"\n",(0,s.jsxs)(r.li,{children:["ROS 2 standardizes ",(0,s.jsx)(r.strong,{children:"sensor interfaces"})," for reusable perception systems"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(r.h3,{id:"exercise-1-depth-camera-calibration",children:"Exercise 1: Depth Camera Calibration"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Set up an Intel RealSense camera"}),"\n",(0,s.jsx)(r.li,{children:"Capture depth frames and verify accuracy at known distances"}),"\n",(0,s.jsx)(r.li,{children:"Compare depth quality in different lighting conditions"}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"exercise-2-ground-plane-removal",children:"Exercise 2: Ground Plane Removal"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Implement RANSAC ground plane detection from LiDAR data"}),"\n",(0,s.jsx)(r.li,{children:"Visualize ground vs. obstacle points with different colors"}),"\n",(0,s.jsx)(r.li,{children:"Test with tilted sensor orientations"}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"exercise-3-object-detection-from-point-clouds",children:"Exercise 3: Object Detection from Point Clouds"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Create a scene with multiple objects on a table"}),"\n",(0,s.jsx)(r.li,{children:"Segment the table surface and cluster objects"}),"\n",(0,s.jsx)(r.li,{children:"Compute bounding boxes and publish as ROS 2 markers"}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"challenge-multi-sensor-fusion",children:"Challenge: Multi-Sensor Fusion"}),"\n",(0,s.jsx)(r.p,{children:"Build a system that:"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Fuses RGB-D and LiDAR data into a unified point cloud"}),"\n",(0,s.jsx)(r.li,{children:"Handles different frame rates and coordinate systems"}),"\n",(0,s.jsx)(r.li,{children:"Maintains temporal consistency for moving objects"}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"up-next",children:"Up Next"}),"\n",(0,s.jsxs)(r.p,{children:["In the ",(0,s.jsx)(r.strong,{children:"Capstone Project"}),", we'll combine all the concepts from this book to build a complete humanoid robot system with integrated perception, planning, and control."]}),"\n",(0,s.jsx)(r.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"https://github.com/IntelRealSense/librealsense",children:"Intel RealSense SDK"})," - RGB-D camera drivers"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"http://www.open3d.org/docs/",children:"Open3D Documentation"})," - Point cloud processing"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"https://pointclouds.org/",children:"PCL (Point Cloud Library)"})," - C++ point cloud algorithms"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"https://octomap.github.io/",children:"Octomap"})," - Efficient 3D mapping"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.a,{href:"https://github.com/ros-perception",children:"ROS 2 Perception Packages"})," - Pre-built perception nodes"]}),"\n"]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsx)(r.strong,{children:"Sources:"})}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://dev.intelrealsense.com/",children:"Intel RealSense Documentation"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://velodynelidar.com/documentation/",children:"Velodyne LiDAR User Guide"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"http://www.open3d.org/docs/release/tutorial/geometry/pointcloud.html",children:"Open3D Tutorials"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://en.wikipedia.org/wiki/Random_sample_consensus",children:"RANSAC Algorithm"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://arxiv.org/abs/1010.1202",children:"OctoMap: 3D Occupancy Mapping"})}),"\n"]})]})}function c(n={}){const{wrapper:r}={...(0,t.R)(),...n.components};return r?(0,s.jsx)(r,{...n,children:(0,s.jsx)(p,{...n})}):p(n)}}}]);