"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[267],{4728:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"assets/vla_diagrams/README","title":"VLA (Vision-Language-Action) Diagrams","description":"This directory contains diagrams for Vision-Language-Action systems from Chapters 8-9.","source":"@site/docs/assets/vla_diagrams/README.md","sourceDirName":"assets/vla_diagrams","slug":"/assets/vla_diagrams/","permalink":"/Physical-AI-Humanoid-Robotics/docs/assets/vla_diagrams/","draft":false,"unlisted":false,"editUrl":"https://github.com/Salwagull/Physical-AI-Humanoid-Robotics/tree/main/docs/assets/vla_diagrams/README.md","tags":[],"version":"current","frontMatter":{}}');var r=i(4848),a=i(8453);const l={},t="VLA (Vision-Language-Action) Diagrams",c={},o=[{value:"Recommended Diagrams",id:"recommended-diagrams",level:2},{value:"Chapter 8: Vision-Language-Action Systems",id:"chapter-8-vision-language-action-systems",level:3},{value:"Chapter 9: LLM Planning and Voice Commands",id:"chapter-9-llm-planning-and-voice-commands",level:3},{value:"Creating Diagrams",id:"creating-diagrams",level:2},{value:"Option 1: Mermaid (Markdown-Compatible)",id:"option-1-mermaid-markdown-compatible",level:3},{value:"Option 2: ASCII Art (In Markdown)",id:"option-2-ascii-art-in-markdown",level:3},{value:"Option 3: Draw.io / Excalidraw",id:"option-3-drawio--excalidraw",level:3},{value:"Mermaid Diagram Examples",id:"mermaid-diagram-examples",level:2},{value:"VLA Pipeline",id:"vla-pipeline",level:3},{value:"Object Detection Flow",id:"object-detection-flow",level:3},{value:"Action Skill Decomposition",id:"action-skill-decomposition",level:3},{value:"Language Parsing",id:"language-parsing",level:3},{value:"Placeholder Image List",id:"placeholder-image-list",level:2},{value:"Chapter 8",id:"chapter-8",level:3},{value:"Chapter 9",id:"chapter-9",level:3},{value:"Diagram Standards",id:"diagram-standards",level:2},{value:"Tools Quick Links",id:"tools-quick-links",level:2},{value:"Rendering Mermaid to PNG",id:"rendering-mermaid-to-png",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"vla-vision-language-action-diagrams",children:"VLA (Vision-Language-Action) Diagrams"})}),"\n",(0,r.jsx)(e.p,{children:"This directory contains diagrams for Vision-Language-Action systems from Chapters 8-9."}),"\n",(0,r.jsx)(e.h2,{id:"recommended-diagrams",children:"Recommended Diagrams"}),"\n",(0,r.jsx)(e.h3,{id:"chapter-8-vision-language-action-systems",children:"Chapter 8: Vision-Language-Action Systems"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"vla_pipeline_overview.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"End-to-end VLA pipeline"}),"\n",(0,r.jsx)(e.li,{children:"Shows: Camera \u2192 Vision \u2192 Language \u2192 Action \u2192 Robot"}),"\n",(0,r.jsx)(e.li,{children:"Flow arrows with data labels"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"vla_architecture.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Detailed VLA architecture"}),"\n",(0,r.jsx)(e.li,{children:"Components: Vision Module, Language Module, Action Module"}),"\n",(0,r.jsx)(e.li,{children:"ROS 2 topics and messages"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"object_detection_pipeline.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"YOLO detection workflow"}),"\n",(0,r.jsx)(e.li,{children:"Shows: Image \u2192 CNN \u2192 Feature Maps \u2192 Predictions \u2192 Bounding Boxes"}),"\n",(0,r.jsx)(e.li,{children:"Example detections annotated"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"language_parsing_flow.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"NLP command parsing"}),"\n",(0,r.jsx)(e.li,{children:"Shows: Text \u2192 Tokenization \u2192 Action Extraction \u2192 Entity Extraction \u2192 Parsed Command"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"action_planning_sequence.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Skill decomposition for pick-and-place"}),"\n",(0,r.jsx)(e.li,{children:"Shows: Goal \u2192 Skills (Navigate, Approach, Grasp, Lift, Move, Place)"}),"\n",(0,r.jsx)(e.li,{children:"Preconditions and effects"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"vision_grounding.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Connecting language to visual objects"}),"\n",(0,r.jsx)(e.li,{children:'Shows: "red cup" \u2192 Vision search \u2192 Matching detection \u2192 3D position'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"chapter-9-llm-planning-and-voice-commands",children:"Chapter 9: LLM Planning and Voice Commands"}),"\n",(0,r.jsxs)(e.ol,{start:"7",children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"voice_command_pipeline.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Voice to robot action"}),"\n",(0,r.jsx)(e.li,{children:"Shows: Microphone \u2192 Speech Recognition \u2192 LLM \u2192 Action Planner \u2192 Robot"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"llm_planning_architecture.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"LLM-based task planning"}),"\n",(0,r.jsx)(e.li,{children:"Shows: Natural Language Goal \u2192 LLM Reasoning \u2192 Step-by-step Plan \u2192 Execution"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"multimodal_fusion.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Vision + Language integration"}),"\n",(0,r.jsx)(e.li,{children:"Shows how visual context influences language understanding"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"error_recovery_flow.png"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Handling failed actions"}),"\n",(0,r.jsx)(e.li,{children:"Shows: Failure Detection \u2192 Replanning \u2192 Retry Logic"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"creating-diagrams",children:"Creating Diagrams"}),"\n",(0,r.jsx)(e.h3,{id:"option-1-mermaid-markdown-compatible",children:"Option 1: Mermaid (Markdown-Compatible)"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-mermaid",children:"graph LR\r\n    subgraph Vision\r\n        A[Camera] --\x3e B[YOLO Detector]\r\n        B --\x3e C[Detections]\r\n    end\r\n\r\n    subgraph Language\r\n        D[Voice Command] --\x3e E[Parser]\r\n        E --\x3e F[Intent + Object]\r\n    end\r\n\r\n    subgraph Action\r\n        C --\x3e G[Grounding]\r\n        F --\x3e G\r\n        G --\x3e H[Planner]\r\n        H --\x3e I[Robot Commands]\r\n    end\n"})}),"\n",(0,r.jsx)(e.h3,{id:"option-2-ascii-art-in-markdown",children:"Option 2: ASCII Art (In Markdown)"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Vision    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Language   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Action    \u2502\r\n\u2502  Detector   \u2502     \u2502   Parser    \u2502     \u2502   Planner   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502                   \u2502                   \u2502\r\n       \u25bc                   \u25bc                   \u25bc\r\n   Objects &           Intent &            Motion\r\n   Locations           Entities           Commands\n"})}),"\n",(0,r.jsx)(e.h3,{id:"option-3-drawio--excalidraw",children:"Option 3: Draw.io / Excalidraw"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Visit ",(0,r.jsx)(e.a,{href:"https://app.diagrams.net/",children:"https://app.diagrams.net/"})," or ",(0,r.jsx)(e.a,{href:"https://excalidraw.com/",children:"https://excalidraw.com/"})]}),"\n",(0,r.jsx)(e.li,{children:"Create flowcharts using the component names above"}),"\n",(0,r.jsx)(e.li,{children:"Export as PNG (300 DPI)"}),"\n",(0,r.jsx)(e.li,{children:"Save source files for editing"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"mermaid-diagram-examples",children:"Mermaid Diagram Examples"}),"\n",(0,r.jsx)(e.h3,{id:"vla-pipeline",children:"VLA Pipeline"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-mermaid",children:'flowchart TD\r\n    subgraph Input\r\n        CAM[Camera]\r\n        MIC[Microphone]\r\n    end\r\n\r\n    subgraph "VLA System"\r\n        CAM --\x3e VIS[Vision Module]\r\n        VIS --\x3e DET[Object Detections]\r\n\r\n        MIC --\x3e ASR[Speech Recognition]\r\n        ASR --\x3e NLU[Language Parser]\r\n        NLU --\x3e INT[Intent + Entities]\r\n\r\n        DET --\x3e FUSE[Multimodal Fusion]\r\n        INT --\x3e FUSE\r\n\r\n        FUSE --\x3e PLAN[Action Planner]\r\n        PLAN --\x3e SKILLS[Skill Sequence]\r\n    end\r\n\r\n    subgraph Output\r\n        SKILLS --\x3e CTRL[Robot Controller]\r\n        CTRL --\x3e ARM[Arm Motion]\r\n        CTRL --\x3e BASE[Base Motion]\r\n    end\n'})}),"\n",(0,r.jsx)(e.h3,{id:"object-detection-flow",children:"Object Detection Flow"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-mermaid",children:"flowchart LR\r\n    A[Input Image] --\x3e B[YOLO Model]\r\n    B --\x3e C{For each detection}\r\n    C --\x3e D[Bounding Box]\r\n    C --\x3e E[Class Label]\r\n    C --\x3e F[Confidence]\r\n    D --\x3e G[Detection Object]\r\n    E --\x3e G\r\n    F --\x3e G\r\n    G --\x3e H[Filter by threshold]\r\n    H --\x3e I[Final Detections]\n"})}),"\n",(0,r.jsx)(e.h3,{id:"action-skill-decomposition",children:"Action Skill Decomposition"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-mermaid",children:"stateDiagram-v2\r\n    [*] --\x3e LookAt: Detect object\r\n    LookAt --\x3e Approach: Object visible\r\n    Approach --\x3e PreGrasp: Within reach\r\n    PreGrasp --\x3e Grasp: Aligned\r\n    Grasp --\x3e Lift: Object secured\r\n    Lift --\x3e [*]: Success\r\n\r\n    Grasp --\x3e Approach: Grasp failed\r\n    LookAt --\x3e [*]: Object not found\n"})}),"\n",(0,r.jsx)(e.h3,{id:"language-parsing",children:"Language Parsing"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-mermaid",children:'flowchart TB\r\n    A["Pick up the red cup"] --\x3e B[Tokenize]\r\n    B --\x3e C[Action Extraction]\r\n    C --\x3e D["Action: PICK"]\r\n\r\n    B --\x3e E[Object Extraction]\r\n    E --\x3e F["Object: cup"]\r\n\r\n    B --\x3e G[Modifier Extraction]\r\n    G --\x3e H["Color: red"]\r\n\r\n    D --\x3e I[ParsedCommand]\r\n    F --\x3e I\r\n    H --\x3e I\n'})}),"\n",(0,r.jsx)(e.h2,{id:"placeholder-image-list",children:"Placeholder Image List"}),"\n",(0,r.jsx)(e.p,{children:"Until diagrams are created, the chapters use ASCII art. Replace with actual images:"}),"\n",(0,r.jsx)(e.h3,{id:"chapter-8",children:"Chapter 8"}),"\n",(0,r.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","vla_pipeline_overview.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","vla_architecture.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","object_detection_pipeline.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","language_parsing_flow.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","action_planning_sequence.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","vision_grounding.png"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"chapter-9",children:"Chapter 9"}),"\n",(0,r.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","voice_command_pipeline.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","llm_planning_architecture.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","multimodal_fusion.png"]}),"\n",(0,r.jsxs)(e.li,{className:"task-list-item",children:[(0,r.jsx)(e.input,{type:"checkbox",disabled:!0})," ","error_recovery_flow.png"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"diagram-standards",children:"Diagram Standards"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Resolution"}),": Minimum 1920x1080 or 300 DPI"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Format"}),": PNG with transparency"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Colors"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Vision: Blue tones"}),"\n",(0,r.jsx)(e.li,{children:"Language: Green tones"}),"\n",(0,r.jsx)(e.li,{children:"Action: Orange tones"}),"\n",(0,r.jsx)(e.li,{children:"Robot: Gray tones"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Font"}),": Sans-serif, readable at 50% zoom"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Arrows"}),": Clear direction, labeled where helpful"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"tools-quick-links",children:"Tools Quick Links"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://app.diagrams.net/",children:"Draw.io"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://excalidraw.com/",children:"Excalidraw"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://mermaid.live/",children:"Mermaid Live Editor"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://www.lucidchart.com/",children:"Lucidchart"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://www.figma.com/",children:"Figma"})}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"rendering-mermaid-to-png",children:"Rendering Mermaid to PNG"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Install mermaid-cli\r\nnpm install -g @mermaid-js/mermaid-cli\r\n\r\n# Render diagram\r\nmmdc -i vla_pipeline.mmd -o vla_pipeline_overview.png -w 1920 -H 1080\n"})})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>l,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function l(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:l(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);