"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[291],{4996:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter9_llm_voice_commands","title":"Chapter 9: LLM Planning and Voice Commands","description":"Integrate LLMs for task planning and build voice-controlled robot systems with speech recognition","source":"@site/docs/chapter9_llm_voice_commands.md","sourceDirName":".","slug":"/chapter9_llm_voice_commands","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter9_llm_voice_commands","draft":false,"unlisted":false,"editUrl":"https://github.com/Salwagull/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter9_llm_voice_commands.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Chapter 9: LLM Planning and Voice Commands","description":"Integrate LLMs for task planning and build voice-controlled robot systems with speech recognition","keywords":["llm","voice commands","speech recognition","whisper","openai","ollama","task planning","ros2"]},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Vision-Language-Action Systems","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter8_vla_systems"},"next":{"title":"Chapter 10: Computer Vision for Robotics","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter10_computer_vision"}}');var o=r(4848),i=r(8453);const s={sidebar_position:9,title:"Chapter 9: LLM Planning and Voice Commands",description:"Integrate LLMs for task planning and build voice-controlled robot systems with speech recognition",keywords:["llm","voice commands","speech recognition","whisper","openai","ollama","task planning","ros2"]},a="Chapter 9: LLM Planning and Voice Commands for Robots",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Why LLMs for Robotics?",id:"why-llms-for-robotics",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:2},{value:"Speech Recognition with Whisper",id:"speech-recognition-with-whisper",level:3},{value:"Alternative: Google Speech Recognition",id:"alternative-google-speech-recognition",level:3},{value:"LLM Integration for Task Planning",id:"llm-integration-for-task-planning",level:2},{value:"Using LLMs for Robot Reasoning",id:"using-llms-for-robot-reasoning",level:3},{value:"Handling Ambiguous Commands",id:"handling-ambiguous-commands",level:2},{value:"Complete Voice-Controlled Robot System",id:"complete-voice-controlled-robot-system",level:2},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Add Wake Word Detection",id:"exercise-1-add-wake-word-detection",level:3},{value:"Exercise 2: Implement Text-to-Speech",id:"exercise-2-implement-text-to-speech",level:3},{value:"Exercise 3: Multi-Object Commands",id:"exercise-3-multi-object-commands",level:3},{value:"Exercise 4: Error Recovery",id:"exercise-4-error-recovery",level:3},{value:"Challenge: Teach New Commands",id:"challenge-teach-new-commands",level:3},{value:"Up Next",id:"up-next",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-9-llm-planning-and-voice-commands-for-robots",children:"Chapter 9: LLM Planning and Voice Commands for Robots"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Integrate Large Language Models (LLMs) for high-level robot task planning"}),"\n",(0,o.jsx)(e.li,{children:"Build a voice command processing pipeline with speech recognition"}),"\n",(0,o.jsx)(e.li,{children:"Translate natural language into executable robot actions"}),"\n",(0,o.jsx)(e.li,{children:"Handle ambiguous and context-dependent commands"}),"\n",(0,o.jsx)(e.li,{children:"Create a complete voice-controlled robot system with ROS 2"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(e.p,{children:["While Chapter 8 introduced the fundamentals of Vision-Language-Action systems, this chapter takes robot intelligence to the next level by integrating ",(0,o.jsx)(e.strong,{children:"Large Language Models (LLMs)"})," for sophisticated reasoning and ",(0,o.jsx)(e.strong,{children:"voice commands"})," for natural human-robot interaction."]}),"\n",(0,o.jsx)(e.h3,{id:"why-llms-for-robotics",children:"Why LLMs for Robotics?"}),"\n",(0,o.jsx)(e.p,{children:"Traditional robot programming requires explicit instructions for every scenario. LLMs enable:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Complex reasoning"}),": Break down multi-step tasks automatically"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context understanding"}),": Remember conversation history and scene state"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Ambiguity resolution"}),": Ask clarifying questions when commands are unclear"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Generalization"}),": Handle novel situations without reprogramming"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Natural dialogue"}),": Communicate with users conversationally"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502              LLM-Powered Voice Control Pipeline                          \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                          \u2502\r\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\r\n\u2502   \u2502  Voice  \u2502\u2500\u2500\u2500\u25b6\u2502  Speech \u2502\u2500\u2500\u2500\u25b6\u2502   LLM   \u2502\u2500\u2500\u2500\u25b6\u2502 Action  \u2502             \u2502\r\n\u2502   \u2502  Input  \u2502    \u2502  to Text\u2502    \u2502 Planner \u2502    \u2502Executor \u2502             \u2502\r\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\r\n\u2502                                      \u2502                                   \u2502\r\n\u2502                                      \u25bc                                   \u2502\r\n\u2502                               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\r\n\u2502                               \u2502   Context   \u2502                           \u2502\r\n\u2502                               \u2502   Memory    \u2502                           \u2502\r\n\u2502                               \u2502 (Scene, History)                        \u2502\r\n\u2502                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\r\n\u2502                                                                          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h2,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,o.jsx)(e.h3,{id:"speech-recognition-with-whisper",children:"Speech Recognition with Whisper"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"OpenAI Whisper"})," provides state-of-the-art speech-to-text for robot commands."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nSpeech Recognition Module for Voice-Controlled Robots\r\n\r\nUses OpenAI Whisper for accurate speech-to-text conversion.\r\n\r\nPrerequisites:\r\n    pip install openai-whisper sounddevice numpy\r\n\r\nUsage:\r\n    python speech_recognition.py\r\n"""\r\n\r\nimport numpy as np\r\nimport queue\r\nimport threading\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional, Callable\r\n\r\ntry:\r\n    import whisper\r\n    import sounddevice as sd\r\n    WHISPER_AVAILABLE = True\r\nexcept ImportError:\r\n    WHISPER_AVAILABLE = False\r\n    print("Warning: Install dependencies with:")\r\n    print("  pip install openai-whisper sounddevice numpy")\r\n\r\n\r\n@dataclass\r\nclass SpeechResult:\r\n    """Result from speech recognition."""\r\n    text: str\r\n    confidence: float\r\n    language: str\r\n    duration: float\r\n\r\n\r\nclass SpeechRecognizer:\r\n    """\r\n    Real-time speech recognition using Whisper.\r\n\r\n    Supports continuous listening with wake word detection\r\n    or push-to-talk operation.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        model_size: str = "base",\r\n        language: str = "en",\r\n        wake_word: Optional[str] = None\r\n    ):\r\n        """\r\n        Initialize speech recognizer.\r\n\r\n        Args:\r\n            model_size: Whisper model size (tiny, base, small, medium, large)\r\n            language: Target language code\r\n            wake_word: Optional wake word to trigger listening (e.g., "robot")\r\n        """\r\n        if not WHISPER_AVAILABLE:\r\n            raise RuntimeError("Whisper not installed. Run: pip install openai-whisper")\r\n\r\n        print(f"Loading Whisper model: {model_size}")\r\n        self.model = whisper.load_model(model_size)\r\n        self.language = language\r\n        self.wake_word = wake_word.lower() if wake_word else None\r\n\r\n        # Audio settings\r\n        self.sample_rate = 16000\r\n        self.channels = 1\r\n        self.dtype = np.float32\r\n\r\n        # Recording state\r\n        self.audio_queue = queue.Queue()\r\n        self.is_listening = False\r\n        self.is_recording = False\r\n\r\n        print(f"Speech recognizer ready (language: {language})")\r\n\r\n    def transcribe(self, audio: np.ndarray) -> SpeechResult:\r\n        """\r\n        Transcribe audio to text.\r\n\r\n        Args:\r\n            audio: Audio samples as numpy array (16kHz, mono)\r\n\r\n        Returns:\r\n            SpeechResult with transcription\r\n        """\r\n        # Normalize audio\r\n        audio = audio.astype(np.float32)\r\n        if audio.max() > 1.0:\r\n            audio = audio / 32768.0\r\n\r\n        # Run Whisper\r\n        result = self.model.transcribe(\r\n            audio,\r\n            language=self.language,\r\n            fp16=False  # Use FP32 for CPU compatibility\r\n        )\r\n\r\n        return SpeechResult(\r\n            text=result["text"].strip(),\r\n            confidence=1.0,  # Whisper doesn\'t provide confidence\r\n            language=result.get("language", self.language),\r\n            duration=len(audio) / self.sample_rate\r\n        )\r\n\r\n    def record_audio(self, duration: float = 5.0) -> np.ndarray:\r\n        """\r\n        Record audio from microphone.\r\n\r\n        Args:\r\n            duration: Recording duration in seconds\r\n\r\n        Returns:\r\n            Audio samples as numpy array\r\n        """\r\n        print(f"Recording for {duration}s... Speak now!")\r\n\r\n        recording = sd.rec(\r\n            int(duration * self.sample_rate),\r\n            samplerate=self.sample_rate,\r\n            channels=self.channels,\r\n            dtype=self.dtype\r\n        )\r\n        sd.wait()\r\n\r\n        print("Recording complete")\r\n        return recording.flatten()\r\n\r\n    def listen_once(self, duration: float = 5.0) -> SpeechResult:\r\n        """\r\n        Record and transcribe a single utterance.\r\n\r\n        Args:\r\n            duration: Max recording duration\r\n\r\n        Returns:\r\n            SpeechResult with transcription\r\n        """\r\n        audio = self.record_audio(duration)\r\n        return self.transcribe(audio)\r\n\r\n    def start_continuous_listening(\r\n        self,\r\n        callback: Callable[[str], None],\r\n        chunk_duration: float = 3.0\r\n    ):\r\n        """\r\n        Start continuous listening with callback.\r\n\r\n        Args:\r\n            callback: Function called with transcribed text\r\n            chunk_duration: Duration of each audio chunk\r\n        """\r\n        self.is_listening = True\r\n\r\n        def listen_thread():\r\n            while self.is_listening:\r\n                try:\r\n                    audio = self.record_audio(chunk_duration)\r\n                    result = self.transcribe(audio)\r\n\r\n                    if result.text:\r\n                        # Check for wake word if configured\r\n                        if self.wake_word:\r\n                            if self.wake_word in result.text.lower():\r\n                                # Remove wake word and process command\r\n                                command = result.text.lower().replace(self.wake_word, "").strip()\r\n                                if command:\r\n                                    callback(command)\r\n                        else:\r\n                            callback(result.text)\r\n\r\n                except Exception as e:\r\n                    print(f"Listening error: {e}")\r\n\r\n        thread = threading.Thread(target=listen_thread, daemon=True)\r\n        thread.start()\r\n\r\n    def stop_listening(self):\r\n        """Stop continuous listening."""\r\n        self.is_listening = False\r\n\r\n\r\ndef demo_speech_recognition():\r\n    """Demonstrate speech recognition."""\r\n    print("\\n" + "=" * 60)\r\n    print("Speech Recognition Demo")\r\n    print("=" * 60)\r\n\r\n    recognizer = SpeechRecognizer(model_size="base")\r\n\r\n    print("\\nSpeak a command (5 seconds)...")\r\n    result = recognizer.listen_once(duration=5.0)\r\n\r\n    print(f"\\nRecognized: \\"{result.text}\\"")\r\n    print(f"Language: {result.language}")\r\n    print(f"Duration: {result.duration:.1f}s")\r\n\r\n\r\nif __name__ == "__main__":\r\n    demo_speech_recognition()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"alternative-google-speech-recognition",children:"Alternative: Google Speech Recognition"}),"\n",(0,o.jsx)(e.p,{children:"For lighter-weight speech recognition:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\r\nLightweight speech recognition using Google Speech API.\r\n\r\nPrerequisites:\r\n    pip install SpeechRecognition pyaudio\r\n"""\r\n\r\nimport speech_recognition as sr\r\nfrom typing import Optional\r\n\r\n\r\nclass GoogleSpeechRecognizer:\r\n    """Simple speech recognition using Google\'s API."""\r\n\r\n    def __init__(self):\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n\r\n        # Adjust for ambient noise\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source, duration=1)\r\n\r\n    def listen(self, timeout: float = 5.0) -> Optional[str]:\r\n        """\r\n        Listen for speech and return transcription.\r\n\r\n        Args:\r\n            timeout: Max listening time\r\n\r\n        Returns:\r\n            Transcribed text or None if failed\r\n        """\r\n        try:\r\n            with self.microphone as source:\r\n                print("Listening...")\r\n                audio = self.recognizer.listen(source, timeout=timeout)\r\n\r\n            print("Processing...")\r\n            text = self.recognizer.recognize_google(audio)\r\n            return text\r\n\r\n        except sr.WaitTimeoutError:\r\n            print("No speech detected")\r\n            return None\r\n        except sr.UnknownValueError:\r\n            print("Could not understand audio")\r\n            return None\r\n        except sr.RequestError as e:\r\n            print(f"API error: {e}")\r\n            return None\n'})}),"\n",(0,o.jsx)(e.h2,{id:"llm-integration-for-task-planning",children:"LLM Integration for Task Planning"}),"\n",(0,o.jsx)(e.h3,{id:"using-llms-for-robot-reasoning",children:"Using LLMs for Robot Reasoning"}),"\n",(0,o.jsx)(e.p,{children:"LLMs can decompose complex commands into executable steps."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nLLM-based Task Planner for Robots\r\n\r\nUses OpenAI GPT or local LLMs for high-level task planning.\r\n\r\nPrerequisites:\r\n    pip install openai  # For OpenAI API\r\n    # OR\r\n    pip install ollama  # For local LLMs\r\n\r\nUsage:\r\n    export OPENAI_API_KEY="your-key"\r\n    python llm_planner.py\r\n"""\r\n\r\nimport os\r\nimport json\r\nfrom dataclasses import dataclass, field\r\nfrom typing import List, Optional, Dict, Any\r\nfrom enum import Enum\r\n\r\n# Try to import LLM libraries\r\ntry:\r\n    import openai\r\n    OPENAI_AVAILABLE = True\r\nexcept ImportError:\r\n    OPENAI_AVAILABLE = False\r\n\r\ntry:\r\n    import ollama\r\n    OLLAMA_AVAILABLE = True\r\nexcept ImportError:\r\n    OLLAMA_AVAILABLE = False\r\n\r\n\r\nclass PlanStep(Enum):\r\n    """Types of plan steps."""\r\n    MOVE = "move"\r\n    PICK = "pick"\r\n    PLACE = "place"\r\n    LOOK = "look"\r\n    SAY = "say"\r\n    WAIT = "wait"\r\n    ASK = "ask"  # Ask for clarification\r\n\r\n\r\n@dataclass\r\nclass TaskPlan:\r\n    """A plan generated by the LLM."""\r\n    goal: str\r\n    steps: List[Dict[str, Any]]\r\n    reasoning: str\r\n    confidence: float\r\n    needs_clarification: bool = False\r\n    clarification_question: Optional[str] = None\r\n\r\n\r\nclass LLMPlanner:\r\n    """\r\n    LLM-based task planner for robots.\r\n\r\n    Converts natural language goals into executable step-by-step plans.\r\n    Supports both cloud APIs (OpenAI) and local models (Ollama).\r\n    """\r\n\r\n    # System prompt for the robot planner\r\n    SYSTEM_PROMPT = """You are a helpful robot assistant that plans tasks.\r\nGiven a user command and the current scene, generate a step-by-step plan.\r\n\r\nAvailable actions:\r\n- MOVE(location): Move to a location (e.g., "kitchen", "table")\r\n- PICK(object): Pick up an object\r\n- PLACE(location): Place held object at location\r\n- LOOK(target): Look at or search for something\r\n- SAY(message): Speak a message to the user\r\n- WAIT(seconds): Wait for specified time\r\n- ASK(question): Ask the user for clarification\r\n\r\nCurrent robot capabilities:\r\n- Can navigate to known locations\r\n- Can pick up objects within reach\r\n- Can place objects on surfaces\r\n- Has camera for object detection\r\n\r\nRespond with a JSON object containing:\r\n{\r\n    "goal": "interpreted goal",\r\n    "reasoning": "why this plan makes sense",\r\n    "steps": [\r\n        {"action": "ACTION_TYPE", "params": {...}, "description": "what this does"}\r\n    ],\r\n    "needs_clarification": false,\r\n    "clarification_question": null\r\n}\r\n\r\nIf the command is ambiguous, set needs_clarification to true and provide a question."""\r\n\r\n    def __init__(\r\n        self,\r\n        provider: str = "openai",\r\n        model: str = "gpt-4o-mini"\r\n    ):\r\n        """\r\n        Initialize LLM planner.\r\n\r\n        Args:\r\n            provider: "openai" or "ollama"\r\n            model: Model name (e.g., "gpt-4o-mini", "llama3.2")\r\n        """\r\n        self.provider = provider\r\n        self.model = model\r\n        self.conversation_history = []\r\n\r\n        if provider == "openai":\r\n            if not OPENAI_AVAILABLE:\r\n                raise RuntimeError("OpenAI not installed. Run: pip install openai")\r\n            self.client = openai.OpenAI()\r\n        elif provider == "ollama":\r\n            if not OLLAMA_AVAILABLE:\r\n                raise RuntimeError("Ollama not installed. Run: pip install ollama")\r\n        else:\r\n            raise ValueError(f"Unknown provider: {provider}")\r\n\r\n        print(f"LLM Planner initialized with {provider}/{model}")\r\n\r\n    def plan(\r\n        self,\r\n        command: str,\r\n        scene_context: Optional[Dict] = None,\r\n        conversation_history: Optional[List] = None\r\n    ) -> TaskPlan:\r\n        """\r\n        Generate a plan for the given command.\r\n\r\n        Args:\r\n            command: Natural language command\r\n            scene_context: Current scene state (objects, locations)\r\n            conversation_history: Previous conversation for context\r\n\r\n        Returns:\r\n            TaskPlan with steps to execute\r\n        """\r\n        # Build context message\r\n        context_parts = [f"User command: \\"{command}\\""]\r\n\r\n        if scene_context:\r\n            context_parts.append(f"\\nCurrent scene:")\r\n            if "objects" in scene_context:\r\n                context_parts.append(f"  Visible objects: {scene_context[\'objects\']}")\r\n            if "robot_location" in scene_context:\r\n                context_parts.append(f"  Robot location: {scene_context[\'robot_location\']}")\r\n            if "held_object" in scene_context:\r\n                context_parts.append(f"  Currently holding: {scene_context[\'held_object\']}")\r\n\r\n        user_message = "\\n".join(context_parts)\r\n\r\n        # Call LLM\r\n        if self.provider == "openai":\r\n            response = self._call_openai(user_message)\r\n        else:\r\n            response = self._call_ollama(user_message)\r\n\r\n        # Parse response\r\n        return self._parse_response(response, command)\r\n\r\n    def _call_openai(self, user_message: str) -> str:\r\n        """Call OpenAI API."""\r\n        messages = [\r\n            {"role": "system", "content": self.SYSTEM_PROMPT},\r\n            {"role": "user", "content": user_message}\r\n        ]\r\n\r\n        response = self.client.chat.completions.create(\r\n            model=self.model,\r\n            messages=messages,\r\n            temperature=0.3,\r\n            response_format={"type": "json_object"}\r\n        )\r\n\r\n        return response.choices[0].message.content\r\n\r\n    def _call_ollama(self, user_message: str) -> str:\r\n        """Call Ollama local model."""\r\n        messages = [\r\n            {"role": "system", "content": self.SYSTEM_PROMPT},\r\n            {"role": "user", "content": user_message}\r\n        ]\r\n\r\n        response = ollama.chat(\r\n            model=self.model,\r\n            messages=messages\r\n        )\r\n\r\n        return response["message"]["content"]\r\n\r\n    def _parse_response(self, response: str, original_command: str) -> TaskPlan:\r\n        """Parse LLM response into TaskPlan."""\r\n        try:\r\n            # Extract JSON from response\r\n            data = json.loads(response)\r\n\r\n            return TaskPlan(\r\n                goal=data.get("goal", original_command),\r\n                steps=data.get("steps", []),\r\n                reasoning=data.get("reasoning", ""),\r\n                confidence=0.9 if data.get("steps") else 0.5,\r\n                needs_clarification=data.get("needs_clarification", False),\r\n                clarification_question=data.get("clarification_question")\r\n            )\r\n\r\n        except json.JSONDecodeError:\r\n            # Fallback for non-JSON response\r\n            return TaskPlan(\r\n                goal=original_command,\r\n                steps=[],\r\n                reasoning=response,\r\n                confidence=0.3,\r\n                needs_clarification=True,\r\n                clarification_question="I couldn\'t understand that command. Could you rephrase?"\r\n            )\r\n\r\n    def explain_plan(self, plan: TaskPlan) -> str:\r\n        """Generate human-readable explanation of plan."""\r\n        lines = [f"Goal: {plan.goal}", "", "Plan:"]\r\n\r\n        for i, step in enumerate(plan.steps, 1):\r\n            action = step.get("action", "UNKNOWN")\r\n            desc = step.get("description", "")\r\n            lines.append(f"  {i}. {action}: {desc}")\r\n\r\n        if plan.reasoning:\r\n            lines.extend(["", f"Reasoning: {plan.reasoning}"])\r\n\r\n        return "\\n".join(lines)\r\n\r\n\r\nclass ConversationalPlanner(LLMPlanner):\r\n    """\r\n    LLM planner with conversation memory for multi-turn interactions.\r\n    """\r\n\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.history = []\r\n        self.scene_state = {}\r\n\r\n    def chat(self, user_input: str) -> str:\r\n        """\r\n        Process user input in conversational context.\r\n\r\n        Handles both commands and questions.\r\n        """\r\n        # Add to history\r\n        self.history.append({"role": "user", "content": user_input})\r\n\r\n        # Check if this is a clarification response\r\n        if self._is_clarification_response(user_input):\r\n            return self._handle_clarification(user_input)\r\n\r\n        # Generate plan\r\n        plan = self.plan(user_input, self.scene_state, self.history)\r\n\r\n        if plan.needs_clarification:\r\n            response = plan.clarification_question\r\n        else:\r\n            # Execute plan (in real system)\r\n            response = self.explain_plan(plan)\r\n            response += "\\n\\nExecuting plan..."\r\n\r\n        self.history.append({"role": "assistant", "content": response})\r\n        return response\r\n\r\n    def _is_clarification_response(self, text: str) -> bool:\r\n        """Check if input is answering a previous question."""\r\n        # Simple heuristic - improve with NLU\r\n        short_responses = ["yes", "no", "the red one", "on the table", "left", "right"]\r\n        return text.lower().strip() in short_responses or len(text.split()) < 5\r\n\r\n    def _handle_clarification(self, response: str) -> str:\r\n        """Handle clarification response."""\r\n        # Re-plan with additional context\r\n        if len(self.history) >= 2:\r\n            original_command = self.history[-2]["content"]\r\n            augmented = f"{original_command}. User clarified: {response}"\r\n            plan = self.plan(augmented, self.scene_state)\r\n            return self.explain_plan(plan)\r\n        return "I\'m not sure what you\'re referring to. Could you repeat your request?"\r\n\r\n    def update_scene(self, objects: List[str], robot_location: str, held_object: Optional[str] = None):\r\n        """Update scene state for context-aware planning."""\r\n        self.scene_state = {\r\n            "objects": objects,\r\n            "robot_location": robot_location,\r\n            "held_object": held_object\r\n        }\r\n\r\n\r\ndef demo_llm_planner():\r\n    """Demonstrate LLM planning."""\r\n    print("\\n" + "=" * 60)\r\n    print("LLM Task Planner Demo")\r\n    print("=" * 60)\r\n\r\n    # Check for API key\r\n    if not os.getenv("OPENAI_API_KEY") and not OLLAMA_AVAILABLE:\r\n        print("\\nNote: Set OPENAI_API_KEY or install Ollama for full demo")\r\n        print("Showing example output instead:\\n")\r\n\r\n        # Example output\r\n        example_plan = TaskPlan(\r\n            goal="Get the red cup from the kitchen and bring it to me",\r\n            steps=[\r\n                {"action": "MOVE", "params": {"location": "kitchen"}, "description": "Navigate to kitchen"},\r\n                {"action": "LOOK", "params": {"target": "red cup"}, "description": "Search for red cup"},\r\n                {"action": "PICK", "params": {"object": "red cup"}, "description": "Pick up the red cup"},\r\n                {"action": "MOVE", "params": {"location": "user"}, "description": "Return to user"},\r\n                {"action": "SAY", "params": {"message": "Here is your cup"}, "description": "Confirm delivery"},\r\n            ],\r\n            reasoning="The user wants the red cup from the kitchen. I need to navigate there, find and pick up the cup, then return.",\r\n            confidence=0.9\r\n        )\r\n\r\n        print("Command: \'Get the red cup from the kitchen and bring it to me\'")\r\n        print("\\nGenerated Plan:")\r\n        for i, step in enumerate(example_plan.steps, 1):\r\n            print(f"  {i}. {step[\'action\']}: {step[\'description\']}")\r\n        print(f"\\nReasoning: {example_plan.reasoning}")\r\n        return\r\n\r\n    # Real demo with API\r\n    try:\r\n        planner = LLMPlanner(provider="openai", model="gpt-4o-mini")\r\n\r\n        test_commands = [\r\n            "Get me a drink from the fridge",\r\n            "Clean up the living room",\r\n            "Find my keys",\r\n        ]\r\n\r\n        for cmd in test_commands:\r\n            print(f"\\nCommand: \\"{cmd}\\"")\r\n            plan = planner.plan(cmd, scene_context={\r\n                "objects": ["cup", "bottle", "book", "remote"],\r\n                "robot_location": "living room",\r\n                "held_object": None\r\n            })\r\n            print(planner.explain_plan(plan))\r\n\r\n    except Exception as e:\r\n        print(f"Error: {e}")\r\n\r\n\r\nif __name__ == "__main__":\r\n    demo_llm_planner()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"handling-ambiguous-commands",children:"Handling Ambiguous Commands"}),"\n",(0,o.jsx)(e.p,{children:"Real-world commands are often ambiguous. The system must detect and resolve ambiguity."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\r\nAmbiguity Detection and Resolution for Robot Commands\r\n\r\nDetects unclear commands and generates clarifying questions.\r\n"""\r\n\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Optional, Tuple\r\nfrom enum import Enum\r\n\r\n\r\nclass AmbiguityType(Enum):\r\n    """Types of command ambiguity."""\r\n    MULTIPLE_OBJECTS = "multiple_objects"  # "Pick up the cup" (which cup?)\r\n    UNCLEAR_LOCATION = "unclear_location"  # "Put it there" (where?)\r\n    MISSING_OBJECT = "missing_object"      # "Pick it up" (what?)\r\n    VAGUE_ACTION = "vague_action"          # "Do something with the cup"\r\n    INCOMPLETE = "incomplete"               # "The red..." (unfinished)\r\n\r\n\r\n@dataclass\r\nclass AmbiguityResult:\r\n    """Result of ambiguity detection."""\r\n    is_ambiguous: bool\r\n    ambiguity_type: Optional[AmbiguityType]\r\n    clarifying_question: Optional[str]\r\n    options: List[str]\r\n\r\n\r\nclass AmbiguityResolver:\r\n    """\r\n    Detects and helps resolve ambiguous robot commands.\r\n    """\r\n\r\n    # Pronouns that need resolution\r\n    PRONOUNS = ["it", "that", "this", "them", "those", "there", "here"]\r\n\r\n    # Vague action words\r\n    VAGUE_ACTIONS = ["do", "something", "stuff", "thing", "handle", "deal with"]\r\n\r\n    def __init__(self, scene_objects: List[str] = None):\r\n        """\r\n        Initialize resolver.\r\n\r\n        Args:\r\n            scene_objects: List of objects currently visible\r\n        """\r\n        self.scene_objects = scene_objects or []\r\n        self.last_mentioned_object = None\r\n        self.last_mentioned_location = None\r\n\r\n    def check_ambiguity(\r\n        self,\r\n        command: str,\r\n        parsed_object: Optional[str] = None,\r\n        parsed_location: Optional[str] = None\r\n    ) -> AmbiguityResult:\r\n        """\r\n        Check if command is ambiguous.\r\n\r\n        Args:\r\n            command: Original command text\r\n            parsed_object: Extracted object (may be pronoun)\r\n            parsed_location: Extracted location\r\n\r\n        Returns:\r\n            AmbiguityResult with detection and clarification\r\n        """\r\n        command_lower = command.lower()\r\n\r\n        # Check for pronouns without clear reference\r\n        if self._has_unresolved_pronoun(command_lower, parsed_object):\r\n            return self._create_pronoun_ambiguity(parsed_object)\r\n\r\n        # Check for multiple matching objects in scene\r\n        if parsed_object:\r\n            matches = self._find_matching_objects(parsed_object)\r\n            if len(matches) > 1:\r\n                return self._create_multiple_objects_ambiguity(parsed_object, matches)\r\n\r\n        # Check for vague actions\r\n        if self._has_vague_action(command_lower):\r\n            return AmbiguityResult(\r\n                is_ambiguous=True,\r\n                ambiguity_type=AmbiguityType.VAGUE_ACTION,\r\n                clarifying_question=f"What would you like me to do with the {parsed_object or \'object\'}?",\r\n                options=["pick it up", "move it", "examine it", "describe it"]\r\n            )\r\n\r\n        # Check for unclear locations\r\n        if parsed_location in self.PRONOUNS:\r\n            return AmbiguityResult(\r\n                is_ambiguous=True,\r\n                ambiguity_type=AmbiguityType.UNCLEAR_LOCATION,\r\n                clarifying_question="Where exactly should I put it?",\r\n                options=["on the table", "on the shelf", "on the floor", "give it to you"]\r\n            )\r\n\r\n        # No ambiguity detected\r\n        return AmbiguityResult(\r\n            is_ambiguous=False,\r\n            ambiguity_type=None,\r\n            clarifying_question=None,\r\n            options=[]\r\n        )\r\n\r\n    def _has_unresolved_pronoun(self, command: str, parsed_object: Optional[str]) -> bool:\r\n        """Check if command uses pronouns without context."""\r\n        if parsed_object and parsed_object.lower() in self.PRONOUNS:\r\n            # Check if we have context from previous interaction\r\n            return self.last_mentioned_object is None\r\n        return False\r\n\r\n    def _create_pronoun_ambiguity(self, pronoun: str) -> AmbiguityResult:\r\n        """Create ambiguity result for unresolved pronoun."""\r\n        if self.scene_objects:\r\n            options = self.scene_objects[:4]  # Limit options\r\n            return AmbiguityResult(\r\n                is_ambiguous=True,\r\n                ambiguity_type=AmbiguityType.MISSING_OBJECT,\r\n                clarifying_question=f"What would you like me to pick up?",\r\n                options=options\r\n            )\r\n        return AmbiguityResult(\r\n            is_ambiguous=True,\r\n            ambiguity_type=AmbiguityType.MISSING_OBJECT,\r\n            clarifying_question="I don\'t see what you\'re referring to. What object do you mean?",\r\n            options=[]\r\n        )\r\n\r\n    def _find_matching_objects(self, object_name: str) -> List[str]:\r\n        """Find objects in scene matching the description."""\r\n        matches = []\r\n        for obj in self.scene_objects:\r\n            if object_name.lower() in obj.lower() or obj.lower() in object_name.lower():\r\n                matches.append(obj)\r\n        return matches\r\n\r\n    def _create_multiple_objects_ambiguity(\r\n        self,\r\n        object_name: str,\r\n        matches: List[str]\r\n    ) -> AmbiguityResult:\r\n        """Create ambiguity result for multiple matching objects."""\r\n        return AmbiguityResult(\r\n            is_ambiguous=True,\r\n            ambiguity_type=AmbiguityType.MULTIPLE_OBJECTS,\r\n            clarifying_question=f"I see multiple {object_name}s. Which one do you mean?",\r\n            options=matches\r\n        )\r\n\r\n    def _has_vague_action(self, command: str) -> bool:\r\n        """Check if command has vague action words."""\r\n        return any(word in command for word in self.VAGUE_ACTIONS)\r\n\r\n    def resolve_with_context(\r\n        self,\r\n        ambiguity: AmbiguityResult,\r\n        user_response: str\r\n    ) -> Tuple[str, str]:\r\n        """\r\n        Resolve ambiguity using user\'s clarifying response.\r\n\r\n        Args:\r\n            ambiguity: The detected ambiguity\r\n            user_response: User\'s response to clarifying question\r\n\r\n        Returns:\r\n            Tuple of (resolved_object, resolved_location)\r\n        """\r\n        response_lower = user_response.lower().strip()\r\n\r\n        # Check if response matches any option\r\n        for option in ambiguity.options:\r\n            if option.lower() in response_lower or response_lower in option.lower():\r\n                if ambiguity.ambiguity_type == AmbiguityType.MULTIPLE_OBJECTS:\r\n                    self.last_mentioned_object = option\r\n                    return (option, None)\r\n                elif ambiguity.ambiguity_type == AmbiguityType.UNCLEAR_LOCATION:\r\n                    self.last_mentioned_location = option\r\n                    return (None, option)\r\n\r\n        # Use response directly if no match\r\n        return (response_lower, None)\r\n\r\n\r\ndef demo_ambiguity():\r\n    """Demonstrate ambiguity detection."""\r\n    print("\\n" + "=" * 60)\r\n    print("Ambiguity Detection Demo")\r\n    print("=" * 60)\r\n\r\n    resolver = AmbiguityResolver(\r\n        scene_objects=["red cup", "blue cup", "water bottle", "book"]\r\n    )\r\n\r\n    test_cases = [\r\n        ("Pick up the cup", "cup", None),\r\n        ("Put it there", "it", "there"),\r\n        ("Get that thing", "thing", None),\r\n        ("Pick up the water bottle", "water bottle", None),  # Not ambiguous\r\n    ]\r\n\r\n    for command, obj, loc in test_cases:\r\n        result = resolver.check_ambiguity(command, obj, loc)\r\n        print(f"\\nCommand: \\"{command}\\"")\r\n        print(f"  Ambiguous: {result.is_ambiguous}")\r\n        if result.is_ambiguous:\r\n            print(f"  Type: {result.ambiguity_type.value}")\r\n            print(f"  Question: {result.clarifying_question}")\r\n            print(f"  Options: {result.options}")\r\n\r\n\r\nif __name__ == "__main__":\r\n    demo_ambiguity()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"complete-voice-controlled-robot-system",children:"Complete Voice-Controlled Robot System"}),"\n",(0,o.jsx)(e.p,{children:"Now let's integrate everything into a complete ROS 2 system:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nVoice-Controlled Robot System - Complete ROS 2 Integration\r\n\r\nIntegrates:\r\n- Speech recognition (Whisper/Google)\r\n- LLM planning (GPT/Ollama)\r\n- Vision (from Chapter 8)\r\n- Action execution\r\n\r\nROS 2 Topics:\r\n    Subscribed:\r\n        /camera/rgb/image_raw - Camera images\r\n    Published:\r\n        /cmd_vel - Velocity commands\r\n        /arm/goal - Arm movement goals\r\n        /robot/speech - Text-to-speech output\r\n\r\nPrerequisites:\r\n    pip install openai-whisper sounddevice numpy\r\n    pip install openai  # or ollama\r\n\r\nUsage:\r\n    ros2 run vla_package voice_robot_control\r\n\r\nAuthor: Physical AI & Humanoid Robotics Book\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom sensor_msgs.msg import Image\r\nimport threading\r\nimport queue\r\nfrom typing import Optional\r\nimport json\r\n\r\n# Import our modules (from previous sections)\r\n# from .speech_recognition import SpeechRecognizer\r\n# from .llm_planner import LLMPlanner, TaskPlan\r\n# from .ambiguity_resolver import AmbiguityResolver\r\n\r\n\r\nclass VoiceControlledRobot(Node):\r\n    """\r\n    Complete voice-controlled robot ROS 2 node.\r\n\r\n    Listens for voice commands, plans using LLM, and executes actions.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__("voice_robot_control")\r\n\r\n        # Parameters\r\n        self.declare_parameter("wake_word", "robot")\r\n        self.declare_parameter("llm_provider", "openai")\r\n        self.declare_parameter("llm_model", "gpt-4o-mini")\r\n        self.declare_parameter("whisper_model", "base")\r\n\r\n        wake_word = self.get_parameter("wake_word").value\r\n        llm_provider = self.get_parameter("llm_provider").value\r\n        llm_model = self.get_parameter("llm_model").value\r\n\r\n        # Initialize components\r\n        self.get_logger().info("Initializing voice control system...")\r\n\r\n        # Speech recognizer (simplified for demo)\r\n        self.speech_queue = queue.Queue()\r\n        self.is_listening = True\r\n\r\n        # LLM Planner (simplified initialization)\r\n        self.llm_available = False\r\n        try:\r\n            # self.planner = LLMPlanner(provider=llm_provider, model=llm_model)\r\n            self.llm_available = True\r\n            self.get_logger().info(f"LLM planner ready: {llm_provider}/{llm_model}")\r\n        except Exception as e:\r\n            self.get_logger().warn(f"LLM not available: {e}. Using rule-based fallback.")\r\n\r\n        # Scene state\r\n        self.visible_objects = []\r\n        self.robot_location = "home"\r\n        self.held_object = None\r\n\r\n        # Conversation state\r\n        self.conversation_history = []\r\n        self.pending_clarification = None\r\n\r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, "/cmd_vel", 10)\r\n        self.arm_goal_pub = self.create_publisher(PoseStamped, "/arm/goal", 10)\r\n        self.speech_pub = self.create_publisher(String, "/robot/speech", 10)\r\n        self.status_pub = self.create_publisher(String, "/robot/status", 10)\r\n\r\n        # Subscribers\r\n        self.speech_sub = self.create_subscription(\r\n            String, "/speech/text", self.speech_callback, 10\r\n        )\r\n        self.image_sub = self.create_subscription(\r\n            Image, "/camera/rgb/image_raw", self.image_callback, 10\r\n        )\r\n\r\n        # Processing timer\r\n        self.timer = self.create_timer(0.1, self.process_commands)\r\n\r\n        self.get_logger().info("Voice-controlled robot ready!")\r\n        self.speak("Hello! I\'m ready for your commands.")\r\n\r\n    def speech_callback(self, msg: String):\r\n        """Handle incoming speech text."""\r\n        text = msg.data.strip()\r\n        if text:\r\n            self.get_logger().info(f"Received: \'{text}\'")\r\n            self.speech_queue.put(text)\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Process camera images for scene understanding."""\r\n        # In real implementation, run object detection here\r\n        # self.visible_objects = self.detector.detect(image)\r\n        pass\r\n\r\n    def process_commands(self):\r\n        """Process queued voice commands."""\r\n        try:\r\n            command = self.speech_queue.get_nowait()\r\n            self.handle_command(command)\r\n        except queue.Empty:\r\n            pass\r\n\r\n    def handle_command(self, command: str):\r\n        """Process a voice command."""\r\n        self.publish_status(f"Processing: {command}")\r\n\r\n        # Check for system commands\r\n        if self._is_system_command(command):\r\n            self._handle_system_command(command)\r\n            return\r\n\r\n        # Check if this is a clarification response\r\n        if self.pending_clarification:\r\n            self._handle_clarification(command)\r\n            return\r\n\r\n        # Plan with LLM or fallback\r\n        if self.llm_available:\r\n            plan = self._plan_with_llm(command)\r\n        else:\r\n            plan = self._plan_rule_based(command)\r\n\r\n        # Check for ambiguity\r\n        if plan.get("needs_clarification"):\r\n            self.pending_clarification = plan\r\n            self.speak(plan["clarification_question"])\r\n            return\r\n\r\n        # Execute plan\r\n        self._execute_plan(plan)\r\n\r\n    def _is_system_command(self, command: str) -> bool:\r\n        """Check if command is a system command."""\r\n        system_words = ["stop", "cancel", "pause", "help", "status", "quit"]\r\n        return any(word in command.lower() for word in system_words)\r\n\r\n    def _handle_system_command(self, command: str):\r\n        """Handle system commands."""\r\n        cmd_lower = command.lower()\r\n\r\n        if "stop" in cmd_lower or "cancel" in cmd_lower:\r\n            self._stop_robot()\r\n            self.speak("Stopping.")\r\n\r\n        elif "help" in cmd_lower:\r\n            self.speak("I can pick up objects, navigate to locations, and follow your instructions. Try saying: pick up the cup, or go to the kitchen.")\r\n\r\n        elif "status" in cmd_lower:\r\n            status = f"I\'m at {self.robot_location}."\r\n            if self.held_object:\r\n                status += f" Holding {self.held_object}."\r\n            if self.visible_objects:\r\n                status += f" I see: {\', \'.join(self.visible_objects[:3])}."\r\n            self.speak(status)\r\n\r\n    def _handle_clarification(self, response: str):\r\n        """Handle clarification response."""\r\n        # Combine with original command\r\n        original = self.pending_clarification.get("original_command", "")\r\n        clarified = f"{original}. Specifically: {response}"\r\n\r\n        self.pending_clarification = None\r\n\r\n        # Re-plan with clarification\r\n        if self.llm_available:\r\n            plan = self._plan_with_llm(clarified)\r\n        else:\r\n            plan = self._plan_rule_based(clarified)\r\n\r\n        self._execute_plan(plan)\r\n\r\n    def _plan_with_llm(self, command: str) -> dict:\r\n        """Generate plan using LLM."""\r\n        # In real implementation:\r\n        # plan = self.planner.plan(command, {\r\n        #     "objects": self.visible_objects,\r\n        #     "robot_location": self.robot_location,\r\n        #     "held_object": self.held_object\r\n        # })\r\n        # return plan.__dict__\r\n\r\n        # Simplified for demo\r\n        return self._plan_rule_based(command)\r\n\r\n    def _plan_rule_based(self, command: str) -> dict:\r\n        """Rule-based planning fallback."""\r\n        cmd_lower = command.lower()\r\n        steps = []\r\n\r\n        # Parse action\r\n        if any(w in cmd_lower for w in ["pick", "grab", "get", "take"]):\r\n            # Extract object (simplified)\r\n            obj = self._extract_object(cmd_lower)\r\n            if obj:\r\n                steps = [\r\n                    {"action": "LOOK", "params": {"target": obj}},\r\n                    {"action": "PICK", "params": {"object": obj}},\r\n                    {"action": "SAY", "params": {"message": f"I have the {obj}"}},\r\n                ]\r\n            else:\r\n                return {\r\n                    "needs_clarification": True,\r\n                    "clarification_question": "What should I pick up?",\r\n                    "original_command": command\r\n                }\r\n\r\n        elif any(w in cmd_lower for w in ["go", "move", "navigate"]):\r\n            loc = self._extract_location(cmd_lower)\r\n            if loc:\r\n                steps = [\r\n                    {"action": "MOVE", "params": {"location": loc}},\r\n                    {"action": "SAY", "params": {"message": f"I\'m at the {loc}"}},\r\n                ]\r\n            else:\r\n                return {\r\n                    "needs_clarification": True,\r\n                    "clarification_question": "Where should I go?",\r\n                    "original_command": command\r\n                }\r\n\r\n        elif any(w in cmd_lower for w in ["put", "place", "set"]):\r\n            loc = self._extract_location(cmd_lower)\r\n            if loc and self.held_object:\r\n                steps = [\r\n                    {"action": "PLACE", "params": {"location": loc}},\r\n                    {"action": "SAY", "params": {"message": f"Placed {self.held_object} on {loc}"}},\r\n                ]\r\n            elif not self.held_object:\r\n                return {\r\n                    "needs_clarification": True,\r\n                    "clarification_question": "I\'m not holding anything. What should I pick up first?",\r\n                    "original_command": command\r\n                }\r\n\r\n        elif any(w in cmd_lower for w in ["find", "look", "search", "where"]):\r\n            obj = self._extract_object(cmd_lower)\r\n            steps = [\r\n                {"action": "LOOK", "params": {"target": obj or "around"}},\r\n            ]\r\n\r\n        else:\r\n            return {\r\n                "needs_clarification": True,\r\n                "clarification_question": "I didn\'t understand that. Could you try rephrasing?",\r\n                "original_command": command\r\n            }\r\n\r\n        return {\r\n            "goal": command,\r\n            "steps": steps,\r\n            "needs_clarification": False\r\n        }\r\n\r\n    def _extract_object(self, text: str) -> Optional[str]:\r\n        """Extract object from command text."""\r\n        objects = ["cup", "bottle", "book", "phone", "ball", "box", "apple", "remote"]\r\n        for obj in objects:\r\n            if obj in text:\r\n                return obj\r\n        return None\r\n\r\n    def _extract_location(self, text: str) -> Optional[str]:\r\n        """Extract location from command text."""\r\n        locations = ["table", "kitchen", "living room", "shelf", "floor", "counter", "desk"]\r\n        for loc in locations:\r\n            if loc in text:\r\n                return loc\r\n        # Check for prepositions\r\n        import re\r\n        match = re.search(r"(?:to|on|at|in)\\s+(?:the\\s+)?(\\w+)", text)\r\n        if match:\r\n            return match.group(1)\r\n        return None\r\n\r\n    def _execute_plan(self, plan: dict):\r\n        """Execute a generated plan."""\r\n        steps = plan.get("steps", [])\r\n\r\n        self.speak(f"Executing: {plan.get(\'goal\', \'task\')}")\r\n\r\n        for step in steps:\r\n            action = step.get("action")\r\n            params = step.get("params", {})\r\n\r\n            self.get_logger().info(f"Executing: {action} with {params}")\r\n\r\n            if action == "MOVE":\r\n                self._execute_move(params.get("location"))\r\n            elif action == "PICK":\r\n                self._execute_pick(params.get("object"))\r\n            elif action == "PLACE":\r\n                self._execute_place(params.get("location"))\r\n            elif action == "LOOK":\r\n                self._execute_look(params.get("target"))\r\n            elif action == "SAY":\r\n                self.speak(params.get("message", ""))\r\n            elif action == "WAIT":\r\n                import time\r\n                time.sleep(params.get("seconds", 1))\r\n\r\n    def _execute_move(self, location: str):\r\n        """Execute navigation to location."""\r\n        self.publish_status(f"Moving to {location}")\r\n\r\n        # Send velocity command (simplified)\r\n        twist = Twist()\r\n        twist.linear.x = 0.3\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n        # In real implementation: use Nav2 action\r\n        self.robot_location = location\r\n\r\n    def _execute_pick(self, obj: str):\r\n        """Execute pick action."""\r\n        self.publish_status(f"Picking up {obj}")\r\n\r\n        # In real implementation: use MoveIt2\r\n        self.held_object = obj\r\n\r\n    def _execute_place(self, location: str):\r\n        """Execute place action."""\r\n        self.publish_status(f"Placing on {location}")\r\n\r\n        # In real implementation: use MoveIt2\r\n        placed = self.held_object\r\n        self.held_object = None\r\n\r\n    def _execute_look(self, target: str):\r\n        """Execute look/search action."""\r\n        self.publish_status(f"Looking for {target}")\r\n\r\n        # In real implementation: pan camera, run detection\r\n        if target in self.visible_objects:\r\n            self.speak(f"I see the {target}")\r\n        else:\r\n            self.speak(f"I don\'t see a {target} right now")\r\n\r\n    def _stop_robot(self):\r\n        """Emergency stop."""\r\n        twist = Twist()\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def speak(self, text: str):\r\n        """Publish text for text-to-speech."""\r\n        msg = String()\r\n        msg.data = text\r\n        self.speech_pub.publish(msg)\r\n        self.get_logger().info(f"Speaking: {text}")\r\n\r\n    def publish_status(self, status: str):\r\n        """Publish status update."""\r\n        msg = String()\r\n        msg.data = status\r\n        self.status_pub.publish(msg)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceControlledRobot()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"Speech Recognition"}),": Convert voice to text using Whisper or Google Speech API"]}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"LLM Integration"}),": Use GPT or local LLMs for intelligent task planning"]}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"Ambiguity Handling"}),": Detect unclear commands and ask clarifying questions"]}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"Conversational Context"}),": Maintain conversation history for multi-turn interactions"]}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"ROS 2 Integration"}),": Build a complete voice-controlled robot system"]}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"End-to-End Pipeline"}),": Voice \u2192 Text \u2192 Plan \u2192 Execute \u2192 Feedback"]}),"\n",(0,o.jsx)(e.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"LLMs enable complex reasoning"})," beyond simple keyword matching"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speech recognition"})," is now accurate enough for robot control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Ambiguity resolution"})," is critical for natural interaction"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context memory"})," enables multi-turn conversations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Fallback strategies"})," ensure robustness when AI fails"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"exercise-1-add-wake-word-detection",children:"Exercise 1: Add Wake Word Detection"}),"\n",(0,o.jsx)(e.p,{children:'Modify the speech recognizer to only process commands after hearing "Hey Robot":'}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Hint: Listen continuously, check for wake word, then process next utterance\n"})}),"\n",(0,o.jsx)(e.h3,{id:"exercise-2-implement-text-to-speech",children:"Exercise 2: Implement Text-to-Speech"}),"\n",(0,o.jsx)(e.p,{children:"Add speech synthesis for robot responses:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Use pyttsx3 or gTTS for text-to-speech\r\npip install pyttsx3\n"})}),"\n",(0,o.jsx)(e.h3,{id:"exercise-3-multi-object-commands",children:"Exercise 3: Multi-Object Commands"}),"\n",(0,o.jsx)(e.p,{children:"Extend the planner to handle commands like:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Pick up all the cups"'}),"\n",(0,o.jsx)(e.li,{children:'"Move the red and blue bottles to the shelf"'}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-4-error-recovery",children:"Exercise 4: Error Recovery"}),"\n",(0,o.jsx)(e.p,{children:"Add error handling when actions fail:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Retry logic with different approach"}),"\n",(0,o.jsx)(e.li,{children:"Ask user for help"}),"\n",(0,o.jsx)(e.li,{children:"Report what went wrong"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"challenge-teach-new-commands",children:"Challenge: Teach New Commands"}),"\n",(0,o.jsx)(e.p,{children:"Build a system where users can teach the robot new tasks:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"\"When I say 'clean up', pick up all objects and put them on the shelf\""}),"\n",(0,o.jsx)(e.li,{children:"Store learned behaviors for later use"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"up-next",children:"Up Next"}),"\n",(0,o.jsxs)(e.p,{children:["In ",(0,o.jsx)(e.strong,{children:"Chapter 10: Building the Humanoid Robot Capstone"}),", you'll combine everything:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"ROS 2 control from Chapters 3-4"}),"\n",(0,o.jsx)(e.li,{children:"Isaac Sim simulation from Chapters 6-7"}),"\n",(0,o.jsx)(e.li,{children:"VLA systems from Chapters 8-9"}),"\n",(0,o.jsx)(e.li,{children:"Build a complete voice-controlled humanoid robot!"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper"})," - Speech Recognition"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://ollama.ai/",children:"Ollama"})," - Local LLM Hosting"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs",children:"OpenAI API"})," - GPT Integration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://github.com/ros-drivers/audio_common",children:"ROS 2 Audio Common"})," - Audio in ROS 2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://moveit.ros.org/",children:"MoveIt 2"})," - Motion Planning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://navigation.ros.org/",children:"Nav2"})," - Navigation Stack"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Sources:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2212.04356",children:"OpenAI Whisper Paper"})}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://say-can.github.io/",children:"SayCan: Google Research"})," - LLM + Robot Affordances"]}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2201.07207",children:"Language Models as Zero-Shot Planners"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Documentation"})}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>a});var t=r(6540);const o={},i=t.createContext(o);function s(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);