"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[319],{4180:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter8_vla_systems","title":"Chapter 8: Vision-Language-Action Systems","description":"Build VLA systems combining computer vision, natural language processing, and action planning for intelligent robot control","source":"@site/docs/chapter8_vla_systems.md","sourceDirName":".","slug":"/chapter8_vla_systems","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter8_vla_systems","draft":false,"unlisted":false,"editUrl":"https://github.com/Salwagull/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter8_vla_systems.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Chapter 8: Vision-Language-Action Systems","description":"Build VLA systems combining computer vision, natural language processing, and action planning for intelligent robot control","keywords":["vla","vision","language","action","yolo","object detection","nlp","spacy","robot control","ros2"]},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: Isaac Sim for Perception and Navigation","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter7_isaac_perception_nav"},"next":{"title":"Chapter 9: LLM Planning and Voice Commands","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter9_llm_voice_commands"}}');var i=r(4848),o=r(8453);const a={sidebar_position:8,title:"Chapter 8: Vision-Language-Action Systems",description:"Build VLA systems combining computer vision, natural language processing, and action planning for intelligent robot control",keywords:["vla","vision","language","action","yolo","object detection","nlp","spacy","robot control","ros2"]},s="Chapter 8: Vision-Language-Action Systems",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to VLA Systems",id:"introduction-to-vla-systems",level:2},{value:"Why VLA Matters for Physical AI",id:"why-vla-matters-for-physical-ai",level:3},{value:"VLA Architecture Overview",id:"vla-architecture-overview",level:2},{value:"1. Vision Module",id:"1-vision-module",level:3},{value:"2. Language Module",id:"2-language-module",level:3},{value:"3. Action Module",id:"3-action-module",level:3},{value:"Vision Processing for Robotic Perception",id:"vision-processing-for-robotic-perception",level:2},{value:"Object Detection with YOLO",id:"object-detection-with-yolo",level:3},{value:"Depth Perception for 3D Understanding",id:"depth-perception-for-3d-understanding",level:3},{value:"Language Understanding for Robot Commands",id:"language-understanding-for-robot-commands",level:2},{value:"Intent Classification and Entity Extraction",id:"intent-classification-and-entity-extraction",level:3},{value:"Action Planning and Execution",id:"action-planning-and-execution",level:2},{value:"From Intent to Robot Motion",id:"from-intent-to-robot-motion",level:3},{value:"Integrating VLA with ROS 2",id:"integrating-vla-with-ros-2",level:2},{value:"VLA System Architecture Diagram",id:"vla-system-architecture-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Extend Object Detection",id:"exercise-1-extend-object-detection",level:3},{value:"Exercise 2: Improve Language Understanding",id:"exercise-2-improve-language-understanding",level:3},{value:"Exercise 3: Build a Pick-and-Place Demo",id:"exercise-3-build-a-pick-and-place-demo",level:3},{value:"Challenge: Add Error Recovery",id:"challenge-add-error-recovery",level:3},{value:"Up Next",id:"up-next",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-8-vision-language-action-systems",children:"Chapter 8: Vision-Language-Action Systems"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you will:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand the architecture and components of Vision-Language-Action (VLA) systems"}),"\n",(0,i.jsx)(e.li,{children:"Implement vision processing pipelines for robotic perception"}),"\n",(0,i.jsx)(e.li,{children:"Integrate language models for understanding natural language commands"}),"\n",(0,i.jsx)(e.li,{children:"Build action planning systems that translate commands into robot motions"}),"\n",(0,i.jsx)(e.li,{children:"Connect VLA components with ROS 2 for end-to-end robot control"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-vla-systems",children:"Introduction to VLA Systems"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Vision-Language-Action (VLA)"})," systems represent the cutting edge of Physical AI, enabling robots to understand their environment through vision, receive instructions in natural language, and execute appropriate actions. These systems bridge the gap between human intent and robot behavior."]}),"\n",(0,i.jsx)(e.h3,{id:"why-vla-matters-for-physical-ai",children:"Why VLA Matters for Physical AI"}),"\n",(0,i.jsx)(e.p,{children:"Traditional robots require precise programming for every task. VLA systems enable:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural interaction"}),": Communicate with robots using everyday language"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Flexible task execution"}),": Handle novel situations without reprogramming"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Contextual understanding"}),": Consider visual context when interpreting commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Generalization"}),": Apply learned behaviors to new environments and objects"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    Vision-Language-Action Pipeline                       \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                                          \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\r\n\u2502  \u2502  Vision \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Language   \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Action    \u2502                \u2502\r\n\u2502  \u2502 Module  \u2502      \u2502    Model     \u2502      \u2502  Planner    \u2502                \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\r\n\u2502       \u2502                 \u2502                     \u2502                         \u2502\r\n\u2502       \u25bc                 \u25bc                     \u25bc                         \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\r\n\u2502  \u2502 Object  \u2502      \u2502   Intent     \u2502      \u2502   Motion    \u2502                \u2502\r\n\u2502  \u2502Detection\u2502      \u2502 Recognition  \u2502      \u2502  Commands   \u2502                \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\r\n\u2502       \u2502                 \u2502                     \u2502                         \u2502\r\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\r\n\u2502                         \u2502                                                \u2502\r\n\u2502                         \u25bc                                                \u2502\r\n\u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                        \u2502\r\n\u2502                  \u2502  ROS 2      \u2502                                        \u2502\r\n\u2502                  \u2502  Robot      \u2502                                        \u2502\r\n\u2502                  \u2502  Control    \u2502                                        \u2502\r\n\u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h2,{id:"vla-architecture-overview",children:"VLA Architecture Overview"}),"\n",(0,i.jsx)(e.p,{children:"A complete VLA system consists of three interconnected modules:"}),"\n",(0,i.jsx)(e.h3,{id:"1-vision-module",children:"1. Vision Module"}),"\n",(0,i.jsx)(e.p,{children:"The vision module processes visual input from cameras to understand the robot's environment."}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Key capabilities:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Object detection"}),": Identify and locate objects (YOLO, DETR)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Semantic segmentation"}),": Classify every pixel in an image"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Depth estimation"}),": Understand 3D scene structure"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Scene understanding"}),": Recognize spatial relationships"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"2-language-module",children:"2. Language Module"}),"\n",(0,i.jsx)(e.p,{children:"The language module interprets natural language commands and extracts actionable intent."}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Key capabilities:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Command parsing"}),": Extract verbs, objects, and modifiers"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Intent classification"}),": Determine what action is requested"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Entity extraction"}),": Identify referenced objects and locations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Disambiguation"}),": Resolve unclear references using context"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"3-action-module",children:"3. Action Module"}),"\n",(0,i.jsx)(e.p,{children:"The action module translates understood intent into executable robot commands."}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Key capabilities:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task planning"}),": Break commands into action sequences"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Motion planning"}),": Generate collision-free trajectories"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Skill execution"}),": Perform atomic actions (grasp, move, place)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feedback processing"}),": Adjust based on execution results"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"vision-processing-for-robotic-perception",children:"Vision Processing for Robotic Perception"}),"\n",(0,i.jsx)(e.h3,{id:"object-detection-with-yolo",children:"Object Detection with YOLO"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"YOLO (You Only Look Once)"})," is a real-time object detection system ideal for robotics."]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nVision module for VLA system using YOLOv8 for object detection.\r\n\r\nPrerequisites:\r\n    pip install ultralytics opencv-python\r\n\r\nUsage:\r\n    python vision_detector.py\r\n"""\r\n\r\nimport cv2\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Tuple, Optional\r\n\r\n# Try to import ultralytics, provide fallback\r\ntry:\r\n    from ultralytics import YOLO\r\n    YOLO_AVAILABLE = True\r\nexcept ImportError:\r\n    YOLO_AVAILABLE = False\r\n    print("Warning: ultralytics not installed. Install with: pip install ultralytics")\r\n\r\n\r\n@dataclass\r\nclass Detection:\r\n    """Represents a detected object."""\r\n    class_name: str\r\n    confidence: float\r\n    bbox: Tuple[int, int, int, int]  # x1, y1, x2, y2\r\n    center: Tuple[int, int]\r\n    area: int\r\n\r\n\r\nclass VisionDetector:\r\n    """\r\n    Vision module for detecting objects in camera images.\r\n\r\n    Uses YOLOv8 for real-time object detection with support for\r\n    80 COCO classes including common household objects.\r\n    """\r\n\r\n    def __init__(self, model_path: str = "yolov8n.pt"):\r\n        """\r\n        Initialize the vision detector.\r\n\r\n        Args:\r\n            model_path: Path to YOLO model (downloads automatically if not found)\r\n        """\r\n        if not YOLO_AVAILABLE:\r\n            raise RuntimeError("ultralytics package required. Install with: pip install ultralytics")\r\n\r\n        self.model = YOLO(model_path)\r\n        self.confidence_threshold = 0.5\r\n\r\n        # Common objects robots interact with\r\n        self.target_classes = [\r\n            "bottle", "cup", "bowl", "apple", "banana", "orange",\r\n            "book", "keyboard", "mouse", "remote", "cell phone",\r\n            "chair", "couch", "potted plant", "bed", "dining table",\r\n            "laptop", "scissors", "toothbrush", "teddy bear"\r\n        ]\r\n\r\n    def detect(self, image: np.ndarray) -> List[Detection]:\r\n        """\r\n        Detect objects in an image.\r\n\r\n        Args:\r\n            image: BGR image from camera (OpenCV format)\r\n\r\n        Returns:\r\n            List of Detection objects\r\n        """\r\n        results = self.model(image, verbose=False)[0]\r\n        detections = []\r\n\r\n        for box in results.boxes:\r\n            class_id = int(box.cls[0])\r\n            class_name = self.model.names[class_id]\r\n            confidence = float(box.conf[0])\r\n\r\n            if confidence < self.confidence_threshold:\r\n                continue\r\n\r\n            # Get bounding box\r\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\r\n\r\n            # Calculate center and area\r\n            center_x = (x1 + x2) // 2\r\n            center_y = (y1 + y2) // 2\r\n            area = (x2 - x1) * (y2 - y1)\r\n\r\n            detections.append(Detection(\r\n                class_name=class_name,\r\n                confidence=confidence,\r\n                bbox=(x1, y1, x2, y2),\r\n                center=(center_x, center_y),\r\n                area=area\r\n            ))\r\n\r\n        return detections\r\n\r\n    def find_object(self, image: np.ndarray, object_name: str) -> Optional[Detection]:\r\n        """\r\n        Find a specific object in the image.\r\n\r\n        Args:\r\n            image: Camera image\r\n            object_name: Name of object to find (e.g., "cup", "bottle")\r\n\r\n        Returns:\r\n            Detection if found, None otherwise\r\n        """\r\n        detections = self.detect(image)\r\n\r\n        # Find best match for requested object\r\n        matches = [d for d in detections if object_name.lower() in d.class_name.lower()]\r\n\r\n        if not matches:\r\n            return None\r\n\r\n        # Return highest confidence match\r\n        return max(matches, key=lambda d: d.confidence)\r\n\r\n    def annotate_image(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\r\n        """Draw bounding boxes and labels on image."""\r\n        annotated = image.copy()\r\n\r\n        for det in detections:\r\n            x1, y1, x2, y2 = det.bbox\r\n\r\n            # Draw box\r\n            cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\r\n\r\n            # Draw label\r\n            label = f"{det.class_name}: {det.confidence:.2f}"\r\n            cv2.putText(annotated, label, (x1, y1 - 10),\r\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n\r\n        return annotated\r\n\r\n\r\ndef demo_vision_detector():\r\n    """Demonstrate the vision detector with webcam."""\r\n    detector = VisionDetector()\r\n    cap = cv2.VideoCapture(0)\r\n\r\n    print("Vision Detector Demo")\r\n    print("Press \'q\' to quit")\r\n\r\n    while True:\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            break\r\n\r\n        # Detect objects\r\n        detections = detector.detect(frame)\r\n\r\n        # Annotate frame\r\n        annotated = detector.annotate_image(frame, detections)\r\n\r\n        # Display\r\n        cv2.imshow("VLA Vision Module", annotated)\r\n\r\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n\r\n\r\nif __name__ == "__main__":\r\n    demo_vision_detector()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"depth-perception-for-3d-understanding",children:"Depth Perception for 3D Understanding"}),"\n",(0,i.jsx)(e.p,{children:"Robots need depth information to plan movements in 3D space."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'"""\r\nDepth estimation for VLA systems using MiDaS or camera depth sensors.\r\n"""\r\n\r\nimport numpy as np\r\nfrom typing import Tuple, Optional\r\n\r\ntry:\r\n    import torch\r\n    TORCH_AVAILABLE = True\r\nexcept ImportError:\r\n    TORCH_AVAILABLE = False\r\n\r\n\r\nclass DepthEstimator:\r\n    """\r\n    Estimates depth from RGB images using MiDaS or processes\r\n    depth camera data directly.\r\n    """\r\n\r\n    def __init__(self, use_gpu: bool = True):\r\n        """Initialize depth estimator."""\r\n        self.device = "cuda" if use_gpu and TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"\r\n        self.model = None\r\n        self.transform = None\r\n\r\n    def load_midas(self, model_type: str = "MiDaS_small"):\r\n        """\r\n        Load MiDaS depth estimation model.\r\n\r\n        Args:\r\n            model_type: "DPT_Large", "DPT_Hybrid", or "MiDaS_small"\r\n        """\r\n        if not TORCH_AVAILABLE:\r\n            raise RuntimeError("PyTorch required for MiDaS. Install with: pip install torch")\r\n\r\n        self.model = torch.hub.load("intel-isl/MiDaS", model_type)\r\n        self.model.to(self.device).eval()\r\n\r\n        midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")\r\n        self.transform = midas_transforms.small_transform\r\n\r\n    def estimate_depth(self, rgb_image: np.ndarray) -> np.ndarray:\r\n        """\r\n        Estimate depth from RGB image.\r\n\r\n        Args:\r\n            rgb_image: RGB image (H, W, 3)\r\n\r\n        Returns:\r\n            Depth map (H, W) with relative depth values\r\n        """\r\n        if self.model is None:\r\n            self.load_midas()\r\n\r\n        import cv2\r\n\r\n        # Transform image\r\n        input_batch = self.transform(rgb_image).to(self.device)\r\n\r\n        # Predict\r\n        with torch.no_grad():\r\n            prediction = self.model(input_batch)\r\n            prediction = torch.nn.functional.interpolate(\r\n                prediction.unsqueeze(1),\r\n                size=rgb_image.shape[:2],\r\n                mode="bicubic",\r\n                align_corners=False,\r\n            ).squeeze()\r\n\r\n        depth_map = prediction.cpu().numpy()\r\n        return depth_map\r\n\r\n    def get_3d_position(\r\n        self,\r\n        pixel: Tuple[int, int],\r\n        depth_value: float,\r\n        camera_intrinsics: dict\r\n    ) -> Tuple[float, float, float]:\r\n        """\r\n        Convert 2D pixel + depth to 3D position.\r\n\r\n        Args:\r\n            pixel: (x, y) pixel coordinates\r\n            depth_value: Depth in meters\r\n            camera_intrinsics: Camera parameters (fx, fy, cx, cy)\r\n\r\n        Returns:\r\n            (X, Y, Z) position in camera frame (meters)\r\n        """\r\n        x, y = pixel\r\n        fx = camera_intrinsics["fx"]\r\n        fy = camera_intrinsics["fy"]\r\n        cx = camera_intrinsics["cx"]\r\n        cy = camera_intrinsics["cy"]\r\n\r\n        # Back-project to 3D\r\n        Z = depth_value\r\n        X = (x - cx) * Z / fx\r\n        Y = (y - cy) * Z / fy\r\n\r\n        return (X, Y, Z)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"language-understanding-for-robot-commands",children:"Language Understanding for Robot Commands"}),"\n",(0,i.jsx)(e.h3,{id:"intent-classification-and-entity-extraction",children:"Intent Classification and Entity Extraction"}),"\n",(0,i.jsx)(e.p,{children:"The language module parses natural language commands to understand what the user wants."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nLanguage understanding module for VLA systems.\r\n\r\nParses natural language commands into structured intents and entities.\r\n\r\nPrerequisites:\r\n    pip install spacy\r\n    python -m spacy download en_core_web_sm\r\n"""\r\n\r\nimport re\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Optional, Dict, Any\r\nfrom enum import Enum\r\n\r\ntry:\r\n    import spacy\r\n    SPACY_AVAILABLE = True\r\nexcept ImportError:\r\n    SPACY_AVAILABLE = False\r\n\r\n\r\nclass RobotAction(Enum):\r\n    """Supported robot actions."""\r\n    PICK = "pick"\r\n    PLACE = "place"\r\n    MOVE = "move"\r\n    PUSH = "push"\r\n    POUR = "pour"\r\n    OPEN = "open"\r\n    CLOSE = "close"\r\n    POINT = "point"\r\n    LOOK = "look"\r\n    STOP = "stop"\r\n    UNKNOWN = "unknown"\r\n\r\n\r\n@dataclass\r\nclass ParsedCommand:\r\n    """Structured representation of a parsed command."""\r\n    action: RobotAction\r\n    target_object: Optional[str]\r\n    target_location: Optional[str]\r\n    modifiers: Dict[str, Any]\r\n    confidence: float\r\n    original_text: str\r\n\r\n\r\nclass LanguageParser:\r\n    """\r\n    Parses natural language commands for robot control.\r\n\r\n    Supports commands like:\r\n    - "Pick up the red cup"\r\n    - "Move to the table"\r\n    - "Place the bottle on the shelf"\r\n    - "Pour water into the glass"\r\n    """\r\n\r\n    # Action verb patterns\r\n    ACTION_PATTERNS = {\r\n        RobotAction.PICK: r"\\b(pick|grab|grasp|take|get|lift)\\b",\r\n        RobotAction.PLACE: r"\\b(place|put|set|drop|release)\\b",\r\n        RobotAction.MOVE: r"\\b(move|go|navigate|drive|travel)\\b",\r\n        RobotAction.PUSH: r"\\b(push|shove|slide)\\b",\r\n        RobotAction.POUR: r"\\b(pour|fill|empty)\\b",\r\n        RobotAction.OPEN: r"\\b(open|unlock)\\b",\r\n        RobotAction.CLOSE: r"\\b(close|shut|lock)\\b",\r\n        RobotAction.POINT: r"\\b(point|indicate|show)\\b",\r\n        RobotAction.LOOK: r"\\b(look|see|find|locate|search)\\b",\r\n        RobotAction.STOP: r"\\b(stop|halt|freeze|pause)\\b",\r\n    }\r\n\r\n    # Location prepositions\r\n    LOCATION_PREPS = ["on", "to", "into", "onto", "in", "at", "near", "by", "beside", "next to"]\r\n\r\n    # Color modifiers\r\n    COLORS = ["red", "blue", "green", "yellow", "black", "white", "orange", "purple", "pink", "brown"]\r\n\r\n    # Size modifiers\r\n    SIZES = ["big", "small", "large", "tiny", "medium", "tall", "short"]\r\n\r\n    def __init__(self):\r\n        """Initialize the language parser."""\r\n        self.nlp = None\r\n        if SPACY_AVAILABLE:\r\n            try:\r\n                self.nlp = spacy.load("en_core_web_sm")\r\n            except OSError:\r\n                print("Warning: spaCy model not found. Using regex-only parsing.")\r\n\r\n    def parse(self, command: str) -> ParsedCommand:\r\n        """\r\n        Parse a natural language command.\r\n\r\n        Args:\r\n            command: Natural language command (e.g., "Pick up the red cup")\r\n\r\n        Returns:\r\n            ParsedCommand with extracted action, object, location, and modifiers\r\n        """\r\n        command_lower = command.lower().strip()\r\n\r\n        # Extract action\r\n        action = self._extract_action(command_lower)\r\n\r\n        # Extract modifiers (colors, sizes)\r\n        modifiers = self._extract_modifiers(command_lower)\r\n\r\n        # Extract target object and location\r\n        target_object = self._extract_object(command_lower, action)\r\n        target_location = self._extract_location(command_lower)\r\n\r\n        # Calculate confidence based on extraction success\r\n        confidence = self._calculate_confidence(action, target_object, target_location)\r\n\r\n        return ParsedCommand(\r\n            action=action,\r\n            target_object=target_object,\r\n            target_location=target_location,\r\n            modifiers=modifiers,\r\n            confidence=confidence,\r\n            original_text=command\r\n        )\r\n\r\n    def _extract_action(self, text: str) -> RobotAction:\r\n        """Extract the primary action from the command."""\r\n        for action, pattern in self.ACTION_PATTERNS.items():\r\n            if re.search(pattern, text, re.IGNORECASE):\r\n                return action\r\n        return RobotAction.UNKNOWN\r\n\r\n    def _extract_modifiers(self, text: str) -> Dict[str, Any]:\r\n        """Extract color, size, and other modifiers."""\r\n        modifiers = {}\r\n\r\n        for color in self.COLORS:\r\n            if color in text:\r\n                modifiers["color"] = color\r\n                break\r\n\r\n        for size in self.SIZES:\r\n            if size in text:\r\n                modifiers["size"] = size\r\n                break\r\n\r\n        return modifiers\r\n\r\n    def _extract_object(self, text: str, action: RobotAction) -> Optional[str]:\r\n        """Extract the target object from the command."""\r\n        if self.nlp:\r\n            return self._extract_object_spacy(text)\r\n        return self._extract_object_regex(text, action)\r\n\r\n    def _extract_object_regex(self, text: str, action: RobotAction) -> Optional[str]:\r\n        """Regex-based object extraction."""\r\n        # Common objects\r\n        objects = [\r\n            "cup", "bottle", "glass", "mug", "bowl", "plate",\r\n            "book", "phone", "remote", "pen", "pencil",\r\n            "apple", "banana", "orange", "ball", "box",\r\n            "chair", "table", "door", "drawer", "shelf"\r\n        ]\r\n\r\n        for obj in objects:\r\n            if obj in text:\r\n                return obj\r\n\r\n        # Pattern: "the [adjective] [noun]"\r\n        pattern = r"the\\s+(?:\\w+\\s+)?(\\w+)(?:\\s|$)"\r\n        match = re.search(pattern, text)\r\n        if match:\r\n            return match.group(1)\r\n\r\n        return None\r\n\r\n    def _extract_object_spacy(self, text: str) -> Optional[str]:\r\n        """spaCy-based object extraction using NLP."""\r\n        doc = self.nlp(text)\r\n\r\n        # Find direct objects\r\n        for token in doc:\r\n            if token.dep_ == "dobj" or token.dep_ == "pobj":\r\n                return token.text\r\n\r\n        # Fall back to nouns\r\n        nouns = [token.text for token in doc if token.pos_ == "NOUN"]\r\n        return nouns[0] if nouns else None\r\n\r\n    def _extract_location(self, text: str) -> Optional[str]:\r\n        """Extract target location from the command."""\r\n        for prep in self.LOCATION_PREPS:\r\n            pattern = rf"{prep}\\s+(?:the\\s+)?(\\w+)"\r\n            match = re.search(pattern, text, re.IGNORECASE)\r\n            if match:\r\n                return match.group(1)\r\n        return None\r\n\r\n    def _calculate_confidence(\r\n        self,\r\n        action: RobotAction,\r\n        target_object: Optional[str],\r\n        target_location: Optional[str]\r\n    ) -> float:\r\n        """Calculate confidence score for the parse."""\r\n        score = 0.0\r\n\r\n        if action != RobotAction.UNKNOWN:\r\n            score += 0.5\r\n\r\n        if target_object:\r\n            score += 0.3\r\n\r\n        if target_location:\r\n            score += 0.2\r\n\r\n        return min(score, 1.0)\r\n\r\n\r\ndef demo_language_parser():\r\n    """Demonstrate the language parser."""\r\n    parser = LanguageParser()\r\n\r\n    test_commands = [\r\n        "Pick up the red cup",\r\n        "Move to the kitchen table",\r\n        "Place the bottle on the shelf",\r\n        "Find the blue ball",\r\n        "Pour water into the glass",\r\n        "Open the drawer",\r\n        "Look at the apple",\r\n    ]\r\n\r\n    print("Language Parser Demo")\r\n    print("=" * 60)\r\n\r\n    for cmd in test_commands:\r\n        result = parser.parse(cmd)\r\n        print(f"\\nCommand: \\"{cmd}\\"")\r\n        print(f"  Action: {result.action.value}")\r\n        print(f"  Object: {result.target_object}")\r\n        print(f"  Location: {result.target_location}")\r\n        print(f"  Modifiers: {result.modifiers}")\r\n        print(f"  Confidence: {result.confidence:.2f}")\r\n\r\n\r\nif __name__ == "__main__":\r\n    demo_language_parser()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"action-planning-and-execution",children:"Action Planning and Execution"}),"\n",(0,i.jsx)(e.h3,{id:"from-intent-to-robot-motion",children:"From Intent to Robot Motion"}),"\n",(0,i.jsx)(e.p,{children:"The action planner translates parsed commands into executable robot behaviors."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nAction planning module for VLA systems.\r\n\r\nTranslates parsed commands into sequences of robot actions.\r\n"""\r\n\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Optional, Tuple\r\nfrom enum import Enum\r\nimport numpy as np\r\n\r\n\r\nclass SkillType(Enum):\r\n    """Primitive robot skills."""\r\n    NAVIGATE = "navigate"\r\n    APPROACH = "approach"\r\n    GRASP = "grasp"\r\n    LIFT = "lift"\r\n    MOVE_ARM = "move_arm"\r\n    RELEASE = "release"\r\n    ROTATE = "rotate"\r\n    LOOK_AT = "look_at"\r\n    WAIT = "wait"\r\n\r\n\r\n@dataclass\r\nclass Skill:\r\n    """A primitive robot skill with parameters."""\r\n    skill_type: SkillType\r\n    parameters: dict\r\n    duration_estimate: float  # seconds\r\n    preconditions: List[str]\r\n    effects: List[str]\r\n\r\n\r\n@dataclass\r\nclass ActionPlan:\r\n    """A sequence of skills to execute."""\r\n    skills: List[Skill]\r\n    total_duration: float\r\n    success_probability: float\r\n    fallback_plan: Optional[\'ActionPlan\'] = None\r\n\r\n\r\nclass ActionPlanner:\r\n    """\r\n    Plans sequences of robot actions to accomplish goals.\r\n\r\n    Uses a skill-based approach where complex tasks are decomposed\r\n    into primitive skills that the robot can execute.\r\n    """\r\n\r\n    def __init__(self, robot_capabilities: dict = None):\r\n        """\r\n        Initialize the action planner.\r\n\r\n        Args:\r\n            robot_capabilities: Dict describing robot\'s capabilities\r\n        """\r\n        self.capabilities = robot_capabilities or self._default_capabilities()\r\n        self.world_state = {}  # Tracked world state\r\n\r\n    def _default_capabilities(self) -> dict:\r\n        """Default capabilities for a mobile manipulator."""\r\n        return {\r\n            "has_gripper": True,\r\n            "has_mobile_base": True,\r\n            "max_reach": 0.8,  # meters\r\n            "gripper_width": 0.1,  # meters\r\n            "navigation_speed": 0.5,  # m/s\r\n            "arm_speed": 0.3,  # m/s\r\n        }\r\n\r\n    def plan_pick(\r\n        self,\r\n        object_name: str,\r\n        object_position: Tuple[float, float, float]\r\n    ) -> ActionPlan:\r\n        """\r\n        Plan a pick/grasp action.\r\n\r\n        Args:\r\n            object_name: Name of object to pick\r\n            object_position: (x, y, z) position of object\r\n\r\n        Returns:\r\n            ActionPlan for picking the object\r\n        """\r\n        skills = []\r\n\r\n        # 1. Look at the object\r\n        skills.append(Skill(\r\n            skill_type=SkillType.LOOK_AT,\r\n            parameters={"target": object_position},\r\n            duration_estimate=0.5,\r\n            preconditions=[],\r\n            effects=["object_visible"]\r\n        ))\r\n\r\n        # 2. Approach if needed (check if object is within reach)\r\n        distance = np.linalg.norm(np.array(object_position[:2]))\r\n        if distance > self.capabilities["max_reach"]:\r\n            approach_pos = self._calculate_approach_position(object_position)\r\n            skills.append(Skill(\r\n                skill_type=SkillType.NAVIGATE,\r\n                parameters={"target": approach_pos},\r\n                duration_estimate=distance / self.capabilities["navigation_speed"],\r\n                preconditions=[],\r\n                effects=["at_approach_position"]\r\n            ))\r\n\r\n        # 3. Move arm to pre-grasp pose\r\n        pre_grasp = (object_position[0], object_position[1], object_position[2] + 0.1)\r\n        skills.append(Skill(\r\n            skill_type=SkillType.MOVE_ARM,\r\n            parameters={"target": pre_grasp, "gripper_open": True},\r\n            duration_estimate=1.5,\r\n            preconditions=["at_approach_position"],\r\n            effects=["arm_at_pregrasp"]\r\n        ))\r\n\r\n        # 4. Move to grasp pose\r\n        skills.append(Skill(\r\n            skill_type=SkillType.MOVE_ARM,\r\n            parameters={"target": object_position, "gripper_open": True},\r\n            duration_estimate=0.5,\r\n            preconditions=["arm_at_pregrasp"],\r\n            effects=["arm_at_grasp"]\r\n        ))\r\n\r\n        # 5. Close gripper\r\n        skills.append(Skill(\r\n            skill_type=SkillType.GRASP,\r\n            parameters={"force": 20.0},  # Newtons\r\n            duration_estimate=0.5,\r\n            preconditions=["arm_at_grasp"],\r\n            effects=["object_grasped"]\r\n        ))\r\n\r\n        # 6. Lift object\r\n        lift_pos = (object_position[0], object_position[1], object_position[2] + 0.15)\r\n        skills.append(Skill(\r\n            skill_type=SkillType.LIFT,\r\n            parameters={"target": lift_pos},\r\n            duration_estimate=0.5,\r\n            preconditions=["object_grasped"],\r\n            effects=["object_lifted"]\r\n        ))\r\n\r\n        total_duration = sum(s.duration_estimate for s in skills)\r\n\r\n        return ActionPlan(\r\n            skills=skills,\r\n            total_duration=total_duration,\r\n            success_probability=0.85\r\n        )\r\n\r\n    def plan_place(\r\n        self,\r\n        target_position: Tuple[float, float, float]\r\n    ) -> ActionPlan:\r\n        """Plan a place action (assumes object is already grasped)."""\r\n        skills = []\r\n\r\n        # 1. Navigate if needed\r\n        distance = np.linalg.norm(np.array(target_position[:2]))\r\n        if distance > self.capabilities["max_reach"]:\r\n            approach_pos = self._calculate_approach_position(target_position)\r\n            skills.append(Skill(\r\n                skill_type=SkillType.NAVIGATE,\r\n                parameters={"target": approach_pos},\r\n                duration_estimate=distance / self.capabilities["navigation_speed"],\r\n                preconditions=["object_grasped"],\r\n                effects=["at_place_position"]\r\n            ))\r\n\r\n        # 2. Move arm to pre-place pose\r\n        pre_place = (target_position[0], target_position[1], target_position[2] + 0.1)\r\n        skills.append(Skill(\r\n            skill_type=SkillType.MOVE_ARM,\r\n            parameters={"target": pre_place},\r\n            duration_estimate=1.5,\r\n            preconditions=["object_grasped"],\r\n            effects=["arm_at_preplace"]\r\n        ))\r\n\r\n        # 3. Move to place pose\r\n        skills.append(Skill(\r\n            skill_type=SkillType.MOVE_ARM,\r\n            parameters={"target": target_position},\r\n            duration_estimate=0.5,\r\n            preconditions=["arm_at_preplace"],\r\n            effects=["arm_at_place"]\r\n        ))\r\n\r\n        # 4. Release object\r\n        skills.append(Skill(\r\n            skill_type=SkillType.RELEASE,\r\n            parameters={},\r\n            duration_estimate=0.3,\r\n            preconditions=["arm_at_place"],\r\n            effects=["object_placed"]\r\n        ))\r\n\r\n        # 5. Retract arm\r\n        retract_pos = (target_position[0], target_position[1], target_position[2] + 0.15)\r\n        skills.append(Skill(\r\n            skill_type=SkillType.MOVE_ARM,\r\n            parameters={"target": retract_pos},\r\n            duration_estimate=0.5,\r\n            preconditions=["object_placed"],\r\n            effects=["arm_retracted"]\r\n        ))\r\n\r\n        total_duration = sum(s.duration_estimate for s in skills)\r\n\r\n        return ActionPlan(\r\n            skills=skills,\r\n            total_duration=total_duration,\r\n            success_probability=0.90\r\n        )\r\n\r\n    def plan_navigate(\r\n        self,\r\n        target_location: str,\r\n        location_positions: dict\r\n    ) -> ActionPlan:\r\n        """Plan navigation to a named location."""\r\n        if target_location not in location_positions:\r\n            # Return a search plan\r\n            return self._plan_search(target_location)\r\n\r\n        target_pos = location_positions[target_location]\r\n\r\n        skills = [\r\n            Skill(\r\n                skill_type=SkillType.NAVIGATE,\r\n                parameters={"target": target_pos, "avoid_obstacles": True},\r\n                duration_estimate=5.0,  # Estimate\r\n                preconditions=[],\r\n                effects=[f"at_{target_location}"]\r\n            )\r\n        ]\r\n\r\n        return ActionPlan(\r\n            skills=skills,\r\n            total_duration=5.0,\r\n            success_probability=0.95\r\n        )\r\n\r\n    def _calculate_approach_position(\r\n        self,\r\n        target: Tuple[float, float, float]\r\n    ) -> Tuple[float, float, float]:\r\n        """Calculate position to approach an object from."""\r\n        # Position robot 0.5m from target\r\n        approach_distance = 0.5\r\n        direction = np.array(target[:2]) / (np.linalg.norm(target[:2]) + 1e-6)\r\n        approach_pos = np.array(target[:2]) - direction * approach_distance\r\n        return (approach_pos[0], approach_pos[1], 0.0)\r\n\r\n    def _plan_search(self, target: str) -> ActionPlan:\r\n        """Plan a search behavior for an unknown location."""\r\n        skills = [\r\n            Skill(\r\n                skill_type=SkillType.ROTATE,\r\n                parameters={"angle": 360, "search_for": target},\r\n                duration_estimate=10.0,\r\n                preconditions=[],\r\n                effects=["searched_area"]\r\n            )\r\n        ]\r\n        return ActionPlan(\r\n            skills=skills,\r\n            total_duration=10.0,\r\n            success_probability=0.7\r\n        )\r\n\r\n\r\ndef demo_action_planner():\r\n    """Demonstrate the action planner."""\r\n    planner = ActionPlanner()\r\n\r\n    print("Action Planner Demo")\r\n    print("=" * 60)\r\n\r\n    # Plan a pick action\r\n    object_pos = (0.5, 0.3, 0.1)\r\n    pick_plan = planner.plan_pick("cup", object_pos)\r\n\r\n    print("\\nPick Plan for \'cup\' at (0.5, 0.3, 0.1):")\r\n    print(f"  Total duration: {pick_plan.total_duration:.1f}s")\r\n    print(f"  Success probability: {pick_plan.success_probability:.0%}")\r\n    print(f"  Skills ({len(pick_plan.skills)}):")\r\n    for i, skill in enumerate(pick_plan.skills):\r\n        print(f"    {i+1}. {skill.skill_type.value}: {skill.parameters}")\r\n\r\n\r\nif __name__ == "__main__":\r\n    demo_action_planner()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"integrating-vla-with-ros-2",children:"Integrating VLA with ROS 2"}),"\n",(0,i.jsx)(e.p,{children:"The complete VLA system runs as interconnected ROS 2 nodes:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nComplete VLA ROS 2 Integration Node\r\n\r\nThis node integrates vision, language, and action modules\r\nfor end-to-end voice-controlled robot operation.\r\n\r\nTopics:\r\n    Subscribed:\r\n        /camera/rgb/image_raw (sensor_msgs/Image)\r\n        /speech/text (std_msgs/String)\r\n\r\n    Published:\r\n        /vla/command (std_msgs/String)\r\n        /vla/status (std_msgs/String)\r\n        /cmd_vel (geometry_msgs/Twist)\r\n        /arm/target_pose (geometry_msgs/Pose)\r\n\r\nPrerequisites:\r\n    pip install ultralytics opencv-python\r\n    ros2 run your_package vla_node\r\n\r\nAuthor: Physical AI & Humanoid Robotics Book\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist, Pose, Point, Quaternion\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nfrom typing import Optional\r\n\r\n# Import our VLA modules (from previous code)\r\n# from .vision_detector import VisionDetector, Detection\r\n# from .language_parser import LanguageParser, ParsedCommand, RobotAction\r\n# from .action_planner import ActionPlanner, ActionPlan\r\n\r\n\r\nclass VLANode(Node):\r\n    """\r\n    Vision-Language-Action ROS 2 node.\r\n\r\n    Processes camera images and voice commands to control\r\n    a robot using the VLA pipeline.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__("vla_node")\r\n\r\n        # Initialize CV bridge for image conversion\r\n        self.bridge = CvBridge()\r\n\r\n        # Initialize VLA modules\r\n        # self.vision = VisionDetector()\r\n        # self.language = LanguageParser()\r\n        # self.planner = ActionPlanner()\r\n\r\n        # State\r\n        self.latest_image: Optional[np.ndarray] = None\r\n        self.latest_detections = []\r\n        self.current_plan: Optional[any] = None\r\n        self.executing = False\r\n\r\n        # Known locations (would be learned or configured)\r\n        self.locations = {\r\n            "table": (1.0, 0.0, 0.0),\r\n            "shelf": (0.0, 1.0, 0.5),\r\n            "kitchen": (2.0, 2.0, 0.0),\r\n        }\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            "/camera/rgb/image_raw",\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.speech_sub = self.create_subscription(\r\n            String,\r\n            "/speech/text",\r\n            self.speech_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.status_pub = self.create_publisher(String, "/vla/status", 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, "/cmd_vel", 10)\r\n        self.arm_pose_pub = self.create_publisher(Pose, "/arm/target_pose", 10)\r\n\r\n        # Timer for vision processing (10 Hz)\r\n        self.vision_timer = self.create_timer(0.1, self.process_vision)\r\n\r\n        self.get_logger().info("VLA Node initialized")\r\n        self.publish_status("Ready for commands")\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Handle incoming camera images."""\r\n        try:\r\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n        except Exception as e:\r\n            self.get_logger().error(f"Image conversion error: {e}")\r\n\r\n    def process_vision(self):\r\n        """Process latest image for object detection."""\r\n        if self.latest_image is None:\r\n            return\r\n\r\n        # Run object detection\r\n        # self.latest_detections = self.vision.detect(self.latest_image)\r\n        pass\r\n\r\n    def speech_callback(self, msg: String):\r\n        """Handle voice commands."""\r\n        command_text = msg.data\r\n        self.get_logger().info(f"Received command: {command_text}")\r\n\r\n        # Parse the command\r\n        # parsed = self.language.parse(command_text)\r\n        # self.handle_parsed_command(parsed)\r\n\r\n        # Simplified demo handling\r\n        self.handle_command_demo(command_text)\r\n\r\n    def handle_command_demo(self, command: str):\r\n        """Simplified command handling for demonstration."""\r\n        command_lower = command.lower()\r\n\r\n        if "stop" in command_lower:\r\n            self.stop_robot()\r\n            self.publish_status("Stopped")\r\n            return\r\n\r\n        if "forward" in command_lower or "move" in command_lower:\r\n            twist = Twist()\r\n            twist.linear.x = 0.5\r\n            self.cmd_vel_pub.publish(twist)\r\n            self.publish_status("Moving forward")\r\n            return\r\n\r\n        if "turn" in command_lower or "rotate" in command_lower:\r\n            twist = Twist()\r\n            twist.angular.z = 0.5 if "left" in command_lower else -0.5\r\n            self.cmd_vel_pub.publish(twist)\r\n            self.publish_status("Turning")\r\n            return\r\n\r\n        if "pick" in command_lower or "grab" in command_lower:\r\n            self.publish_status("Planning pick action...")\r\n            # Would trigger pick action plan\r\n            return\r\n\r\n        self.publish_status(f"Unknown command: {command}")\r\n\r\n    def stop_robot(self):\r\n        """Stop all robot motion."""\r\n        twist = Twist()\r\n        self.cmd_vel_pub.publish(twist)\r\n        self.executing = False\r\n\r\n    def publish_status(self, status: str):\r\n        """Publish VLA system status."""\r\n        msg = String()\r\n        msg.data = status\r\n        self.status_pub.publish(msg)\r\n        self.get_logger().info(f"Status: {status}")\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VLANode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"vla-system-architecture-diagram",children:"VLA System Architecture Diagram"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                        VLA System Architecture                            \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                              \u2502   User Voice    \u2502\r\n                              \u2502    Command      \u2502\r\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                       \u2502\r\n                                       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Camera       \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   VLA Node      \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Robot        \u2502\r\n\u2502   /camera/rgb  \u2502           \u2502                 \u2502           \u2502   /cmd_vel     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502           \u2502   /arm/pose    \u2502\r\n                             \u2502  \u2502  Vision   \u2502  \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                             \u2502  \u2502  Detector \u2502  \u2502\r\n                             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n                             \u2502        \u2502        \u2502\r\n                             \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502  \u2502 Language  \u2502  \u2502\r\n\u2502   Microphone   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  \u2502  Parser   \u2502  \u2502\r\n\u2502   /speech/text \u2502           \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502        \u2502        \u2502\r\n                             \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n                             \u2502  \u2502  Action   \u2502  \u2502\r\n                             \u2502  \u2502  Planner  \u2502  \u2502\r\n                             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n                             \u2502                 \u2502\r\n                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                       \u2502\r\n                                       \u25bc\r\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                              \u2502   ROS 2 Topics  \u2502\r\n                              \u2502   & Services    \u2502\r\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,i.jsxs)(e.p,{children:["\u2705 ",(0,i.jsx)(e.strong,{children:"VLA architecture"}),": Vision, Language, and Action modules work together for intelligent robot control"]}),"\n",(0,i.jsxs)(e.p,{children:["\u2705 ",(0,i.jsx)(e.strong,{children:"Vision processing"}),": Object detection with YOLO provides scene understanding for robots"]}),"\n",(0,i.jsxs)(e.p,{children:["\u2705 ",(0,i.jsx)(e.strong,{children:"Language parsing"}),": Extract actions, objects, and locations from natural language commands"]}),"\n",(0,i.jsxs)(e.p,{children:["\u2705 ",(0,i.jsx)(e.strong,{children:"Action planning"}),": Decompose high-level commands into executable robot skills"]}),"\n",(0,i.jsxs)(e.p,{children:["\u2705 ",(0,i.jsx)(e.strong,{children:"ROS 2 integration"}),": Connect VLA components through topics and services"]}),"\n",(0,i.jsxs)(e.p,{children:["\u2705 ",(0,i.jsx)(e.strong,{children:"End-to-end pipeline"}),": Camera input \u2192 Command parsing \u2192 Action execution"]}),"\n",(0,i.jsx)(e.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["VLA systems enable ",(0,i.jsx)(e.strong,{children:"natural human-robot interaction"})," through language"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Object detection"})," provides spatial grounding for language references"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Intent classification"})," maps natural language to robot actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Skill-based planning"})," breaks complex tasks into primitive actions"]}),"\n",(0,i.jsxs)(e.li,{children:["ROS 2 provides the ",(0,i.jsx)(e.strong,{children:"communication infrastructure"})," for VLA components"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(e.h3,{id:"exercise-1-extend-object-detection",children:"Exercise 1: Extend Object Detection"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["Modify ",(0,i.jsx)(e.code,{children:"VisionDetector"})," to track objects across frames"]}),"\n",(0,i.jsx)(e.li,{children:"Add distance estimation using object size"}),"\n",(0,i.jsx)(e.li,{children:'Implement a "find all red objects" capability'}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"exercise-2-improve-language-understanding",children:"Exercise 2: Improve Language Understanding"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:'Add support for compound commands: "Pick up the cup and place it on the table"'}),"\n",(0,i.jsx)(e.li,{children:'Handle pronouns: "Pick it up" (referring to previously mentioned object)'}),"\n",(0,i.jsx)(e.li,{children:"Add confirmation for ambiguous commands"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"exercise-3-build-a-pick-and-place-demo",children:"Exercise 3: Build a Pick-and-Place Demo"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Create a complete pick-and-place pipeline"}),"\n",(0,i.jsx)(e.li,{children:"Use simulated vision (hardcoded detections) if camera unavailable"}),"\n",(0,i.jsx)(e.li,{children:'Test with commands: "Pick up the bottle", "Place it on the shelf"'}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"challenge-add-error-recovery",children:"Challenge: Add Error Recovery"}),"\n",(0,i.jsx)(e.p,{children:"Implement a robust VLA system that:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Detects when grasp fails (object not acquired)"}),"\n",(0,i.jsx)(e.li,{children:"Re-plans and retries with different approach"}),"\n",(0,i.jsx)(e.li,{children:"Reports failure to user after max retries"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"up-next",children:"Up Next"}),"\n",(0,i.jsxs)(e.p,{children:["In ",(0,i.jsx)(e.strong,{children:"Chapter 9: LLM Planning and Voice Commands"}),", we'll enhance our VLA system with:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Large Language Model (LLM) integration for complex reasoning"}),"\n",(0,i.jsx)(e.li,{children:"Voice command processing with speech recognition"}),"\n",(0,i.jsx)(e.li,{children:"Multi-step task planning using LLM capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Handling ambiguous and context-dependent commands"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://openai.com/research/clip",children:"OpenAI CLIP"})," - Vision-Language Foundation Models"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://robotics-transformer2.github.io/",children:"RT-2: Vision-Language-Action Models"})," - Google DeepMind"]}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://docs.ultralytics.com/",children:"YOLOv8 Documentation"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://spacy.io/",children:"spaCy NLP Library"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://navigation.ros.org/",children:"ROS 2 Navigation Stack"})}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Sources:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://www.physicalintelligence.company/",children:"Physical Intelligence Pi0"})," - VLA Research"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2307.15818",children:"Google RT-2 Paper"})," - Vision-Language-Action Models"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"https://openai.com/research",children:"OpenAI Research"})," - Multimodal Learning"]}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Documentation"})}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var t=r(6540);const i={},o=t.createContext(i);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);