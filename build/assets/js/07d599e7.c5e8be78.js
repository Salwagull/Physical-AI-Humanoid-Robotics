"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[719],{7897:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapter2_embodied_intelligence","title":"Chapter 2: Embodied Intelligence and Robot Interaction","description":"Introduction","source":"@site/docs/chapter2_embodied_intelligence.md","sourceDirName":".","slug":"/chapter2_embodied_intelligence","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter2_embodied_intelligence","draft":false,"unlisted":false,"editUrl":"https://github.com/Salwagull/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter2_embodied_intelligence.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to Physical AI","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter1_intro_physical_ai"},"next":{"title":"Chapter 3: ROS 2 Basics - Nodes, Topics, Services","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter3_ros2_basics"}}');var s=r(4848),o=r(8453);const t={sidebar_position:3},l="Chapter 2: Embodied Intelligence and Robot Interaction",a={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Conceptual Overview",id:"conceptual-overview",level:2},{value:"The Embodied Intelligence Paradigm",id:"the-embodied-intelligence-paradigm",level:3},{value:"Sensing and Perception in Physical Environments",id:"sensing-and-perception-in-physical-environments",level:3},{value:"Sensor Fusion: Building a Coherent World Model",id:"sensor-fusion-building-a-coherent-world-model",level:3},{value:"Action and Control Mechanisms",id:"action-and-control-mechanisms",level:3},{value:"Spatial Reasoning and Environmental Awareness",id:"spatial-reasoning-and-environmental-awareness",level:3},{value:"Feedback Loops in Robotic Systems",id:"feedback-loops-in-robotic-systems",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"Sensor Integration in ROS 2",id:"sensor-integration-in-ros-2",level:3},{value:"Implementing a Simple Feedback Controller",id:"implementing-a-simple-feedback-controller",level:3},{value:"Visual Aids",id:"visual-aids",level:2},{value:"Embodied Intelligence: The Perception-Action Loop",id:"embodied-intelligence-the-perception-action-loop",level:3},{value:"Sensor Modalities Comparison",id:"sensor-modalities-comparison",level:3},{value:"Summary and Next Steps",id:"summary-and-next-steps",level:2},{value:"Exercises and Challenges",id:"exercises-and-challenges",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-2-embodied-intelligence-and-robot-interaction",children:"Chapter 2: Embodied Intelligence and Robot Interaction"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Embodied intelligence is the cornerstone of Physical AI. It's the idea that intelligence doesn't exist in isolation\u2014it emerges from the interaction between a physical body, its sensors, and the environment. In this chapter, we explore how robots perceive, interact with, and learn from the physical world through their embodiment."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Learning Objectives:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the principles of embodied intelligence"}),"\n",(0,s.jsx)(n.li,{children:"Learn about different sensor modalities and their roles in perception"}),"\n",(0,s.jsx)(n.li,{children:"Explore feedback control and how robots maintain desired behaviors"}),"\n",(0,s.jsx)(n.li,{children:"Recognize the importance of action-perception loops in robotics"}),"\n",(0,s.jsx)(n.li,{children:"Grasp spatial reasoning and environmental awareness"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites:"})," Chapter 1 (Introduction to Physical AI), basic understanding of control systems"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why This Matters:"})," Embodied intelligence is what makes robots more than just computers on wheels. It's the foundation for adaptive behavior, learning, and effective interaction with complex, unpredictable environments."]}),"\n",(0,s.jsx)(n.h2,{id:"conceptual-overview",children:"Conceptual Overview"}),"\n",(0,s.jsx)(n.h3,{id:"the-embodied-intelligence-paradigm",children:"The Embodied Intelligence Paradigm"}),"\n",(0,s.jsxs)(n.p,{children:["Traditional AI views intelligence as abstract reasoning\u2014solving puzzles, playing games, answering questions. Embodied intelligence takes a different stance: ",(0,s.jsx)(n.strong,{children:"intelligence is fundamentally about effective action in the world"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Consider a simple example: catching a ball. You don't compute trajectories in your head using physics equations. Instead, your eyes track the ball, your body adjusts continuously, and your hand reaches out\u2014all in a seamless sensorimotor loop. This is embodied intelligence."}),"\n",(0,s.jsx)(n.p,{children:"For robots, embodiment means:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The body shapes perception"}),": A wheeled robot experiences the world differently than a humanoid"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action and perception are coupled"}),": What you see influences what you do, and what you do influences what you see"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intelligence emerges from interaction"}),": Smart behavior arises from the dynamic interplay between body, brain, and environment"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"sensing-and-perception-in-physical-environments",children:"Sensing and Perception in Physical Environments"}),"\n",(0,s.jsx)(n.p,{children:"Robots perceive the world through sensors. Unlike humans with integrated multisensory systems, robots use discrete sensor modules that must be carefully integrated."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Common Robot Sensor Modalities:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Vision (Cameras)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RGB Cameras"}),": Capture color images, good for object recognition and scene understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Cameras"}),": Measure distance to objects, useful for 3D reconstruction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo Cameras"}),": Two cameras provide depth through triangulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Event Cameras"}),": Detect changes in brightness, excel in dynamic scenes"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use Cases"}),": Object detection, localization, visual servoing, human-robot interaction"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Range Sensors (Lidar, Sonar, Radar)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lidar"}),": Laser-based distance measurement, provides accurate 3D point clouds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sonar"}),": Ultrasonic distance measurement, cheap but less accurate"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Radar"}),": Radio waves for long-range detection, works in adverse weather"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use Cases"}),": Obstacle avoidance, mapping, localization, navigation"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Inertial Sensors (IMU)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accelerometers"}),": Measure linear acceleration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gyroscopes"}),": Measure angular velocity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Magnetometers"}),": Measure orientation relative to Earth's magnetic field"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use Cases"}),": Orientation tracking, motion estimation, stabilization"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"4. Proprioceptive Sensors (Internal State)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Encoders"}),": Measure joint angles and motor positions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Force/Torque Sensors"}),": Measure forces applied to the robot"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Current Sensors"}),": Detect motor load and contact"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use Cases"}),": Precise control, contact detection, manipulation"]}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion-building-a-coherent-world-model",children:"Sensor Fusion: Building a Coherent World Model"}),"\n",(0,s.jsx)(n.p,{children:"No single sensor tells the complete story. Sensor fusion combines data from multiple sensors to build a more accurate, robust understanding of the world."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why Sensor Fusion?"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Redundancy"}),": If one sensor fails, others provide backup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complementary Information"}),": Cameras see color, lidar sees geometry"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise Reduction"}),": Combining multiple noisy measurements improves accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disambiguation"}),": Different sensors resolve different kinds of uncertainty"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: Robot Localization"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU"})," provides high-frequency orientation updates but drifts over time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPS"})," provides absolute position but is noisy and has low update rates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lidar"})," provides relative position through scan matching but accumulates error"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion (Kalman Filter)"}),": Combines all three for accurate, drift-free localization"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"action-and-control-mechanisms",children:"Action and Control Mechanisms"}),"\n",(0,s.jsx)(n.p,{children:"Perception is only half the story. Robots must act\u2014move, manipulate, and change the world. This requires control systems that translate high-level intentions into motor commands."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Control Hierarchy:"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. High-Level Planning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Task planning: "Navigate to the kitchen, then grasp the cup"'}),"\n",(0,s.jsx)(n.li,{children:'Path planning: "Find a collision-free path from here to there"'}),"\n",(0,s.jsx)(n.li,{children:'Grasp planning: "Determine how to approach and grasp this object"'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Mid-Level Control"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Trajectory generation: "Smooth out the planned path into a time-parameterized trajectory"'}),"\n",(0,s.jsx)(n.li,{children:'Motion primitives: "Execute a learned behavior for specific situations"'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Low-Level Control"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"PID controllers: Maintain desired motor velocities or joint positions"}),"\n",(0,s.jsx)(n.li,{children:"Force control: Apply precise forces for manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Balance control: Keep a legged robot upright"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Feedback Control: The Key to Robustness"})}),"\n",(0,s.jsx)(n.p,{children:"Open-loop control executes a pre-planned sequence of actions without sensing. It's simple but fragile\u2014any disturbance causes failure."}),"\n",(0,s.jsx)(n.p,{children:"Closed-loop (feedback) control continuously measures the system state and adjusts actions accordingly. This makes robots robust to disturbances and modeling errors."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"PID Control Example:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"error = desired_state - current_state\r\ncontrol_signal = Kp * error + Ki * integral(error) + Kd * derivative(error)\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Proportional (P)"}),": React to current error"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integral (I)"}),": Correct accumulated past errors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Derivative (D)"}),": Anticipate future error based on rate of change"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"spatial-reasoning-and-environmental-awareness",children:"Spatial Reasoning and Environmental Awareness"}),"\n",(0,s.jsx)(n.p,{children:"Robots must reason about space. Where am I? Where are obstacles? How do I get from A to B? Spatial reasoning is fundamental to navigation and manipulation."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Coordinate Frames and Transforms"})}),"\n",(0,s.jsx)(n.p,{children:"Robots work with multiple coordinate frames:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"World Frame"}),": Fixed reference in the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robot Frame"}),": Centered on the robot's body"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Frames"}),": Centered on each sensor"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Frames"}),": Centered on objects of interest"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Transformations (translations and rotations) convert positions and orientations between frames. ROS 2's ",(0,s.jsx)(n.code,{children:"tf2"})," library manages these transforms automatically."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Occupancy Grids and Maps"})}),"\n",(0,s.jsxs)(n.p,{children:["Robots build maps to navigate. An ",(0,s.jsx)(n.strong,{children:"occupancy grid"})," divides space into cells, marking each as free, occupied, or unknown. This representation supports efficient path planning."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Grid Cell States:\r\n0 = Free (robot can go here)\r\n1 = Occupied (obstacle)\r\n-1 = Unknown (not yet explored)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"feedback-loops-in-robotic-systems",children:"Feedback Loops in Robotic Systems"}),"\n",(0,s.jsxs)(n.p,{children:["Embodied intelligence relies on tight feedback loops between perception and action. Let's examine the classic ",(0,s.jsx)(n.strong,{children:"action-perception loop"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sense"}),": Gather data from sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perceive"}),": Process sensor data to understand the current state"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan"}),": Decide what action to take"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Act"}),": Execute motor commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observe"}),": See the effects of actions on the world"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update"}),": Refine internal models based on observed outcomes"]}),"\n",(0,s.jsx)(n.li,{children:"Repeat"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This loop runs continuously, at rates from 10 Hz (slow deliberative planning) to 1000 Hz (fast motor control)."}),"\n",(0,s.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-integration-in-ros-2",children:"Sensor Integration in ROS 2"}),"\n",(0,s.jsx)(n.p,{children:"In ROS 2, sensors publish data on topics. Nodes subscribe to these topics to access sensor data."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: Reading Lidar Data in ROS 2"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\n\r\nclass LidarListener(Node):\r\n    def __init__(self):\r\n        super().__init__('lidar_listener')\r\n        # Subscribe to the lidar scan topic\r\n        self.subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',  # Topic name\r\n            self.lidar_callback,\r\n            10  # QoS queue size\r\n        )\r\n\r\n    def lidar_callback(self, msg):\r\n        \"\"\"\r\n        Callback function executed when lidar data arrives\r\n        \"\"\"\r\n        # Extract useful information from the scan\r\n        min_distance = min(msg.ranges)\r\n        max_distance = max(msg.ranges)\r\n        num_readings = len(msg.ranges)\r\n\r\n        self.get_logger().info(\r\n            f'Lidar: {num_readings} readings, '\r\n            f'min distance: {min_distance:.2f}m, '\r\n            f'max distance: {max_distance:.2f}m'\r\n        )\r\n\r\n        # Detect obstacles: if any reading < 0.5m, obstacle is near\r\n        if min_distance < 0.5:\r\n            self.get_logger().warn('Obstacle detected nearby!')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    lidar_listener = LidarListener()\r\n    rclpy.spin(lidar_listener)\r\n    lidar_listener.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[INFO] [lidar_listener]: Lidar: 360 readings, min distance: 0.82m, max distance: 10.00m\r\n[INFO] [lidar_listener]: Lidar: 360 readings, min distance: 0.45m, max distance: 10.00m\r\n[WARN] [lidar_listener]: Obstacle detected nearby!\n"})}),"\n",(0,s.jsx)(n.h3,{id:"implementing-a-simple-feedback-controller",children:"Implementing a Simple Feedback Controller"}),"\n",(0,s.jsx)(n.p,{children:"Let's implement a basic proportional controller for a mobile robot to maintain a desired distance from a wall:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom geometry_msgs.msg import Twist\r\n\r\nclass WallFollower(Node):\r\n    def __init__(self):\r\n        super().__init__('wall_follower')\r\n\r\n        # Desired distance to wall (meters)\r\n        self.desired_distance = 1.0\r\n\r\n        # Proportional gain\r\n        self.Kp = 0.5\r\n\r\n        # Subscribers and publishers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, '/scan', self.scan_callback, 10\r\n        )\r\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"\r\n        Use lidar data to maintain distance from right wall\r\n        \"\"\"\r\n        # Get distance to right wall (assume right is at index 270 for 360-deg lidar)\r\n        right_index = 270\r\n        current_distance = msg.ranges[right_index]\r\n\r\n        # Compute error\r\n        error = self.desired_distance - current_distance\r\n\r\n        # Proportional control: turn based on error\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.3  # Constant forward speed\r\n        cmd.angular.z = self.Kp * error  # Turn to maintain distance\r\n\r\n        # Publish command\r\n        self.cmd_pub.publish(cmd)\r\n\r\n        self.get_logger().info(\r\n            f'Distance: {current_distance:.2f}m, '\r\n            f'Error: {error:.2f}m, '\r\n            f'Turn rate: {cmd.angular.z:.2f} rad/s'\r\n        )\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    wall_follower = WallFollower()\r\n    rclpy.spin(wall_follower)\r\n    wall_follower.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Expected Behavior:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Robot moves forward at constant speed"}),"\n",(0,s.jsx)(n.li,{children:"If too close to the wall (< 1.0m), turns left (positive angular velocity)"}),"\n",(0,s.jsx)(n.li,{children:"If too far from the wall (> 1.0m), turns right (negative angular velocity)"}),"\n",(0,s.jsx)(n.li,{children:"Maintains approximately 1.0m distance through continuous feedback"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Common Issues:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Oscillation"}),": Kp too high causes the robot to overshoot. Reduce Kp or add derivative term (PD control)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Steady-State Error"}),": Robot doesn't quite reach desired distance. Add integral term (PI or PID control)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Noise"}),": Lidar readings fluctuate. Apply moving average filter"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"visual-aids",children:"Visual Aids"}),"\n",(0,s.jsx)(n.h3,{id:"embodied-intelligence-the-perception-action-loop",children:"Embodied Intelligence: The Perception-Action Loop"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    Environment                       \u2502\r\n\u2502  (Obstacles, Surfaces, Objects, Lighting, etc.)      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n               \u2502                        \u2502\r\n            Sensors                  Actuators\r\n               \u2502                        \u2502\r\n               \u2193                        \u2191\r\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n      \u2502   Perception   \u2502      \u2502   Action       \u2502\r\n      \u2502                \u2502      \u2502                \u2502\r\n      \u2502 - Filter noise \u2502      \u2502 - Motor cmds   \u2502\r\n      \u2502 - Estimate     \u2502      \u2502 - Trajectories \u2502\r\n      \u2502   state        \u2502      \u2502 - Force ctrl   \u2502\r\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n               \u2502                       \u2191\r\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500> Planning \u2500\u2500\u2500\u2500\u2500\u2518\r\n                        (Decision-making)\n"})}),"\n",(0,s.jsx)(n.p,{children:"The robot perceives the world, makes decisions, acts, and observes the results\u2014continuously."}),"\n",(0,s.jsx)(n.h3,{id:"sensor-modalities-comparison",children:"Sensor Modalities Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Sensor"}),(0,s.jsx)(n.th,{children:"Range"}),(0,s.jsx)(n.th,{children:"Resolution"}),(0,s.jsx)(n.th,{children:"Environment"}),(0,s.jsx)(n.th,{children:"Cost"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"RGB Camera"}),(0,s.jsx)(n.td,{children:"Visual"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Lighting-dependent"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Object recognition"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Depth Camera"}),(0,s.jsx)(n.td,{children:"0.5-10m"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Indoor"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"3D mapping"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Lidar"}),(0,s.jsx)(n.td,{children:"0.1-100m"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"All conditions"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Navigation, mapping"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Sonar"}),(0,s.jsx)(n.td,{children:"0.02-5m"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"All conditions"}),(0,s.jsx)(n.td,{children:"Very Low"}),(0,s.jsx)(n.td,{children:"Proximity sensing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"IMU"}),(0,s.jsx)(n.td,{children:"N/A"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"All conditions"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Orientation tracking"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"summary-and-next-steps",children:"Summary and Next Steps"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Takeaways:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Embodied intelligence emerges from the interaction between body, sensors, and environment"}),"\n",(0,s.jsx)(n.li,{children:"Robots use diverse sensor modalities (vision, lidar, IMU) to perceive the world"}),"\n",(0,s.jsx)(n.li,{children:"Sensor fusion combines multiple sensors for robust, accurate perception"}),"\n",(0,s.jsx)(n.li,{children:"Feedback control enables robots to adapt to disturbances and achieve desired behaviors"}),"\n",(0,s.jsx)(n.li,{children:"Spatial reasoning and coordinate transforms are fundamental to navigation and manipulation"}),"\n",(0,s.jsx)(n.li,{children:"Action-perception loops run continuously, enabling reactive and adaptive behavior"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What You've Learned:"}),"\r\nYou've explored how robots sense and interact with their environments through embodied intelligence. You understand sensor modalities, feedback control, and the action-perception loop that underpins all robotic behavior. You've also seen practical ROS 2 code for sensor processing and control."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Up Next:"}),"\r\nIn ",(0,s.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics/docs/chapter3_ros2_basics",children:"Chapter 3: ROS 2 Basics - Nodes, Topics, Services"}),", we'll dive deep into the Robot Operating System (ROS 2), the standard middleware for robotics. You'll learn to create nodes, publish and subscribe to topics, and build distributed robotic systems. ROS 2 is the foundation for all subsequent chapters, so this knowledge is critical."]}),"\n",(0,s.jsx)(n.h2,{id:"exercises-and-challenges",children:"Exercises and Challenges"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Exercise 1: Sensor Selection"}),"\r\nYou're designing a mobile robot for the following scenarios. For each, select appropriate sensors and justify your choices:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Indoor office navigation"}),"\n",(0,s.jsx)(n.li,{children:"Outdoor autonomous car"}),"\n",(0,s.jsx)(n.li,{children:"Underwater exploration robot"}),"\n",(0,s.jsx)(n.li,{children:"Warehouse picking robot"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Exercise 2: Feedback Control Analysis"}),"\r\nThe wall-following code uses only proportional (P) control. Research PID control and explain:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When would you add an integral (I) term?"}),"\n",(0,s.jsx)(n.li,{children:"When would you add a derivative (D) term?"}),"\n",(0,s.jsx)(n.li,{children:"What problems could arise from tuning Kp too high or too low?"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Challenge: Multi-Sensor Fusion"}),"\r\nDesign a simple sensor fusion algorithm that combines lidar and IMU data for robot localization. Sketch the algorithm and explain how each sensor contributes to the overall estimate."]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/humble/p/sensor_msgs/",children:"ROS 2 Sensor Messages"})," - Standard sensor data formats"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"http://ais.informatik.uni-freiburg.de/teaching/ss23/robotics/",children:"Introduction to Mobile Robotics"})," - Comprehensive robotics course"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.mathworks.com/campaigns/offers/pid-control-made-easy.html",children:"Control Systems Fundamentals"})," - PID control tutorial"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ready to continue?"})," Move on to ",(0,s.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics/docs/chapter3_ros2_basics",children:"Chapter 3: ROS 2 Basics - Nodes, Topics, Services"}),"!"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(6540);const s={},o=i.createContext(s);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);