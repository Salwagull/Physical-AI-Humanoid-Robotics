"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[376],{4987:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"chapter7_isaac_perception_nav","title":"Chapter 7: Isaac Sim for Perception and Navigation","description":"Learning Objectives","source":"@site/docs/chapter7_isaac_perception_nav.md","sourceDirName":".","slug":"/chapter7_isaac_perception_nav","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter7_isaac_perception_nav","draft":false,"unlisted":false,"editUrl":"https://github.com/Salwagull/Physical-AI-Humanoid-Robotics/tree/main/docs/chapter7_isaac_perception_nav.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Introduction to NVIDIA Isaac Sim & Machine Learning for Physical AI","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter6_intro_isaac_sim"},"next":{"title":"Chapter 8: Vision-Language-Action Systems","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter8_vla_systems"}}');var i=n(4848),s=n(8453);const t={},o="Chapter 7: Isaac Sim for Perception and Navigation",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"RTX Sensor Simulation",id:"rtx-sensor-simulation",level:2},{value:"Camera Sensors",id:"camera-sensors",level:3},{value:"Creating an RGB Camera",id:"creating-an-rgb-camera",level:4},{value:"Stereo Camera Setup",id:"stereo-camera-setup",level:4},{value:"Depth Cameras",id:"depth-cameras",level:3},{value:"LiDAR Sensors",id:"lidar-sensors",level:3},{value:"Rotating LiDAR (e.g., Velodyne VLP-16)",id:"rotating-lidar-eg-velodyne-vlp-16",level:4},{value:"Reading LiDAR Data",id:"reading-lidar-data",level:4},{value:"Publishing Sensors to ROS 2",id:"publishing-sensors-to-ros-2",level:3},{value:"RGB Camera to ROS 2",id:"rgb-camera-to-ros-2",level:4},{value:"LiDAR to ROS 2",id:"lidar-to-ros-2",level:4},{value:"Synthetic Data Generation with Replicator",id:"synthetic-data-generation-with-replicator",level:2},{value:"Domain Randomization Workflow",id:"domain-randomization-workflow",level:3},{value:"Step 1: Create Scene with Randomizable Elements",id:"step-1-create-scene-with-randomizable-elements",level:4},{value:"Step 2: Setup Camera and Render Product",id:"step-2-setup-camera-and-render-product",level:4},{value:"Step 3: Define Randomization",id:"step-3-define-randomization",level:4},{value:"Step 4: Setup Data Writer",id:"step-4-setup-data-writer",level:4},{value:"Step 5: Run Randomization Loop",id:"step-5-run-randomization-loop",level:4},{value:"Advanced Randomization: Textures and Materials",id:"advanced-randomization-textures-and-materials",level:3},{value:"Use Case: Training Object Detection",id:"use-case-training-object-detection",level:3},{value:"ROS 2 Navigation with Isaac Sim",id:"ros-2-navigation-with-isaac-sim",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Navigation Setup: Carter Robot",id:"navigation-setup-carter-robot",level:3},{value:"Step 1: Launch Isaac Sim with Carter",id:"step-1-launch-isaac-sim-with-carter",level:4},{value:"Step 2: Configure ROS 2 Bridge",id:"step-2-configure-ros-2-bridge",level:4},{value:"Step 3: Launch Nav2",id:"step-3-launch-nav2",level:4},{value:"Step 4: Set Navigation Goal",id:"step-4-set-navigation-goal",level:4},{value:"Creating a Custom Map",id:"creating-a-custom-map",level:3},{value:"Step 1: Generate Occupancy Grid",id:"step-1-generate-occupancy-grid",level:4},{value:"Step 2: Use Map in Nav2",id:"step-2-use-map-in-nav2",level:4},{value:"Isaac Perceptor Integration",id:"isaac-perceptor-integration",level:2},{value:"What is Isaac Perceptor?",id:"what-is-isaac-perceptor",level:3},{value:"Running Perceptor in Isaac Sim",id:"running-perceptor-in-isaac-sim",level:3},{value:"Prerequisites",id:"prerequisites-1",level:4},{value:"Step 1: Setup Nova Carter with Cameras",id:"step-1-setup-nova-carter-with-cameras",level:4},{value:"Step 2: Launch Perceptor Stack",id:"step-2-launch-perceptor-stack",level:4},{value:"Step 3: Visualize in RViz",id:"step-3-visualize-in-rviz",level:4},{value:"Step 4: Navigate with Vision",id:"step-4-navigate-with-vision",level:4},{value:"Perceptor Features",id:"perceptor-features",level:3},{value:"Complete Example: Warehouse Navigation",id:"complete-example-warehouse-navigation",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Multi-Sensor Fusion",id:"exercise-1-multi-sensor-fusion",level:3},{value:"Exercise 2: Dataset Generation",id:"exercise-2-dataset-generation",level:3},{value:"Exercise 3: Navigation Challenge",id:"exercise-3-navigation-challenge",level:3},{value:"Challenge: Vision-Based Navigation",id:"challenge-vision-based-navigation",level:3},{value:"Up Next",id:"up-next",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"chapter-7-isaac-sim-for-perception-and-navigation",children:"Chapter 7: Isaac Sim for Perception and Navigation"})}),"\n",(0,i.jsx)(r.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(r.p,{children:"By the end of this chapter, you will:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Configure and use Isaac Sim's RTX sensors (cameras, LiDAR, depth)"}),"\n",(0,i.jsx)(r.li,{children:"Generate synthetic training datasets with Omniverse Replicator"}),"\n",(0,i.jsx)(r.li,{children:"Integrate Isaac Sim with ROS 2 Nav2 for autonomous navigation"}),"\n",(0,i.jsx)(r.li,{children:"Implement Isaac Perceptor for vision-based SLAM"}),"\n",(0,i.jsx)(r.li,{children:"Build a complete perception and navigation pipeline"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"rtx-sensor-simulation",children:"RTX Sensor Simulation"}),"\n",(0,i.jsxs)(r.p,{children:["Isaac Sim's ",(0,i.jsx)(r.strong,{children:"RTX-accelerated sensors"})," provide physically accurate simulation using ray tracing."]}),"\n",(0,i.jsx)(r.h3,{id:"camera-sensors",children:"Camera Sensors"}),"\n",(0,i.jsx)(r.p,{children:"Isaac Sim supports multiple camera types with realistic optics."}),"\n",(0,i.jsx)(r.h4,{id:"creating-an-rgb-camera",children:"Creating an RGB Camera"}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Method 1: GUI"})}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.strong,{children:"Create \u2192 Isaac \u2192 Sensors \u2192 Camera"})}),"\n",(0,i.jsx)(r.li,{children:"Position camera at desired location"}),"\n",(0,i.jsxs)(r.li,{children:["Configure properties in ",(0,i.jsx)(r.strong,{children:"Property"})," panel:","\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Resolution: 1280 x 720"}),"\n",(0,i.jsx)(r.li,{children:"Focal Length: 24mm"}),"\n",(0,i.jsx)(r.li,{children:"F-Stop: 1.8 (depth of field)"}),"\n",(0,i.jsx)(r.li,{children:"Focus Distance: 2.0m"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Method 2: Python Script"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from omni.isaac.core import World\r\nfrom omni.isaac.sensor import Camera\r\nimport numpy as np\r\n\r\n# Create world\r\nworld = World()\r\nworld.scene.add_default_ground_plane()\r\n\r\n# Create RGB camera\r\ncamera = Camera(\r\n    prim_path="/World/Camera",\r\n    position=np.array([2.0, 2.0, 1.5]),\r\n    frequency=30,  # 30 Hz\r\n    resolution=(1280, 720),\r\n)\r\n\r\n# Add camera to scene\r\nworld.scene.add(camera)\r\nworld.reset()\r\n\r\n# Capture image\r\nfor _ in range(10):\r\n    world.step(render=True)\r\n\r\n# Get camera data\r\nrgba = camera.get_rgba()  # Shape: (720, 1280, 4)\r\nrgb = rgba[:, :, :3]\r\n\r\nprint(f"Captured image shape: {rgb.shape}")\n'})}),"\n",(0,i.jsx)(r.h4,{id:"stereo-camera-setup",children:"Stereo Camera Setup"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from omni.isaac.sensor import Camera\r\nimport numpy as np\r\n\r\n# Left camera\r\ncamera_left = Camera(\r\n    prim_path="/World/StereoRig/CameraLeft",\r\n    position=np.array([0.0, -0.06, 0.0]),  # 12cm baseline\r\n    frequency=20,\r\n    resolution=(1280, 720),\r\n)\r\n\r\n# Right camera\r\ncamera_right = Camera(\r\n    prim_path="/World/StereoRig/CameraRight",\r\n    position=np.array([0.0, 0.06, 0.0]),\r\n    frequency=20,\r\n    resolution=(1280, 720),\r\n)\r\n\r\n# Compute disparity (simplified)\r\nleft_img = camera_left.get_rgba()\r\nright_img = camera_right.get_rgba()\r\n\r\n# Use stereo matching algorithm\r\n# (In practice, use OpenCV or Isaac Sim\'s built-in depth camera)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,i.jsx)(r.p,{children:"Depth cameras provide direct 3D measurements."}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from omni.isaac.sensor import Camera\r\nfrom omni.isaac.core.utils.types import CameraTypes\r\n\r\n# Create depth camera\r\ndepth_camera = Camera(\r\n    prim_path="/World/DepthCamera",\r\n    position=np.array([2.0, 0.0, 1.0]),\r\n    frequency=30,\r\n    resolution=(640, 480),\r\n)\r\n\r\nworld.scene.add(depth_camera)\r\nworld.reset()\r\nworld.step(render=True)\r\n\r\n# Get depth data\r\ndepth = depth_camera.get_depth()  # Shape: (480, 640), values in meters\r\nprint(f"Min depth: {depth.min():.2f}m, Max depth: {depth.max():.2f}m")\r\n\r\n# Get point cloud\r\npoints = depth_camera.get_pointcloud()  # Shape: (N, 3)\r\nprint(f"Point cloud has {len(points)} points")\n'})}),"\n",(0,i.jsx)(r.h3,{id:"lidar-sensors",children:"LiDAR Sensors"}),"\n",(0,i.jsxs)(r.p,{children:["Isaac Sim provides ",(0,i.jsx)(r.strong,{children:"RTX-accelerated LiDAR"})," with realistic physics."]}),"\n",(0,i.jsx)(r.h4,{id:"rotating-lidar-eg-velodyne-vlp-16",children:"Rotating LiDAR (e.g., Velodyne VLP-16)"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import omni.isaac.core.utils.nucleus as nucleus\r\nfrom pxr import UsdGeom\r\n\r\n# Load LiDAR sensor from USD\r\nlidar_path = "/World/Lidar"\r\nnucleus_server = nucleus.get_assets_root_path()\r\nlidar_usd = f"{nucleus_server}/Isaac/Sensors/Lidar/Rotary/OS1_64ch20hz1024res.usd"\r\n\r\n# Add LiDAR to stage\r\nprim = stage.DefinePrim(lidar_path, "Xform")\r\nprim.GetReferences().AddReference(lidar_usd)\r\n\r\n# Or use Python API\r\nfrom omni.isaac.range_sensor import LidarRtx\r\n\r\nlidar = LidarRtx(\r\n    prim_path="/World/Lidar",\r\n    config={\r\n        "minRange": 0.4,          # 0.4m minimum range\r\n        "maxRange": 100.0,        # 100m maximum range\r\n        "drawPoints": False,      # Don\'t visualize points\r\n        "drawLines": False,       # Don\'t visualize rays\r\n        "horizontalFov": 360.0,   # Full 360\xb0 scan\r\n        "verticalFov": 30.0,      # \xb115\xb0 vertical\r\n        "horizontalResolution": 1024,  # 1024 points/rotation\r\n        "verticalResolution": 64,      # 64 laser channels\r\n        "rotationRate": 20.0,     # 20 Hz rotation\r\n    },\r\n)\n'})}),"\n",(0,i.jsx)(r.h4,{id:"reading-lidar-data",children:"Reading LiDAR Data"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Capture LiDAR scan\r\nworld.step(render=True)\r\n\r\n# Get point cloud\r\npoint_cloud = lidar.get_point_cloud()\r\n# Returns: numpy array of shape (N, 3) with XYZ coordinates\r\n\r\n# Optionally get structured data\r\nlinear_depth = lidar.get_linear_depth_data()\r\n# Returns: 2D array (vertical_res, horizontal_res)\r\n\r\nintensities = lidar.get_intensity_data()\r\n# Returns: Reflectance values\r\n\r\nprint(f"LiDAR captured {len(point_cloud)} points")\r\nprint(f"Point cloud range: [{point_cloud.min():.2f}, {point_cloud.max():.2f}]")\n'})}),"\n",(0,i.jsx)(r.h3,{id:"publishing-sensors-to-ros-2",children:"Publishing Sensors to ROS 2"}),"\n",(0,i.jsxs)(r.p,{children:["Use ",(0,i.jsx)(r.strong,{children:"Action Graphs"})," to publish sensor data to ROS 2."]}),"\n",(0,i.jsx)(r.h4,{id:"rgb-camera-to-ros-2",children:"RGB Camera to ROS 2"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import omni.graph.core as og\r\n\r\n# Create Action Graph for camera publishing\r\nkeys = og.Controller.Keys\r\ngraph_path = "/World/CameraGraph"\r\n\r\n(graph, nodes, _, _) = og.Controller.edit(\r\n    {"graph_path": graph_path, "evaluator_name": "execution"},\r\n    {\r\n        keys.CREATE_NODES: [\r\n            ("OnTick", "omni.graph.action.OnPlaybackTick"),\r\n            ("CameraHelper", "omni.isaac.ros2_bridge.ROS2CameraHelper"),\r\n        ],\r\n        keys.SET_VALUES: [\r\n            ("CameraHelper.inputs:topicName", "/camera/image_raw"),\r\n            ("CameraHelper.inputs:frameId", "camera_link"),\r\n            ("CameraHelper.inputs:type", "rgb"),\r\n            ("CameraHelper.inputs:cameraPrim", [usd.get_stage_next_free_path("/World/Camera", "")]),\r\n        ],\r\n        keys.CONNECT: [\r\n            ("OnTick.outputs:tick", "CameraHelper.inputs:execIn"),\r\n        ],\r\n    },\r\n)\n'})}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Verify in ROS 2:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"ros2 topic list | grep camera\r\n# /camera/image_raw\r\n# /camera/camera_info\r\n\r\nros2 topic hz /camera/image_raw\r\n# Should show ~30 Hz\r\n\r\n# View in RViz\r\nrviz2\r\n# Add \u2192 Image \u2192 Topic: /camera/image_raw\n"})}),"\n",(0,i.jsx)(r.h4,{id:"lidar-to-ros-2",children:"LiDAR to ROS 2"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'keys = og.Controller.Keys\r\n(graph, nodes, _, _) = og.Controller.edit(\r\n    {"graph_path": "/World/LidarGraph", "evaluator_name": "execution"},\r\n    {\r\n        keys.CREATE_NODES: [\r\n            ("OnTick", "omni.graph.action.OnPlaybackTick"),\r\n            ("LidarHelper", "omni.isaac.ros2_bridge.ROS2RtxLidarHelper"),\r\n        ],\r\n        keys.SET_VALUES: [\r\n            ("LidarHelper.inputs:topicName", "/scan"),\r\n            ("LidarHelper.inputs:frameId", "lidar_link"),\r\n            ("LidarHelper.inputs:lidarPrim", [usd.get_stage_next_free_path("/World/Lidar", "")]),\r\n        ],\r\n        keys.CONNECT: [\r\n            ("OnTick.outputs:tick", "LidarHelper.inputs:execIn"),\r\n        ],\r\n    },\r\n)\n'})}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Verify:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"ros2 topic echo /scan --once\r\n# Should see LaserScan or PointCloud2 message\r\n\r\n# Visualize in RViz\r\nrviz2\r\n# Add \u2192 LaserScan \u2192 Topic: /scan\r\n# Or Add \u2192 PointCloud2 \u2192 Topic: /lidar/points\n"})}),"\n",(0,i.jsx)(r.h2,{id:"synthetic-data-generation-with-replicator",children:"Synthetic Data Generation with Replicator"}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Omniverse Replicator"})," enables automated dataset creation for ML training."]}),"\n",(0,i.jsx)(r.h3,{id:"domain-randomization-workflow",children:"Domain Randomization Workflow"}),"\n",(0,i.jsx)(r.h4,{id:"step-1-create-scene-with-randomizable-elements",children:"Step 1: Create Scene with Randomizable Elements"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import omni.replicator.core as rep\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.objects import DynamicCuboid\r\n\r\n# Create world\r\nworld = World()\r\nworld.scene.add_default_ground_plane()\r\n\r\n# Create multiple objects to randomize\r\nfor i in range(10):\r\n    cube = DynamicCuboid(\r\n        prim_path=f"/World/Objects/Cube_{i}",\r\n        position=[i * 0.5 - 2.5, 0, 0.5],\r\n        size=0.3,\r\n        color=[1.0, 0.0, 0.0],\r\n    )\r\n    world.scene.add(cube)\r\n\r\nworld.reset()\n'})}),"\n",(0,i.jsx)(r.h4,{id:"step-2-setup-camera-and-render-product",children:"Step 2: Setup Camera and Render Product"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"# Create camera\r\ncamera = rep.create.camera(position=(5, 5, 3), look_at=(0, 0, 0))\r\n\r\n# Create render product (what to capture)\r\nrender_product = rep.create.render_product(camera, (1280, 720))\n"})}),"\n",(0,i.jsx)(r.h4,{id:"step-3-define-randomization",children:"Step 3: Define Randomization"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Get objects to randomize\r\nobjects = rep.get.prims(path_pattern="/World/Objects/Cube_*")\r\n\r\ndef randomize_scene():\r\n    with objects:\r\n        # Randomize positions\r\n        rep.modify.pose(\r\n            position=rep.distribution.uniform((-3, -3, 0.5), (3, 3, 2.0)),\r\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360)),\r\n        )\r\n\r\n        # Randomize scale\r\n        rep.modify.attribute(\r\n            "xformOp:scale",\r\n            rep.distribution.uniform((0.5, 0.5, 0.5), (2.0, 2.0, 2.0))\r\n        )\r\n\r\n    # Randomize lighting\r\n    lights = rep.get.light()\r\n    with lights:\r\n        rep.modify.attribute(\r\n            "intensity",\r\n            rep.distribution.uniform(1000, 10000)\r\n        )\r\n        rep.modify.attribute(\r\n            "color",\r\n            rep.distribution.uniform((0.8, 0.8, 0.8), (1.0, 1.0, 1.0))\r\n        )\r\n\r\n    return True  # Indicates randomization succeeded\n'})}),"\n",(0,i.jsx)(r.h4,{id:"step-4-setup-data-writer",children:"Step 4: Setup Data Writer"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Initialize writer for annotations\r\nwriter = rep.WriterRegistry.get("BasicWriter")\r\nwriter.initialize(\r\n    output_dir="~/isaac_sim_data/cubes",\r\n    rgb=True,                    # Save RGB images\r\n    bounding_box_2d_tight=True,  # Save 2D bounding boxes\r\n    semantic_segmentation=True,  # Save segmentation masks\r\n    distance_to_camera=True,     # Save depth maps\r\n)\r\n\r\n# Attach writer to render product\r\nwriter.attach([render_product])\n'})}),"\n",(0,i.jsx)(r.h4,{id:"step-5-run-randomization-loop",children:"Step 5: Run Randomization Loop"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"# Register randomization trigger\r\nwith rep.trigger.on_frame(num_frames=1000):\r\n    rep.randomizer.register(randomize_scene)\r\n\r\n# Run simulation and capture data\r\nrep.orchestrator.run()\r\n\r\n# This will generate 1000 images with annotations:\r\n# - rgb/0000.png, rgb/0001.png, ...\r\n# - bounding_box_2d_tight/0000.json, 0001.json, ...\r\n# - semantic_segmentation/0000.png, ...\r\n# - distance_to_camera/0000.npy, ...\n"})}),"\n",(0,i.jsx)(r.h3,{id:"advanced-randomization-textures-and-materials",children:"Advanced Randomization: Textures and Materials"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Randomize object materials\r\ndef randomize_materials():\r\n    materials = rep.get.prims(semantics=[("class", "material")])\r\n    with materials:\r\n        rep.randomizer.materials(\r\n            textures=[\r\n                "~/textures/wood.jpg",\r\n                "~/textures/metal.jpg",\r\n                "~/textures/concrete.jpg",\r\n            ]\r\n        )\r\n\r\nwith rep.trigger.on_frame():\r\n    rep.randomizer.register(randomize_materials)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"use-case-training-object-detection",children:"Use Case: Training Object Detection"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Generate dataset for YOLOv8 training\r\nimport omni.replicator.core as rep\r\n\r\ncamera = rep.create.camera(position=(3, 3, 2))\r\nrender_product = rep.create.render_product(camera, (640, 640))\r\n\r\n# Writer for YOLO format\r\nwriter = rep.WriterRegistry.get("YOLOWriter")\r\nwriter.initialize(\r\n    output_dir="~/datasets/robot_objects_yolo",\r\n    classes=["robot", "obstacle", "target"],\r\n)\r\nwriter.attach([render_product])\r\n\r\n# Randomize robot and obstacles\r\nwith rep.trigger.on_frame(num_frames=10000):\r\n    robots = rep.get.prims(semantics=[("class", "robot")])\r\n    with robots:\r\n        rep.modify.pose(\r\n            position=rep.distribution.uniform((-2, -2, 0), (2, 2, 0)),\r\n            rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360)),\r\n        )\r\n\r\nrep.orchestrator.run()\r\nprint("Generated 10,000 training images for YOLOv8!")\n'})}),"\n",(0,i.jsx)(r.h2,{id:"ros-2-navigation-with-isaac-sim",children:"ROS 2 Navigation with Isaac Sim"}),"\n",(0,i.jsxs)(r.p,{children:["Isaac Sim integrates seamlessly with ",(0,i.jsx)(r.strong,{children:"ROS 2 Nav2"})," stack."]}),"\n",(0,i.jsx)(r.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Install Nav2\r\nsudo apt install ros-humble-navigation2 ros-humble-nav2-bringup\r\n\r\n# Install Isaac Sim ROS 2 packages\r\ncd ~/.local/share/ov/pkg/isaac-sim-*/ros2_workspace\r\nsource /opt/ros/humble/setup.bash\r\ncolcon build --packages-select carter_navigation isaac_ros_navigation_goal\r\nsource install/setup.bash\n"})}),"\n",(0,i.jsx)(r.h3,{id:"navigation-setup-carter-robot",children:"Navigation Setup: Carter Robot"}),"\n",(0,i.jsx)(r.h4,{id:"step-1-launch-isaac-sim-with-carter",children:"Step 1: Launch Isaac Sim with Carter"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Launch Isaac Sim with hospital environment\r\n~/.local/share/ov/pkg/isaac-sim-*/isaac-sim.sh\n"})}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"In Isaac Sim GUI:"})}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"File \u2192 Open"})," \u2192 Select hospital world USD"]}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.strong,{children:"Create \u2192 Isaac \u2192 Robots \u2192 Carter v2"})}),"\n",(0,i.jsx)(r.li,{children:"Position Carter at (0, 0, 0)"}),"\n"]}),"\n",(0,i.jsx)(r.h4,{id:"step-2-configure-ros-2-bridge",children:"Step 2: Configure ROS 2 Bridge"}),"\n",(0,i.jsx)(r.p,{children:"Create Action Graph for Nav2 integration:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import omni.graph.core as og\r\n\r\nkeys = og.Controller.Keys\r\n(graph, nodes, _, _) = og.Controller.edit(\r\n    {"graph_path": "/World/Nav2Graph", "evaluator_name": "execution"},\r\n    {\r\n        keys.CREATE_NODES: [\r\n            ("OnTick", "omni.graph.action.OnPlaybackTick"),\r\n            ("PublishClock", "omni.isaac.ros2_bridge.ROS2PublishClock"),\r\n            ("PublishTF", "omni.isaac.ros2_bridge.ROS2PublishTransformTree"),\r\n            ("SubscribeTwist", "omni.isaac.ros2_bridge.ROS2SubscribeTwist"),\r\n            ("DiffController", "omni.isaac.wheeled_robots.DifferentialController"),\r\n            ("ArticController", "omni.isaac.core_nodes.IsaacArticulationController"),\r\n            ("PublishOdom", "omni.isaac.ros2_bridge.ROS2PublishOdometry"),\r\n            ("PublishLaser", "omni.isaac.ros2_bridge.ROS2RtxLidarHelper"),\r\n        ],\r\n        keys.SET_VALUES: [\r\n            ("SubscribeTwist.inputs:topicName", "/cmd_vel"),\r\n            ("PublishOdom.inputs:topicName", "/odom"),\r\n            ("PublishOdom.inputs:chassisPrim", "/World/Carter"),\r\n            ("PublishLaser.inputs:topicName", "/scan"),\r\n            ("DiffController.inputs:wheelDistance", 0.4132),\r\n            ("DiffController.inputs:wheelRadius", 0.0775),\r\n            ("ArticController.inputs:robotPath", "/World/Carter"),\r\n        ],\r\n        keys.CONNECT: [\r\n            ("OnTick.outputs:tick", "PublishClock.inputs:execIn"),\r\n            ("OnTick.outputs:tick", "PublishTF.inputs:execIn"),\r\n            ("OnTick.outputs:tick", "PublishOdom.inputs:execIn"),\r\n            ("OnTick.outputs:tick", "PublishLaser.inputs:execIn"),\r\n            ("SubscribeTwist.outputs:angularVelocity", "DiffController.inputs:angularVelocity"),\r\n            ("SubscribeTwist.outputs:linearVelocity", "DiffController.inputs:linearVelocity"),\r\n            ("DiffController.outputs:velocityCommand", "ArticController.inputs:velocityCommand"),\r\n        ],\r\n    },\r\n)\n'})}),"\n",(0,i.jsx)(r.h4,{id:"step-3-launch-nav2",children:"Step 3: Launch Nav2"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Terminal 1: Launch Nav2\r\nsource ~/ros2_workspace/install/setup.bash\r\nros2 launch carter_navigation carter_navigation.launch.py\r\n\r\n# This starts:\r\n# - AMCL (localization)\r\n# - Map server\r\n# - Planner server\r\n# - Controller server\r\n# - Behavior server\n"})}),"\n",(0,i.jsx)(r.h4,{id:"step-4-set-navigation-goal",children:"Step 4: Set Navigation Goal"}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Option 1: RViz"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:'# Terminal 2: Launch RViz with Nav2 config\r\nrviz2 -d $(ros2 pkg prefix carter_navigation)/share/carter_navigation/rviz/carter_nav2.rviz\r\n\r\n# In RViz:\r\n# - Click "2D Pose Estimate" to set initial pose\r\n# - Click "Nav2 Goal" to set goal location\r\n# - Robot navigates autonomously!\n'})}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Option 2: Python Script"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import rclpy\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom rclpy.node import Node\r\n\r\nclass GoalPublisher(Node):\r\n    def __init__(self):\r\n        super().__init__('goal_publisher')\r\n        self.publisher = self.create_publisher(PoseStamped, '/goal_pose', 10)\r\n\r\n    def send_goal(self, x, y, theta):\r\n        goal = PoseStamped()\r\n        goal.header.frame_id = 'map'\r\n        goal.header.stamp = self.get_clock().now().to_msg()\r\n        goal.pose.position.x = x\r\n        goal.pose.position.y = y\r\n        goal.pose.orientation.z = theta\r\n        goal.pose.orientation.w = 1.0\r\n        self.publisher.publish(goal)\r\n        self.get_logger().info(f'Sent goal: ({x}, {y}, {theta})')\r\n\r\nrclpy.init()\r\nnode = GoalPublisher()\r\nnode.send_goal(5.0, 3.0, 0.0)  # Navigate to (5, 3)\r\nrclpy.spin_once(node)\r\nnode.destroy_node()\r\nrclpy.shutdown()\n"})}),"\n",(0,i.jsx)(r.h3,{id:"creating-a-custom-map",children:"Creating a Custom Map"}),"\n",(0,i.jsx)(r.h4,{id:"step-1-generate-occupancy-grid",children:"Step 1: Generate Occupancy Grid"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Use Isaac Sim\'s occupancy map generator\r\nfrom omni.isaac.occupancy_map import OccupancyMapGenerator\r\n\r\n# Create generator\r\nmap_generator = OccupancyMapGenerator(\r\n    cell_size=0.05,          # 5cm resolution\r\n    bounds_min=[-10, -10],   # Map bounds\r\n    bounds_max=[10, 10],\r\n    height_threshold=0.1,    # Consider obstacles above 10cm\r\n)\r\n\r\n# Generate map from current scene\r\noccupancy_grid = map_generator.generate()\r\n\r\n# Save as ROS map format (.pgm + .yaml)\r\nmap_generator.save_map("~/maps/hospital_map")\n'})}),"\n",(0,i.jsx)(r.h4,{id:"step-2-use-map-in-nav2",children:"Step 2: Use Map in Nav2"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:"# hospital_map.yaml\r\nimage: hospital_map.pgm\r\nresolution: 0.05\r\norigin: [-10.0, -10.0, 0.0]\r\nnegate: 0\r\noccupied_thresh: 0.65\r\nfree_thresh: 0.196\n"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Launch Nav2 with custom map\r\nros2 launch carter_navigation carter_navigation.launch.py \\\r\n  map:=$HOME/maps/hospital_map.yaml\n"})}),"\n",(0,i.jsx)(r.h2,{id:"isaac-perceptor-integration",children:"Isaac Perceptor Integration"}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Isaac Perceptor"})," is NVIDIA's vision-based SLAM and perception stack."]}),"\n",(0,i.jsx)(r.h3,{id:"what-is-isaac-perceptor",children:"What is Isaac Perceptor?"}),"\n",(0,i.jsx)(r.p,{children:"Isaac Perceptor combines:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"nvblox"}),": GPU-accelerated 3D reconstruction"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"cuVSLAM"}),": Visual-inertial SLAM"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"cuMotion"}),": Trajectory optimization"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Depth estimation"}),": AI-based depth from stereo or monocular cameras"]}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"running-perceptor-in-isaac-sim",children:"Running Perceptor in Isaac Sim"}),"\n",(0,i.jsx)(r.h4,{id:"prerequisites-1",children:"Prerequisites"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Install Isaac ROS Perceptor\r\nsudo apt install ros-humble-isaac-ros-perceptor\r\n\r\n# Or build from source\r\ncd ~/ros2_workspace/src\r\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_perceptor.git\r\ncd ~/ros2_workspace\r\ncolcon build --packages-up-to isaac_perceptor\n"})}),"\n",(0,i.jsx)(r.h4,{id:"step-1-setup-nova-carter-with-cameras",children:"Step 1: Setup Nova Carter with Cameras"}),"\n",(0,i.jsxs)(r.p,{children:["Nova Carter is NVIDIA's reference robot with ",(0,i.jsx)(r.strong,{children:"6 stereo cameras"})," for 360\xb0 perception."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Load in Isaac Sim:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Nova Carter has built-in camera rig\r\nfrom omni.isaac.kit import SimulationApp\r\nsimulation_app = SimulationApp({"headless": False})\r\n\r\nfrom omni.isaac.core import World\r\nimport omni.isaac.core.utils.nucleus as nucleus\r\n\r\nworld = World()\r\n\r\n# Load Nova Carter robot\r\nnucleus_server = nucleus.get_assets_root_path()\r\nnova_carter_usd = f"{nucleus_server}/Isaac/Robots/Carter/nova_carter_sensors.usd"\r\nworld.scene.add_reference_to_stage(nova_carter_usd, "/World/NovaCarter")\r\n\r\nworld.reset()\n'})}),"\n",(0,i.jsx)(r.h4,{id:"step-2-launch-perceptor-stack",children:"Step 2: Launch Perceptor Stack"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Terminal 1: Launch Isaac Perceptor\r\nros2 launch isaac_perceptor isaac_perceptor.launch.py\r\n\r\n# This starts:\r\n# - Visual SLAM (cuVSLAM)\r\n# - 3D reconstruction (nvblox)\r\n# - Costmap generation\r\n# - Localization\n"})}),"\n",(0,i.jsx)(r.h4,{id:"step-3-visualize-in-rviz",children:"Step 3: Visualize in RViz"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Terminal 2: RViz\r\nrviz2 -d $(ros2 pkg prefix isaac_perceptor)/share/isaac_perceptor/rviz/perceptor.rviz\r\n\r\n# You'll see:\r\n# - Live camera feeds (6 stereo pairs)\r\n# - 3D mesh reconstruction (nvblox)\r\n# - Robot trajectory\r\n# - Costmap for navigation\n"})}),"\n",(0,i.jsx)(r.h4,{id:"step-4-navigate-with-vision",children:"Step 4: Navigate with Vision"}),"\n",(0,i.jsxs)(r.p,{children:["Perceptor provides ",(0,i.jsx)(r.code,{children:"/costmap"})," for Nav2:"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Use Perceptor costmap instead of LiDAR-based costmap\r\nros2 launch nav2_bringup navigation_launch.py \\\r\n  use_sim_time:=True \\\r\n  params_file:=~/isaac_perceptor_nav_params.yaml\n"})}),"\n",(0,i.jsx)(r.h3,{id:"perceptor-features",children:"Perceptor Features"}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"1. Real-time 3D Reconstruction (nvblox)"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Builds voxel-based 3D map from camera depth"}),"\n",(0,i.jsx)(r.li,{children:"Updates at 10-30 Hz"}),"\n",(0,i.jsx)(r.li,{children:"GPU-accelerated fusion"}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"2. Visual SLAM (cuVSLAM)"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Tracks robot pose using camera features"}),"\n",(0,i.jsx)(r.li,{children:"No LiDAR required"}),"\n",(0,i.jsx)(r.li,{children:"Works in GPS-denied environments"}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"3. Semantic Costmap"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"AI-based obstacle detection"}),"\n",(0,i.jsx)(r.li,{children:"Classifies: floor, walls, obstacles, dynamic objects"}),"\n",(0,i.jsx)(r.li,{children:"Safer than purely geometric methods"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"complete-example-warehouse-navigation",children:"Complete Example: Warehouse Navigation"}),"\n",(0,i.jsx)(r.p,{children:"Let's build an end-to-end navigation system."}),"\n",(0,i.jsx)(r.h3,{id:"scenario",children:"Scenario"}),"\n",(0,i.jsx)(r.p,{children:"A mobile robot navigates a warehouse, avoids obstacles, and reaches shelf locations."}),"\n",(0,i.jsx)(r.h3,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# warehouse_navigation.py\r\nimport omni.isaac.core.utils.nucleus as nucleus\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.wheeled_robots import WheeledRobot\r\nfrom omni.isaac.core.utils.types import ArticulationAction\r\nimport numpy as np\r\n\r\n# Create simulation\r\nworld = World()\r\n\r\n# Load warehouse environment\r\nnucleus_server = nucleus.get_assets_root_path()\r\nwarehouse_usd = f"{nucleus_server}/Isaac/Environments/Simple_Warehouse/warehouse.usd"\r\nworld.scene.add_reference_to_stage(warehouse_usd, "/World/Warehouse")\r\n\r\n# Add mobile robot\r\nrobot = world.scene.add(\r\n    WheeledRobot(\r\n        prim_path="/World/Robot",\r\n        name="warehouse_robot",\r\n        wheel_dof_names=["left_wheel_joint", "right_wheel_joint"],\r\n        position=np.array([0, 0, 0.2]),\r\n    )\r\n)\r\n\r\n# Add sensors (camera + LiDAR)\r\n# ... (omitted for brevity, see previous camera/LiDAR examples)\r\n\r\nworld.reset()\r\n\r\n# Simple navigation loop\r\ngoal_positions = [\r\n    (5.0, 0.0),\r\n    (5.0, 5.0),\r\n    (0.0, 5.0),\r\n    (0.0, 0.0),\r\n]\r\n\r\nfor goal_x, goal_y in goal_positions:\r\n    print(f"Navigating to ({goal_x}, {goal_y})")\r\n\r\n    # In practice, use Nav2 for planning\r\n    # Here: simple proportional controller\r\n    while True:\r\n        robot_pos, _ = robot.get_world_pose()\r\n        dx = goal_x - robot_pos[0]\r\n        dy = goal_y - robot_pos[1]\r\n        distance = np.sqrt(dx**2 + dy**2)\r\n\r\n        if distance < 0.5:  # Reached goal\r\n            break\r\n\r\n        # Compute velocities\r\n        linear_vel = min(1.0, distance * 0.5)\r\n        angle_to_goal = np.arctan2(dy, dx)\r\n        angular_vel = angle_to_goal * 0.5\r\n\r\n        # Apply to robot\r\n        robot.apply_wheel_actions(\r\n            ArticulationAction(joint_velocities=[linear_vel - angular_vel, linear_vel + angular_vel])\r\n        )\r\n\r\n        world.step(render=True)\r\n\r\n    print(f"Reached ({goal_x}, {goal_y})")\r\n\r\nprint("Warehouse patrol complete!")\n'})}),"\n",(0,i.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(r.p,{children:"In this chapter, you learned:"}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"RTX Sensors"}),": Configured cameras, depth sensors, and LiDAR with realistic physics"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"ROS 2 Publishing"}),": Published sensor data to ROS 2 topics for external processing"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Synthetic Data"}),": Used Replicator for domain randomization and dataset generation"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"ROS 2 Nav2"}),": Integrated Isaac Sim with Nav2 for autonomous navigation"]}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Isaac Perceptor"}),": Implemented vision-based SLAM and perception"]}),"\n",(0,i.jsx)(r.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["Isaac Sim's ",(0,i.jsx)(r.strong,{children:"RTX sensors"})," provide physically accurate simulations"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Omniverse Replicator"})," enables automated synthetic dataset creation"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Nav2 integration"})," allows testing navigation algorithms in realistic environments"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Isaac Perceptor"})," provides cutting-edge camera-based perception"]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(r.h3,{id:"exercise-1-multi-sensor-fusion",children:"Exercise 1: Multi-Sensor Fusion"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsx)(r.li,{children:"Create a robot with RGB camera, depth camera, and LiDAR"}),"\n",(0,i.jsx)(r.li,{children:"Publish all three to ROS 2"}),"\n",(0,i.jsxs)(r.li,{children:["Use ",(0,i.jsx)(r.code,{children:"sensor_msgs::PointCloud2"})," fusion in ROS 2 to combine data"]}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"exercise-2-dataset-generation",children:"Exercise 2: Dataset Generation"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsx)(r.li,{children:"Create a scene with 20 random objects"}),"\n",(0,i.jsx)(r.li,{children:"Use Replicator to randomize poses, scales, and lighting"}),"\n",(0,i.jsx)(r.li,{children:"Generate 1000 images with bounding box annotations"}),"\n",(0,i.jsx)(r.li,{children:"Train a simple YOLOv8 model on the synthetic data"}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"exercise-3-navigation-challenge",children:"Exercise 3: Navigation Challenge"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsx)(r.li,{children:"Load a complex environment (e.g., hospital or warehouse)"}),"\n",(0,i.jsx)(r.li,{children:"Configure Nav2 with Isaac Sim"}),"\n",(0,i.jsx)(r.li,{children:"Set 5 waypoints and autonomously navigate between them"}),"\n",(0,i.jsx)(r.li,{children:"Handle dynamic obstacles (add moving objects)"}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"challenge-vision-based-navigation",children:"Challenge: Vision-Based Navigation"}),"\n",(0,i.jsxs)(r.p,{children:["Use ",(0,i.jsx)(r.strong,{children:"only cameras"})," (no LiDAR) with Isaac Perceptor:"]}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsx)(r.li,{children:"Setup Nova Carter with 6 stereo cameras"}),"\n",(0,i.jsx)(r.li,{children:"Launch Isaac Perceptor for 3D reconstruction"}),"\n",(0,i.jsx)(r.li,{children:"Navigate to goal using vision-based costmap"}),"\n",(0,i.jsx)(r.li,{children:"Record trajectory and compare with LiDAR-based navigation"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"up-next",children:"Up Next"}),"\n",(0,i.jsxs)(r.p,{children:["In ",(0,i.jsx)(r.strong,{children:"Chapter 8: Vision-Language-Action (VLA) Systems"}),", we'll explore:"]}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Integrating vision models with language understanding"}),"\n",(0,i.jsx)(r.li,{children:"Using LLMs for high-level task planning"}),"\n",(0,i.jsx)(r.li,{children:"Voice-commanded robot control"}),"\n",(0,i.jsx)(r.li,{children:"Multimodal perception and reasoning"}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://docs.isaacsim.omniverse.nvidia.com/latest/features/sensors_simulation/index.html",children:"Isaac Sim Sensors Documentation"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://docs.omniverse.nvidia.com/py/replicator/index.html",children:"Omniverse Replicator Documentation"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://navigation.ros.org/",children:"ROS 2 Nav2 Documentation"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://nvidia-isaac-ros.github.io/reference_workflows/isaac_perceptor/run_perceptor_in_sim.html",children:"Isaac Perceptor Tutorial"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"Isaac ROS GitHub"})}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Sources:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://www.marvik.ai/blog/isaac-sim-integration-with-ros-2",children:"Isaac Sim ROS 2 Integration - Marvik"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://docs.isaacsim.omniverse.nvidia.com/5.0.0/ros2_tutorials/tutorial_ros2_navigation.html",children:"ROS 2 Navigation Tutorial - Isaac Sim Docs"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://nvidia-isaac-ros.github.io/reference_workflows/isaac_perceptor/run_perceptor_in_sim.html",children:"Isaac Perceptor in Sim - Isaac ROS Docs"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://docs.isaacsim.omniverse.nvidia.com/5.1.0/ros2_tutorials/tutorial_ros2_camera.html",children:"ROS 2 Cameras - Isaac Sim Docs"})}),"\n"]})]})}function p(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>t,x:()=>o});var a=n(6540);const i={},s=a.createContext(i);function t(e){const r=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),a.createElement(s.Provider,{value:r},e.children)}}}]);